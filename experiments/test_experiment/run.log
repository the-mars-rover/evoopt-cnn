2023-07-31 19:02:17,951 INFO: Starting experiment with the arguments logged below.
2023-07-31 19:02:17,952 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=512, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-07-31 19:02:17,952 INFO: Setting the random number generator seed for this experiment.
2023-07-31 19:02:17,952 INFO: Loading dataset for the experiment.
2023-07-31 19:02:18,523 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-07-31 19:02:18,523 INFO: Setting up DEAP toolbox.
2023-07-31 19:02:18,523 INFO: Registering individual initialization method.
2023-07-31 19:02:18,523 INFO: Registering population initialization method.
2023-07-31 19:02:18,523 INFO: Registering the selection method.
2023-07-31 19:02:18,523 INFO: Registering the evaluation method.
2023-07-31 19:02:18,523 INFO: Registering the genetic operators.
2023-07-31 19:02:18,524 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-07-31 19:02:18,524 INFO: Initializing the initial population.
2023-07-31 19:02:18,526 INFO: Running the evolutionary algorithm.
2023-07-31 19:02:18,526 INFO: Evaluating fitness for the initial generation of individuals.
2023-07-31 19:02:18,526 INFO: Will evaluate fitness for 5 individuals.
2023-07-31 19:02:18,527 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:02:18,528 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:02:18,651 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:02:18,657 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:02:18,683 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:02:18,685 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:02:18,711 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:02:18,714 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:02:18,736 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:02:18,738 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:02:18,760 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:02:18,762 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:02:52,157 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-07-31 19:02:52,158 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-07-31 19:03:17,283 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-07-31 19:03:17,284 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-07-31 19:03:42,447 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-07-31 19:03:42,447 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-07-31 19:04:14,745 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6286628662866287 ,beta_2=0.8396839683968397 ,learning_rate_power=-0.7882788278827882 ,initial_accumulator_value=0.8498849884988499 ,l1_regularization_strength=0.9197919791979198 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:04:14,745 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6286628662866287 ,beta_2=0.8396839683968397 ,learning_rate_power=-0.7882788278827882 ,initial_accumulator_value=0.8498849884988499 ,l1_regularization_strength=0.9197919791979198 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:04:40,701 INFO: Applying selection operators for generation 1.
2023-07-31 19:04:40,702 INFO: Applying genetic operators for generation 1.
2023-07-31 19:04:40,703 INFO: Evaluating fitness for for generation 1.
2023-07-31 19:04:40,703 INFO: Will evaluate fitness for 4 individuals.
2023-07-31 19:04:40,703 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:04:40,703 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:05:08,253 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-07-31 19:05:08,254 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-07-31 19:05:39,830 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.30803080308030806, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8295829582958296 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=False ,beta_1=0.05020502050205021 ,beta_2=0.9434943494349435 ,learning_rate_power=-0.7882788278827882 ,initial_accumulator_value=0.8498849884988499 ,l1_regularization_strength=0.4892489248924893 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.34753475347534757 ,beta=0.1252125212521252].
2023-07-31 19:05:39,831 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.30803080308030806, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8295829582958296 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=False ,beta_1=0.05020502050205021 ,beta_2=0.9434943494349435 ,learning_rate_power=-0.7882788278827882 ,initial_accumulator_value=0.8498849884988499 ,l1_regularization_strength=0.4892489248924893 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.34753475347534757 ,beta=0.1252125212521252].
2023-07-31 19:06:06,730 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:06:06,730 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:06:31,953 INFO: Applying selection operators for generation 2.
2023-07-31 19:06:31,954 INFO: Applying genetic operators for generation 2.
2023-07-31 19:06:31,954 INFO: Evaluating fitness for for generation 2.
2023-07-31 19:06:31,954 INFO: Will evaluate fitness for 4 individuals.
2023-07-31 19:06:31,954 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:06:31,954 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:08:30,563 INFO: Starting experiment with the arguments logged below.
2023-07-31 19:08:30,565 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-07-31 19:08:30,565 INFO: Setting the random number generator seed for this experiment.
2023-07-31 19:08:30,565 INFO: Loading dataset for the experiment.
2023-07-31 19:08:31,078 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-07-31 19:08:31,078 INFO: Setting up DEAP toolbox.
2023-07-31 19:08:31,078 INFO: Registering individual initialization method.
2023-07-31 19:08:31,078 INFO: Registering population initialization method.
2023-07-31 19:08:31,078 INFO: Registering the selection method.
2023-07-31 19:08:31,078 INFO: Registering the evaluation method.
2023-07-31 19:08:31,078 INFO: Registering the genetic operators.
2023-07-31 19:08:31,078 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-07-31 19:08:31,078 INFO: Initializing the initial population.
2023-07-31 19:08:31,081 INFO: Running the evolutionary algorithm.
2023-07-31 19:08:31,081 INFO: Evaluating fitness for the initial generation of individuals.
2023-07-31 19:08:31,081 INFO: Will evaluate fitness for 5 individuals.
2023-07-31 19:08:31,082 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:08:31,082 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:08:31,164 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:08:31,168 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:08:31,202 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:08:31,205 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:08:31,239 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:08:31,242 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:08:31,269 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:08:31,272 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:08:31,299 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:08:31,302 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:08:32,442 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:08:33,719 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:08:59,022 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-07-31 19:08:59,022 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-07-31 19:09:00,103 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:09:01,536 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:09:21,321 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-07-31 19:09:21,322 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-07-31 19:09:22,382 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:09:23,955 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:09:43,999 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-07-31 19:09:43,999 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-07-31 19:09:45,065 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:09:48,309 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:10:16,078 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6286628662866287 ,beta_2=0.8396839683968397 ,learning_rate_power=-0.7882788278827882 ,initial_accumulator_value=0.8498849884988499 ,l1_regularization_strength=0.9197919791979198 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:10:16,078 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6286628662866287 ,beta_2=0.8396839683968397 ,learning_rate_power=-0.7882788278827882 ,initial_accumulator_value=0.8498849884988499 ,l1_regularization_strength=0.9197919791979198 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:10:17,139 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:10:18,719 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:10:40,390 INFO: Applying selection operators for generation 1.
2023-07-31 19:10:40,391 INFO: Applying genetic operators for generation 1.
2023-07-31 19:10:40,392 INFO: Evaluating fitness for for generation 1.
2023-07-31 19:10:40,392 INFO: Will evaluate fitness for 4 individuals.
2023-07-31 19:10:40,392 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:10:40,392 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:11:05,613 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-07-31 19:11:05,613 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-07-31 19:11:37,644 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.30803080308030806, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8295829582958296 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.05020502050205021 ,beta_2=0.9434943494349435 ,learning_rate_power=-0.7882788278827882 ,initial_accumulator_value=0.8498849884988499 ,l1_regularization_strength=0.4892489248924893 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.34753475347534757 ,beta=0.1252125212521252].
2023-07-31 19:11:37,644 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.30803080308030806, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8295829582958296 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.05020502050205021 ,beta_2=0.9434943494349435 ,learning_rate_power=-0.7882788278827882 ,initial_accumulator_value=0.8498849884988499 ,l1_regularization_strength=0.4892489248924893 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.34753475347534757 ,beta=0.1252125212521252].
2023-07-31 19:12:19,294 INFO: Starting experiment with the arguments logged below.
2023-07-31 19:12:19,294 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=2048, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-07-31 19:12:19,294 INFO: Setting the random number generator seed for this experiment.
2023-07-31 19:12:19,294 INFO: Loading dataset for the experiment.
2023-07-31 19:12:19,785 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-07-31 19:12:19,785 INFO: Setting up DEAP toolbox.
2023-07-31 19:12:19,785 INFO: Registering individual initialization method.
2023-07-31 19:12:19,785 INFO: Registering population initialization method.
2023-07-31 19:12:19,785 INFO: Registering the selection method.
2023-07-31 19:12:19,785 INFO: Registering the evaluation method.
2023-07-31 19:12:19,785 INFO: Registering the genetic operators.
2023-07-31 19:12:19,786 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-07-31 19:12:19,786 INFO: Initializing the initial population.
2023-07-31 19:12:19,788 INFO: Running the evolutionary algorithm.
2023-07-31 19:12:19,788 INFO: Evaluating fitness for the initial generation of individuals.
2023-07-31 19:12:19,788 INFO: Will evaluate fitness for 5 individuals.
2023-07-31 19:12:19,789 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:12:19,789 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:12:19,866 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:12:19,870 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:12:19,905 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:12:19,908 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:12:19,944 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:12:19,947 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:12:19,975 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:12:19,978 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:12:20,006 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:12:20,010 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:12:21,161 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:12:22,436 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:12:46,072 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-07-31 19:12:46,072 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-07-31 19:12:47,148 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:12:48,614 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:13:06,601 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-07-31 19:13:06,602 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-07-31 19:13:07,970 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:13:09,402 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:13:27,547 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-07-31 19:13:27,547 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-07-31 19:13:28,618 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:13:31,759 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:13:56,459 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6286628662866287 ,beta_2=0.8396839683968397 ,learning_rate_power=-0.7882788278827882 ,initial_accumulator_value=0.8498849884988499 ,l1_regularization_strength=0.9197919791979198 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:13:56,459 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6286628662866287 ,beta_2=0.8396839683968397 ,learning_rate_power=-0.7882788278827882 ,initial_accumulator_value=0.8498849884988499 ,l1_regularization_strength=0.9197919791979198 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:13:57,512 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:13:58,740 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-07-31 19:19:26,372 INFO: Starting experiment with the arguments logged below.
2023-07-31 19:19:26,372 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=2048, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-07-31 19:19:26,373 INFO: Setting the random number generator seed for this experiment.
2023-07-31 19:19:26,373 INFO: Loading dataset for the experiment.
2023-07-31 19:19:26,894 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-07-31 19:19:26,894 INFO: Setting up DEAP toolbox.
2023-07-31 19:19:26,894 INFO: Registering individual initialization method.
2023-07-31 19:19:26,894 INFO: Registering population initialization method.
2023-07-31 19:19:26,894 INFO: Registering the selection method.
2023-07-31 19:19:26,895 INFO: Registering the evaluation method.
2023-07-31 19:19:26,895 INFO: Registering the genetic operators.
2023-07-31 19:19:26,895 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-07-31 19:19:26,895 INFO: Initializing the initial population.
2023-07-31 19:19:26,897 INFO: Running the evolutionary algorithm.
2023-07-31 19:19:26,897 INFO: Evaluating fitness for the initial generation of individuals.
2023-07-31 19:19:26,897 INFO: Will evaluate fitness for 5 individuals.
2023-07-31 19:19:26,898 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:19:26,898 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:19:27,792 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:27,797 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:27,815 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:27,819 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:27,837 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:27,841 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:27,857 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:27,862 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:27,878 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:27,882 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:28,064 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:28,066 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:28,067 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:28,068 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:28,069 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:28,071 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:28,072 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:28,073 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:28,074 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:28,075 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:19:34,843 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:19:36,096 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-07-31 19:19:36,097 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-07-31 19:19:39,968 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:19:41,079 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-07-31 19:19:41,079 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-07-31 19:19:44,616 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:19:45,621 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-07-31 19:19:45,621 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-07-31 19:19:52,298 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:19:53,451 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6286628662866287 ,beta_2=0.8396839683968397 ,learning_rate_power=-0.7882788278827882 ,initial_accumulator_value=0.8498849884988499 ,l1_regularization_strength=0.9197919791979198 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:19:53,451 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6286628662866287 ,beta_2=0.8396839683968397 ,learning_rate_power=-0.7882788278827882 ,initial_accumulator_value=0.8498849884988499 ,l1_regularization_strength=0.9197919791979198 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:19:57,261 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:19:58,262 INFO: Applying selection operators for generation 1.
2023-07-31 19:19:58,262 INFO: Applying genetic operators for generation 1.
2023-07-31 19:19:58,263 INFO: Evaluating fitness for for generation 1.
2023-07-31 19:19:58,263 INFO: Will evaluate fitness for 4 individuals.
2023-07-31 19:19:58,263 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:19:58,263 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:20:01,724 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:20:02,932 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:20:02,933 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:20:06,419 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:20:07,433 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.30803080308030806, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8295829582958296 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=False ,beta_1=0.05020502050205021 ,beta_2=0.9434943494349435 ,learning_rate_power=-0.7882788278827882 ,initial_accumulator_value=0.8498849884988499 ,l1_regularization_strength=0.4892489248924893 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.34753475347534757 ,beta=0.1252125212521252].
2023-07-31 19:20:07,433 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.30803080308030806, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8295829582958296 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=False ,beta_1=0.05020502050205021 ,beta_2=0.9434943494349435 ,learning_rate_power=-0.7882788278827882 ,initial_accumulator_value=0.8498849884988499 ,l1_regularization_strength=0.4892489248924893 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.34753475347534757 ,beta=0.1252125212521252].
2023-07-31 19:20:12,222 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:20:13,342 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:20:13,342 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:20:16,922 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:20:17,926 INFO: Applying selection operators for generation 2.
2023-07-31 19:20:17,926 INFO: Applying genetic operators for generation 2.
2023-07-31 19:20:17,927 INFO: Evaluating fitness for for generation 2.
2023-07-31 19:20:17,927 INFO: Will evaluate fitness for 4 individuals.
2023-07-31 19:20:17,927 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:20:17,927 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:20:21,822 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:20:22,849 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:20:22,849 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:20:26,329 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:20:27,332 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:20:27,333 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:20:31,111 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:20:32,136 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:20:32,136 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:20:35,659 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:20:36,661 INFO: Applying selection operators for generation 3.
2023-07-31 19:20:36,661 INFO: Applying genetic operators for generation 3.
2023-07-31 19:20:36,662 INFO: Evaluating fitness for for generation 3.
2023-07-31 19:20:36,662 INFO: Will evaluate fitness for 5 individuals.
2023-07-31 19:20:36,662 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:20:36,662 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:20:40,546 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:20:41,571 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:20:41,571 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:20:45,122 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:20:46,430 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:20:46,430 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:20:49,919 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:20:50,939 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:20:50,939 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:20:54,807 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:20:55,839 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0749074907490749, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.9614961496149615 ,rho=0.18931893189318932 ,epsilon=0.00039553961440144013 ,centered=True ,beta_1=0.6197619761976197 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.34013401340134014 ,beta=0.09190919091909192].
2023-07-31 19:20:55,839 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0749074907490749, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.9614961496149615 ,rho=0.18931893189318932 ,epsilon=0.00039553961440144013 ,centered=True ,beta_1=0.6197619761976197 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.34013401340134014 ,beta=0.09190919091909192].
2023-07-31 19:20:59,399 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:21:00,403 INFO: Applying selection operators for generation 4.
2023-07-31 19:21:00,403 INFO: Applying genetic operators for generation 4.
2023-07-31 19:21:00,404 INFO: Evaluating fitness for for generation 4.
2023-07-31 19:21:00,404 INFO: Will evaluate fitness for 3 individuals.
2023-07-31 19:21:00,405 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:21:00,405 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:21:04,506 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:21:05,562 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:21:05,562 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:21:09,197 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:21:10,214 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.11611161116111611, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9614961496149615 ,rho=0.2838283828382838 ,epsilon=0.00024502457794779476 ,centered=True ,beta_1=0.6197619761976197 ,beta_2=0.17511751175117513 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.05200520052005201 ,beta=0.09190919091909192].
2023-07-31 19:21:10,215 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.11611161116111611, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9614961496149615 ,rho=0.2838283828382838 ,epsilon=0.00024502457794779476 ,centered=True ,beta_1=0.6197619761976197 ,beta_2=0.17511751175117513 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.05200520052005201 ,beta=0.09190919091909192].
2023-07-31 19:21:14,373 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:21:15,465 INFO: Applying selection operators for generation 5.
2023-07-31 19:21:15,465 INFO: Applying genetic operators for generation 5.
2023-07-31 19:21:15,466 INFO: Evaluating fitness for for generation 5.
2023-07-31 19:21:15,466 INFO: Will evaluate fitness for 4 individuals.
2023-07-31 19:21:15,467 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.11611161116111611, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9614961496149615 ,rho=0.2838283828382838 ,epsilon=0.00024502457794779476 ,centered=True ,beta_1=0.6197619761976197 ,beta_2=0.17511751175117513 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.05200520052005201 ,beta=0.09190919091909192].
2023-07-31 19:21:15,467 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.11611161116111611, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9614961496149615 ,rho=0.2838283828382838 ,epsilon=0.00024502457794779476 ,centered=True ,beta_1=0.6197619761976197 ,beta_2=0.17511751175117513 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.05200520052005201 ,beta=0.09190919091909192].
2023-07-31 19:21:19,371 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:21:20,687 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.9241924192419242, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.36983698369836987 ,rho=0.8111811181118113 ,epsilon=0.00024502457794779476 ,centered=True ,beta_1=0.6197619761976197 ,beta_2=0.45094509450945097 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.11721172117211721 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.8382838283828383 ,beta=0.09190919091909192].
2023-07-31 19:21:20,688 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.9241924192419242, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.36983698369836987 ,rho=0.8111811181118113 ,epsilon=0.00024502457794779476 ,centered=True ,beta_1=0.6197619761976197 ,beta_2=0.45094509450945097 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.11721172117211721 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.8382838283828383 ,beta=0.09190919091909192].
2023-07-31 19:21:24,728 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:21:25,863 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:21:25,863 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:21:29,850 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:21:30,896 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:21:30,896 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8228822882288229, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006627000000000001 ,use_ema=False ,ema_momentum=0.7522752275227523 ,rho=0.44114411441144114 ,epsilon=0.000897889799189919 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.33663366336633666 ,l2_shrinkage_regularization_strength=0.6981698169816982 ,beta=0.09190919091909192].
2023-07-31 19:21:34,439 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-07-31 19:21:35,453 INFO: Saving the results to the folder specified in the arguments.
2023-07-31 19:21:35,456 INFO: Experiment finished.
2023-07-31 19:24:16,603 INFO: Starting experiment with the arguments logged below.
2023-07-31 19:24:16,603 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=512, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-07-31 19:24:16,603 INFO: Setting the random number generator seed for this experiment.
2023-07-31 19:24:16,604 INFO: Loading dataset for the experiment.
2023-07-31 19:24:17,126 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-07-31 19:24:17,126 INFO: Setting up DEAP toolbox.
2023-07-31 19:24:17,127 INFO: Registering individual initialization method.
2023-07-31 19:24:17,127 INFO: Registering population initialization method.
2023-07-31 19:24:17,127 INFO: Registering the selection method.
2023-07-31 19:24:17,127 INFO: Registering the evaluation method.
2023-07-31 19:24:17,127 INFO: Registering the genetic operators.
2023-07-31 19:24:17,127 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-07-31 19:24:17,127 INFO: Initializing the initial population.
2023-07-31 19:24:17,129 INFO: Running the evolutionary algorithm.
2023-07-31 19:24:17,129 INFO: Evaluating fitness for the initial generation of individuals.
2023-07-31 19:24:17,129 INFO: Will evaluate fitness for 5 individuals.
2023-07-31 19:24:17,130 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:24:17,130 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-07-31 19:24:17,971 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:17,975 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:17,992 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:17,996 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:18,013 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:18,017 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:18,032 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:18,036 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:18,051 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:18,055 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:18,236 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:18,238 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:18,239 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:18,240 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:18,242 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:18,243 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:18,244 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:18,245 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:18,246 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:24:18,248 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-07-31 19:25:14,528 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-07-31 19:25:14,528 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-07-31 19:26:14,993 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-07-31 19:26:14,994 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 09:11:21,139 INFO: Starting experiment with the arguments logged below.
2023-08-01 09:11:21,159 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 09:11:21,159 INFO: Setting the random number generator seed for this experiment.
2023-08-01 09:11:21,159 INFO: Loading dataset for the experiment.
2023-08-01 09:11:21,709 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 09:11:21,710 INFO: Setting up DEAP toolbox.
2023-08-01 09:11:21,710 INFO: Registering individual initialization method.
2023-08-01 09:11:21,710 INFO: Registering population initialization method.
2023-08-01 09:11:21,710 INFO: Registering the selection method.
2023-08-01 09:11:21,710 INFO: Registering the evaluation method.
2023-08-01 09:11:21,710 INFO: Registering the genetic operators.
2023-08-01 09:11:21,710 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 09:11:21,710 INFO: Initializing the initial population.
2023-08-01 09:11:21,713 INFO: Running the evolutionary algorithm.
2023-08-01 09:11:21,713 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 09:11:21,713 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 09:11:21,714 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-08-01 09:11:21,714 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-08-01 09:11:22,619 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,624 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,643 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,647 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,665 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,670 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,686 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,690 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,706 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,710 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,895 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,896 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,898 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,899 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,900 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,901 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,903 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,904 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,905 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:22,906 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:11:58,992 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-08-01 09:11:58,992 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-08-01 09:12:34,412 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 09:12:34,412 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 09:13:06,276 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-08-01 09:13:06,276 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-08-01 09:13:32,626 INFO: Starting experiment with the arguments logged below.
2023-08-01 09:13:32,627 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=2048, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 09:13:32,627 INFO: Setting the random number generator seed for this experiment.
2023-08-01 09:13:32,627 INFO: Loading dataset for the experiment.
2023-08-01 09:13:33,164 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 09:13:33,165 INFO: Setting up DEAP toolbox.
2023-08-01 09:13:33,165 INFO: Registering individual initialization method.
2023-08-01 09:13:33,165 INFO: Registering population initialization method.
2023-08-01 09:13:33,165 INFO: Registering the selection method.
2023-08-01 09:13:33,165 INFO: Registering the evaluation method.
2023-08-01 09:13:33,165 INFO: Registering the genetic operators.
2023-08-01 09:13:33,165 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 09:13:33,165 INFO: Initializing the initial population.
2023-08-01 09:13:33,167 INFO: Running the evolutionary algorithm.
2023-08-01 09:13:33,167 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 09:13:33,167 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 09:13:33,168 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-08-01 09:13:33,168 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-08-01 09:13:34,054 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,058 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,076 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,080 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,097 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,101 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,117 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,121 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,137 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,141 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,326 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,328 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,329 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,330 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,332 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,333 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,334 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,335 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,337 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:34,338 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:13:41,154 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-08-01 09:13:42,413 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-08-01 09:13:42,414 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-08-01 09:13:46,880 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-08-01 09:13:48,002 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 09:13:48,003 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 09:13:51,570 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-08-01 09:13:52,602 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-08-01 09:13:52,602 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-08-01 09:13:55,134 INFO: Error reported to Coordinator: 
Traceback (most recent call last):
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/training/coordinator.py", line 293, in stop_on_exception
    yield
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/distribute/mirrored_run.py", line 277, in _call_for_each_replica
    merge_result = threads[0].merge_fn(distribution, *merge_args,
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py", line 689, in wrapper
    return converted_call(f, args, kwargs, options=options)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py", line 331, in converted_call
    return _call_unconverted(f, args, kwargs, options, False)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py", line 458, in _call_unconverted
    return f(*args, **kwargs)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1256, in _distributed_apply_gradients_fn
    self._update_model_variables_moving_average(var_list)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1234, in _update_model_variables_moving_average
    self._distribution_strategy.extended.update(
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py", line 2637, in update
    return self._update(var, fn, args, kwargs, group)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/distribute/parameter_server_strategy.py", line 551, in _update
    result = fn(var, *self._select_single_value(args),
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py", line 689, in wrapper
    return converted_call(f, args, kwargs, options=options)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py", line 331, in converted_call
    return _call_unconverted(f, args, kwargs, options, False)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py", line 458, in _call_unconverted
    return f(*args, **kwargs)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1228, in update_average
    self.ema_momentum * average + (1 - self.ema_momentum) * var
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/distribute/ps_values.py", line 255, in __rmul__
    return o * self._v
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/ops/variables.py", line 1106, in _run_op
    return tensor_oper(a.value(), *args, **kwargs)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py", line 150, in error_handler
    return fn(*args, **kwargs)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py", line 1494, in r_binary_op_wrapper
    return func(x, y, name=name)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py", line 1819, in _mul_dispatch
    return multiply(x, y, name=name)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py", line 150, in error_handler
    return fn(*args, **kwargs)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py", line 1176, in op_dispatch_handler
    return dispatch_target(*args, **kwargs)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py", line 530, in multiply
    return gen_math_ops.mul(x, y, name)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py", line 6589, in mul
    _, _, _op, _outputs = _op_def_library._apply_op_helper(
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py", line 777, in _apply_op_helper
    _ExtractInputsAndAttrs(op_type_name, op_def, allowed_list_attr_map,
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py", line 658, in _ExtractInputsAndAttrs
    _SatisfiesTypeConstraint(
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py", line 54, in _SatisfiesTypeConstraint
    allowed_values = ", ".join(dtypes.as_dtype(x).name for x in allowed_list)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py", line 54, in <genexpr>
    allowed_values = ", ".join(dtypes.as_dtype(x).name for x in allowed_list)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/framework/dtypes.py", line 805, in as_dtype
    if isinstance(type_value, DType):
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/abc.py", line 117, in __instancecheck__
    def __instancecheck__(cls, instance):
KeyboardInterrupt
2023-08-01 09:14:58,663 INFO: Starting experiment with the arguments logged below.
2023-08-01 09:14:58,663 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=1536, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 09:14:58,664 INFO: Setting the random number generator seed for this experiment.
2023-08-01 09:14:58,664 INFO: Loading dataset for the experiment.
2023-08-01 09:14:59,192 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 09:14:59,192 INFO: Setting up DEAP toolbox.
2023-08-01 09:14:59,192 INFO: Registering individual initialization method.
2023-08-01 09:14:59,192 INFO: Registering population initialization method.
2023-08-01 09:14:59,192 INFO: Registering the selection method.
2023-08-01 09:14:59,192 INFO: Registering the evaluation method.
2023-08-01 09:14:59,192 INFO: Registering the genetic operators.
2023-08-01 09:14:59,193 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 09:14:59,193 INFO: Initializing the initial population.
2023-08-01 09:14:59,195 INFO: Running the evolutionary algorithm.
2023-08-01 09:14:59,195 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 09:14:59,195 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 09:14:59,196 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-08-01 09:14:59,196 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-08-01 09:15:00,043 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,048 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,065 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,070 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,087 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,091 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,107 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,111 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,127 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,131 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,317 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,318 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,319 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,321 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,322 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,323 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,324 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,326 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,327 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:00,328 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:15:27,580 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-08-01 09:15:27,580 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-08-01 09:15:54,697 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 09:15:54,697 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 09:17:49,966 INFO: Starting experiment with the arguments logged below.
2023-08-01 09:17:49,967 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=2048, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 09:17:49,967 INFO: Setting the random number generator seed for this experiment.
2023-08-01 09:17:49,967 INFO: Loading dataset for the experiment.
2023-08-01 09:17:50,478 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 09:17:50,478 INFO: Setting up DEAP toolbox.
2023-08-01 09:17:50,478 INFO: Registering individual initialization method.
2023-08-01 09:17:50,478 INFO: Registering population initialization method.
2023-08-01 09:17:50,478 INFO: Registering the selection method.
2023-08-01 09:17:50,478 INFO: Registering the evaluation method.
2023-08-01 09:17:50,478 INFO: Registering the genetic operators.
2023-08-01 09:17:50,478 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 09:17:50,478 INFO: Initializing the initial population.
2023-08-01 09:17:50,481 INFO: Running the evolutionary algorithm.
2023-08-01 09:17:50,481 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 09:17:50,481 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 09:17:50,481 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-08-01 09:17:50,482 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-08-01 09:17:50,564 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:17:50,570 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:17:50,605 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:17:50,608 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:17:50,644 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:17:50,647 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:17:50,678 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:17:50,681 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:17:50,711 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:17:50,714 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:17:51,894 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 09:17:53,169 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 09:18:16,342 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-08-01 09:18:16,342 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-08-01 09:18:17,453 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 09:18:19,012 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 09:18:36,634 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 09:18:36,634 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 09:18:37,948 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 09:18:39,309 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 09:24:11,741 INFO: Starting experiment with the arguments logged below.
2023-08-01 09:24:11,742 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=4096, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 09:24:11,742 INFO: Setting the random number generator seed for this experiment.
2023-08-01 09:24:11,742 INFO: Loading dataset for the experiment.
2023-08-01 09:24:12,419 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 09:24:12,420 INFO: Setting up DEAP toolbox.
2023-08-01 09:24:12,420 INFO: Registering individual initialization method.
2023-08-01 09:24:12,420 INFO: Registering population initialization method.
2023-08-01 09:24:12,420 INFO: Registering the selection method.
2023-08-01 09:24:12,420 INFO: Registering the evaluation method.
2023-08-01 09:24:12,420 INFO: Registering the genetic operators.
2023-08-01 09:24:12,420 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 09:24:12,420 INFO: Initializing the initial population.
2023-08-01 09:24:12,423 INFO: Running the evolutionary algorithm.
2023-08-01 09:24:12,423 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 09:24:12,423 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 09:24:12,424 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-08-01 09:24:12,424 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-08-01 09:24:12,512 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:24:12,518 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:24:12,555 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:24:12,558 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:24:12,597 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:24:12,600 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:24:12,632 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:24:12,636 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:24:12,666 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:24:12,670 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 09:24:13,985 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 09:24:15,361 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 09:24:39,220 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-08-01 09:24:39,220 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-08-01 09:24:40,314 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 09:24:41,767 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 09:24:58,537 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 09:24:58,537 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 09:24:59,863 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 09:25:01,245 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 11:53:30,704 INFO: Starting experiment with the arguments logged below.
2023-08-01 11:53:30,722 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 11:53:30,723 INFO: Setting the random number generator seed for this experiment.
2023-08-01 11:53:30,723 INFO: Loading dataset for the experiment.
2023-08-01 11:54:55,973 INFO: Starting experiment with the arguments logged below.
2023-08-01 11:54:55,974 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 11:54:55,974 INFO: Setting the random number generator seed for this experiment.
2023-08-01 11:54:55,974 INFO: Loading dataset for the experiment.
2023-08-01 11:54:56,474 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 11:54:56,474 INFO: Setting up DEAP toolbox.
2023-08-01 11:54:56,474 INFO: Registering individual initialization method.
2023-08-01 11:54:56,474 INFO: Registering population initialization method.
2023-08-01 11:54:56,474 INFO: Registering the selection method.
2023-08-01 11:54:56,474 INFO: Registering the evaluation method.
2023-08-01 11:54:56,474 INFO: Registering the genetic operators.
2023-08-01 11:54:56,474 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 11:54:56,474 INFO: Initializing the initial population.
2023-08-01 11:54:56,476 INFO: Running the evolutionary algorithm.
2023-08-01 11:54:56,476 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 11:54:56,477 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 11:54:56,477 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-08-01 11:54:56,477 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-08-01 11:54:56,545 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:54:56,549 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:54:56,572 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:54:56,575 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:54:56,598 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:54:56,600 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:54:56,620 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:54:56,622 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:54:56,641 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:54:56,643 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:55:25,758 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-08-01 11:55:25,759 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-08-01 11:55:49,370 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 11:55:49,370 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 11:56:12,503 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-08-01 11:56:12,504 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-08-01 11:56:47,163 INFO: Starting experiment with the arguments logged below.
2023-08-01 11:56:47,163 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=2048, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 11:56:47,163 INFO: Setting the random number generator seed for this experiment.
2023-08-01 11:56:47,163 INFO: Loading dataset for the experiment.
2023-08-01 11:56:47,655 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 11:56:47,655 INFO: Setting up DEAP toolbox.
2023-08-01 11:56:47,655 INFO: Registering individual initialization method.
2023-08-01 11:56:47,655 INFO: Registering population initialization method.
2023-08-01 11:56:47,655 INFO: Registering the selection method.
2023-08-01 11:56:47,655 INFO: Registering the evaluation method.
2023-08-01 11:56:47,655 INFO: Registering the genetic operators.
2023-08-01 11:56:47,655 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 11:56:47,655 INFO: Initializing the initial population.
2023-08-01 11:56:47,658 INFO: Running the evolutionary algorithm.
2023-08-01 11:56:47,658 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 11:56:47,658 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 11:56:47,658 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-08-01 11:56:47,659 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-08-01 11:56:47,715 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:56:47,718 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:56:47,742 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:56:47,744 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:56:47,767 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:56:47,769 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:56:47,789 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:56:47,791 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:56:47,810 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:56:47,812 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:57:15,963 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-08-01 11:57:15,963 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-08-01 11:57:38,008 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 11:57:38,009 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 11:57:59,557 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-08-01 11:57:59,557 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.6788678867886789, momentum=0.2834283428342834 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8535853585358536 ,rho=0.6443644364436444 ,epsilon=0.0006070607453645365 ,centered=False ,beta_1=0.0484048404840484 ,beta_2=0.768976897689769 ,learning_rate_power=-0.9287928792879288 ,initial_accumulator_value=0.5054505450545055 ,l1_regularization_strength=0.9718971897189719 ,l2_regularization_strength=0.9472947294729473 ,l2_shrinkage_regularization_strength=0.6448644864486449 ,beta=0.27912791279127913].
2023-08-01 11:58:35,727 INFO: Starting experiment with the arguments logged below.
2023-08-01 11:58:35,727 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 11:58:35,727 INFO: Setting the random number generator seed for this experiment.
2023-08-01 11:58:35,727 INFO: Loading dataset for the experiment.
2023-08-01 11:58:36,218 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 11:58:36,218 INFO: Setting up DEAP toolbox.
2023-08-01 11:58:36,218 INFO: Registering individual initialization method.
2023-08-01 11:58:36,218 INFO: Registering population initialization method.
2023-08-01 11:58:36,218 INFO: Registering the selection method.
2023-08-01 11:58:36,218 INFO: Registering the evaluation method.
2023-08-01 11:58:36,218 INFO: Registering the genetic operators.
2023-08-01 11:58:36,218 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 11:58:36,218 INFO: Initializing the initial population.
2023-08-01 11:58:36,220 INFO: Running the evolutionary algorithm.
2023-08-01 11:58:36,220 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 11:58:36,220 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 11:58:36,221 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-08-01 11:58:36,221 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9325932593259326, momentum=0.10331033103310332 ,nesterov=True ,amsgrad=False ,weight_decay=0.007365000000000002 ,use_ema=False ,ema_momentum=0.34393439343934396 ,rho=0.15371537153715373 ,epsilon=0.0007993799580558056 ,centered=True ,beta_1=0.6386638663866386 ,beta_2=0.7090709070907091 ,learning_rate_power=-0.00470047004700469 ,initial_accumulator_value=0.0034003400340034003 ,l1_regularization_strength=0.7297729772977298 ,l2_regularization_strength=0.4363436343634364 ,l2_shrinkage_regularization_strength=0.37483748374837483 ,beta=0.9685968596859686].
2023-08-01 11:58:36,278 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:58:36,281 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:58:36,303 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:58:36,306 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:58:36,328 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:58:36,330 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:58:36,350 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:58:36,352 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:58:36,370 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:58:36,372 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 11:59:04,523 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-08-01 11:59:04,523 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.5200520052005201, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.04750475047504751 ,rho=0.8644864486448646 ,epsilon=0.00036323638730873084 ,centered=False ,beta_1=0.8123812381238124 ,beta_2=0.905890589058906 ,learning_rate_power=-0.6181618161816181 ,initial_accumulator_value=0.5663566356635664 ,l1_regularization_strength=0.37823782378237825 ,l2_regularization_strength=0.3584358435843584 ,l2_shrinkage_regularization_strength=0.7530753075307531 ,beta=0.47474747474747475].
2023-08-01 11:59:27,891 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 11:59:27,891 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6818681868186819, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0019810000000000006 ,use_ema=False ,ema_momentum=0.8318831883188319 ,rho=0.311031103110311 ,epsilon=0.0004970497552655266 ,centered=False ,beta_1=0.9626962696269628 ,beta_2=0.8181818181818182 ,learning_rate_power=-0.17211721172117211 ,initial_accumulator_value=0.6444644464446445 ,l1_regularization_strength=0.9650965096509652 ,l2_regularization_strength=0.056505650565056506 ,l2_shrinkage_regularization_strength=0.7868786878687869 ,beta=0.39773977397739774].
2023-08-01 14:18:06,078 INFO: Starting experiment with the arguments logged below.
2023-08-01 14:18:06,106 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 14:18:06,106 INFO: Setting the random number generator seed for this experiment.
2023-08-01 14:18:06,106 INFO: Loading dataset for the experiment.
2023-08-01 14:18:06,626 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 14:18:06,626 INFO: Setting up DEAP toolbox.
2023-08-01 14:18:06,626 INFO: Registering individual initialization method.
2023-08-01 14:18:06,626 INFO: Registering population initialization method.
2023-08-01 14:18:06,626 INFO: Registering the selection method.
2023-08-01 14:18:06,626 INFO: Registering the evaluation method.
2023-08-01 14:18:06,626 INFO: Registering the genetic operators.
2023-08-01 14:18:06,626 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 14:18:06,626 INFO: Initializing the initial population.
2023-08-01 14:18:06,632 INFO: Running the evolutionary algorithm.
2023-08-01 14:18:06,632 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 14:18:06,632 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 14:18:06,633 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.37901473007826336, momentum=0.3551123592269153 ,nesterov=True ,amsgrad=False ,weight_decay=0.005336504925596363 ,use_ema=False ,ema_momentum=0.3961376246631869 ,rho=0.4798228365215089 ,epsilon=0.0005150698177103618 ,centered=True ,beta_1=0.036292793272961754 ,beta_2=0.6476049338565967 ,learning_rate_power=-0.36898348506954626 ,initial_accumulator_value=0.11489566438390053 ,l1_regularization_strength=0.8428883565754746 ,l2_regularization_strength=0.763808525337135 ,l2_shrinkage_regularization_strength=0.5002730746540524 ,beta=0.14548283303607912].
2023-08-01 14:18:06,633 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.37901473007826336, momentum=0.3551123592269153 ,nesterov=True ,amsgrad=False ,weight_decay=0.005336504925596363 ,use_ema=False ,ema_momentum=0.3961376246631869 ,rho=0.4798228365215089 ,epsilon=0.0005150698177103618 ,centered=True ,beta_1=0.036292793272961754 ,beta_2=0.6476049338565967 ,learning_rate_power=-0.36898348506954626 ,initial_accumulator_value=0.11489566438390053 ,l1_regularization_strength=0.8428883565754746 ,l2_regularization_strength=0.763808525337135 ,l2_shrinkage_regularization_strength=0.5002730746540524 ,beta=0.14548283303607912].
2023-08-01 14:18:06,720 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:18:06,725 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:18:06,758 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:18:06,762 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:18:06,796 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:18:06,799 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:18:06,826 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:18:06,830 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:18:06,856 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:18:06,860 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:18:08,021 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:18:09,396 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:18:36,972 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.9265809893332791, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.7384120103567502 ,rho=0.5134519306428085 ,epsilon=7.23164539866441e-05 ,centered=False ,beta_1=0.18797375329450616 ,beta_2=0.109598534214152 ,learning_rate_power=-0.9848721417377507 ,initial_accumulator_value=0.6323336546356001 ,l1_regularization_strength=0.5140058429484361 ,l2_regularization_strength=0.11919977996851061 ,l2_shrinkage_regularization_strength=0.21121757770745486 ,beta=0.6954100120676004].
2023-08-01 14:18:36,973 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.9265809893332791, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.7384120103567502 ,rho=0.5134519306428085 ,epsilon=7.23164539866441e-05 ,centered=False ,beta_1=0.18797375329450616 ,beta_2=0.109598534214152 ,learning_rate_power=-0.9848721417377507 ,initial_accumulator_value=0.6323336546356001 ,l1_regularization_strength=0.5140058429484361 ,l2_regularization_strength=0.11919977996851061 ,l2_shrinkage_regularization_strength=0.21121757770745486 ,beta=0.6954100120676004].
2023-08-01 14:18:38,124 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:18:39,773 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:19:00,353 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.22715362647055037, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=8.58798272360094e-05 ,use_ema=False ,ema_momentum=0.00739288308856334 ,rho=0.569123888943864 ,epsilon=0.0006806704542745859 ,centered=False ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.9385694459824191 ,l2_shrinkage_regularization_strength=0.31189701645451784 ,beta=0.499455573778502].
2023-08-01 14:19:00,354 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.22715362647055037, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=8.58798272360094e-05 ,use_ema=False ,ema_momentum=0.00739288308856334 ,rho=0.569123888943864 ,epsilon=0.0006806704542745859 ,centered=False ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.9385694459824191 ,l2_shrinkage_regularization_strength=0.31189701645451784 ,beta=0.499455573778502].
2023-08-01 14:19:01,730 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:19:03,209 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:19:23,375 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.6986943112070445, momentum=0.8066357228660451 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.09126229755207826 ,rho=0.1968003782279264 ,epsilon=0.0006748568680467847 ,centered=False ,beta_1=0.8143151348892413 ,beta_2=0.4623544498455916 ,learning_rate_power=-0.16220974717828296 ,initial_accumulator_value=0.6466575553009105 ,l1_regularization_strength=0.7033917340278286 ,l2_regularization_strength=0.6521238420657436 ,l2_shrinkage_regularization_strength=0.4140088577841926 ,beta=0.5642063886534525].
2023-08-01 14:19:23,375 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.6986943112070445, momentum=0.8066357228660451 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.09126229755207826 ,rho=0.1968003782279264 ,epsilon=0.0006748568680467847 ,centered=False ,beta_1=0.8143151348892413 ,beta_2=0.4623544498455916 ,learning_rate_power=-0.16220974717828296 ,initial_accumulator_value=0.6466575553009105 ,l1_regularization_strength=0.7033917340278286 ,l2_regularization_strength=0.6521238420657436 ,l2_shrinkage_regularization_strength=0.4140088577841926 ,beta=0.5642063886534525].
2023-08-01 14:19:24,545 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:19:27,925 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:19:55,871 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.002856741923265888 ,use_ema=False ,ema_momentum=0.16648471738537862 ,rho=0.6336847586851185 ,epsilon=0.000789286796541743 ,centered=True ,beta_1=0.3253541435706545 ,beta_2=0.38868334607318156 ,learning_rate_power=-0.23153003645962378 ,initial_accumulator_value=0.6523718319816098 ,l1_regularization_strength=0.7844687725744196 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:19:55,871 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.002856741923265888 ,use_ema=False ,ema_momentum=0.16648471738537862 ,rho=0.6336847586851185 ,epsilon=0.000789286796541743 ,centered=True ,beta_1=0.3253541435706545 ,beta_2=0.38868334607318156 ,learning_rate_power=-0.23153003645962378 ,initial_accumulator_value=0.6523718319816098 ,l1_regularization_strength=0.7844687725744196 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:19:56,993 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:19:58,323 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:20:19,859 INFO: Applying selection operators for generation 1.
2023-08-01 14:20:19,859 INFO: Applying genetic operators for generation 1.
2023-08-01 14:20:19,860 INFO: Evaluating fitness for for generation 1.
2023-08-01 14:20:19,860 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 14:20:19,861 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.37901473007826336, momentum=0.3551123592269153 ,nesterov=True ,amsgrad=False ,weight_decay=0.005336504925596363 ,use_ema=False ,ema_momentum=0.00739288308856334 ,rho=0.569123888943864 ,epsilon=0.0006806704542745859 ,centered=False ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.9385694459824191 ,l2_shrinkage_regularization_strength=0.5002730746540524 ,beta=0.14548283303607912].
2023-08-01 14:20:19,861 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.37901473007826336, momentum=0.3551123592269153 ,nesterov=True ,amsgrad=False ,weight_decay=0.005336504925596363 ,use_ema=False ,ema_momentum=0.00739288308856334 ,rho=0.569123888943864 ,epsilon=0.0006806704542745859 ,centered=False ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.9385694459824191 ,l2_shrinkage_regularization_strength=0.5002730746540524 ,beta=0.14548283303607912].
2023-08-01 14:20:43,617 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.22715362647055037, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=8.58798272360094e-05 ,use_ema=False ,ema_momentum=0.3961376246631869 ,rho=0.4798228365215089 ,epsilon=0.0005150698177103618 ,centered=True ,beta_1=0.036292793272961754 ,beta_2=0.6476049338565967 ,learning_rate_power=-0.36898348506954626 ,initial_accumulator_value=0.11489566438390053 ,l1_regularization_strength=0.8428883565754746 ,l2_regularization_strength=0.763808525337135 ,l2_shrinkage_regularization_strength=0.31189701645451784 ,beta=0.499455573778502].
2023-08-01 14:20:43,618 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.22715362647055037, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=8.58798272360094e-05 ,use_ema=False ,ema_momentum=0.3961376246631869 ,rho=0.4798228365215089 ,epsilon=0.0005150698177103618 ,centered=True ,beta_1=0.036292793272961754 ,beta_2=0.6476049338565967 ,learning_rate_power=-0.36898348506954626 ,initial_accumulator_value=0.11489566438390053 ,l1_regularization_strength=0.8428883565754746 ,l2_regularization_strength=0.763808525337135 ,l2_shrinkage_regularization_strength=0.31189701645451784 ,beta=0.499455573778502].
2023-08-01 14:21:05,520 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.771127306418945, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8293276898663858 ,rho=0.569123888943864 ,epsilon=0.0006806704542745859 ,centered=False ,beta_1=0.2218408889662017 ,beta_2=0.3916302939834647 ,learning_rate_power=-0.23153003645962378 ,initial_accumulator_value=0.6523718319816098 ,l1_regularization_strength=0.49696119892014123 ,l2_regularization_strength=0.9385694459824191 ,l2_shrinkage_regularization_strength=0.40958170017155293 ,beta=0.965300715656446].
2023-08-01 14:21:05,521 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.771127306418945, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8293276898663858 ,rho=0.569123888943864 ,epsilon=0.0006806704542745859 ,centered=False ,beta_1=0.2218408889662017 ,beta_2=0.3916302939834647 ,learning_rate_power=-0.23153003645962378 ,initial_accumulator_value=0.6523718319816098 ,l1_regularization_strength=0.49696119892014123 ,l2_regularization_strength=0.9385694459824191 ,l2_shrinkage_regularization_strength=0.40958170017155293 ,beta=0.965300715656446].
2023-08-01 14:21:30,293 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.002856741923265888 ,use_ema=False ,ema_momentum=0.16648471738537862 ,rho=0.6336847586851185 ,epsilon=0.000789286796541743 ,centered=False ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:21:30,293 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.002856741923265888 ,use_ema=False ,ema_momentum=0.16648471738537862 ,rho=0.6336847586851185 ,epsilon=0.000789286796541743 ,centered=False ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:21:53,069 INFO: Applying selection operators for generation 2.
2023-08-01 14:21:53,069 INFO: Applying genetic operators for generation 2.
2023-08-01 14:21:53,069 INFO: Evaluating fitness for for generation 2.
2023-08-01 14:21:53,070 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 14:21:53,070 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.22715362647055037, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.002856741923265888 ,use_ema=False ,ema_momentum=0.16648471738537862 ,rho=0.6336847586851185 ,epsilon=0.000789286796541743 ,centered=False ,beta_1=0.036292793272961754 ,beta_2=0.6476049338565967 ,learning_rate_power=-0.36898348506954626 ,initial_accumulator_value=0.11489566438390053 ,l1_regularization_strength=0.8428883565754746 ,l2_regularization_strength=0.763808525337135 ,l2_shrinkage_regularization_strength=0.31189701645451784 ,beta=0.499455573778502].
2023-08-01 14:21:53,070 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.22715362647055037, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.002856741923265888 ,use_ema=False ,ema_momentum=0.16648471738537862 ,rho=0.6336847586851185 ,epsilon=0.000789286796541743 ,centered=False ,beta_1=0.036292793272961754 ,beta_2=0.6476049338565967 ,learning_rate_power=-0.36898348506954626 ,initial_accumulator_value=0.11489566438390053 ,l1_regularization_strength=0.8428883565754746 ,l2_regularization_strength=0.763808525337135 ,l2_shrinkage_regularization_strength=0.31189701645451784 ,beta=0.499455573778502].
2023-08-01 14:22:15,128 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=8.58798272360094e-05 ,use_ema=False ,ema_momentum=0.3961376246631869 ,rho=0.4798228365215089 ,epsilon=0.0005150698177103618 ,centered=True ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:22:15,129 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=8.58798272360094e-05 ,use_ema=False ,ema_momentum=0.3961376246631869 ,rho=0.4798228365215089 ,epsilon=0.0005150698177103618 ,centered=True ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:22:37,968 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.002856741923265888 ,use_ema=False ,ema_momentum=0.00739288308856334 ,rho=0.569123888943864 ,epsilon=0.0006806704542745859 ,centered=False ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:22:37,968 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.002856741923265888 ,use_ema=False ,ema_momentum=0.00739288308856334 ,rho=0.569123888943864 ,epsilon=0.0006806704542745859 ,centered=False ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:23:00,917 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.22715362647055037, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=8.58798272360094e-05 ,use_ema=False ,ema_momentum=0.16648471738537862 ,rho=0.6336847586851185 ,epsilon=0.000789286796541743 ,centered=False ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.9385694459824191 ,l2_shrinkage_regularization_strength=0.31189701645451784 ,beta=0.499455573778502].
2023-08-01 14:23:00,917 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.22715362647055037, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=8.58798272360094e-05 ,use_ema=False ,ema_momentum=0.16648471738537862 ,rho=0.6336847586851185 ,epsilon=0.000789286796541743 ,centered=False ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.9385694459824191 ,l2_shrinkage_regularization_strength=0.31189701645451784 ,beta=0.499455573778502].
2023-08-01 14:23:23,217 INFO: Applying selection operators for generation 3.
2023-08-01 14:23:23,217 INFO: Applying genetic operators for generation 3.
2023-08-01 14:23:23,218 INFO: Evaluating fitness for for generation 3.
2023-08-01 14:23:23,218 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 14:23:23,218 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.002856741923265888 ,use_ema=False ,ema_momentum=0.00739288308856334 ,rho=0.569123888943864 ,epsilon=0.0006806704542745859 ,centered=False ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:23:23,218 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.002856741923265888 ,use_ema=False ,ema_momentum=0.00739288308856334 ,rho=0.569123888943864 ,epsilon=0.0006806704542745859 ,centered=False ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:23:46,239 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=8.58798272360094e-05 ,use_ema=False ,ema_momentum=0.3961376246631869 ,rho=0.4798228365215089 ,epsilon=0.0005150698177103618 ,centered=True ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:23:46,240 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=8.58798272360094e-05 ,use_ema=False ,ema_momentum=0.3961376246631869 ,rho=0.4798228365215089 ,epsilon=0.0005150698177103618 ,centered=True ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:24:09,242 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.22715362647055037, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=8.58798272360094e-05 ,use_ema=False ,ema_momentum=0.3961376246631869 ,rho=0.6336847586851185 ,epsilon=0.000789286796541743 ,centered=False ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.9385694459824191 ,l2_shrinkage_regularization_strength=0.31189701645451784 ,beta=0.499455573778502].
2023-08-01 14:24:09,243 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.22715362647055037, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=8.58798272360094e-05 ,use_ema=False ,ema_momentum=0.3961376246631869 ,rho=0.6336847586851185 ,epsilon=0.000789286796541743 ,centered=False ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.9385694459824191 ,l2_shrinkage_regularization_strength=0.31189701645451784 ,beta=0.499455573778502].
2023-08-01 14:24:31,006 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=8.58798272360094e-05 ,use_ema=False ,ema_momentum=0.16648471738537862 ,rho=0.4798228365215089 ,epsilon=0.0005150698177103618 ,centered=True ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:24:31,006 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=8.58798272360094e-05 ,use_ema=False ,ema_momentum=0.16648471738537862 ,rho=0.4798228365215089 ,epsilon=0.0005150698177103618 ,centered=True ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:24:53,854 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.006231301554702973, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.002856741923265888 ,use_ema=False ,ema_momentum=0.4748823996188015 ,rho=0.24310047194468876 ,epsilon=0.0007276295362427771 ,centered=True ,beta_1=0.9830114634593039 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.394670007189863 ,beta=0.00023032711943882234].
2023-08-01 14:24:53,855 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.006231301554702973, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.002856741923265888 ,use_ema=False ,ema_momentum=0.4748823996188015 ,rho=0.24310047194468876 ,epsilon=0.0007276295362427771 ,centered=True ,beta_1=0.9830114634593039 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.394670007189863 ,beta=0.00023032711943882234].
2023-08-01 14:25:18,020 INFO: Applying selection operators for generation 4.
2023-08-01 14:25:18,020 INFO: Applying genetic operators for generation 4.
2023-08-01 14:25:18,021 INFO: Evaluating fitness for for generation 4.
2023-08-01 14:25:18,021 INFO: Will evaluate fitness for 3 individuals.
2023-08-01 14:25:18,021 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.002856741923265888 ,use_ema=False ,ema_momentum=0.00739288308856334 ,rho=0.569123888943864 ,epsilon=0.0006806704542745859 ,centered=False ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:25:18,021 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.002856741923265888 ,use_ema=False ,ema_momentum=0.00739288308856334 ,rho=0.569123888943864 ,epsilon=0.0006806704542745859 ,centered=False ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:25:41,022 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=8.58798272360094e-05 ,use_ema=False ,ema_momentum=0.16648471738537862 ,rho=0.4798228365215089 ,epsilon=0.0005150698177103618 ,centered=True ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:25:41,022 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.19594629704653743, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=8.58798272360094e-05 ,use_ema=False ,ema_momentum=0.16648471738537862 ,rho=0.4798228365215089 ,epsilon=0.0005150698177103618 ,centered=True ,beta_1=0.5250686909075523 ,beta_2=0.03499604483739738 ,learning_rate_power=-0.40088810999972746 ,initial_accumulator_value=0.00567221898825343 ,l1_regularization_strength=0.4586580194447951 ,l2_regularization_strength=0.8323121406994322 ,l2_shrinkage_regularization_strength=0.5381034903420896 ,beta=0.00023032711943882234].
2023-08-01 14:26:07,929 INFO: Starting experiment with the arguments logged below.
2023-08-01 14:26:07,929 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 14:26:07,929 INFO: Setting the random number generator seed for this experiment.
2023-08-01 14:26:07,929 INFO: Loading dataset for the experiment.
2023-08-01 14:26:08,449 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 14:26:08,449 INFO: Setting up DEAP toolbox.
2023-08-01 14:26:08,450 INFO: Registering individual initialization method.
2023-08-01 14:26:08,450 INFO: Registering population initialization method.
2023-08-01 14:26:08,450 INFO: Registering the selection method.
2023-08-01 14:26:08,450 INFO: Registering the evaluation method.
2023-08-01 14:26:08,450 INFO: Registering the genetic operators.
2023-08-01 14:26:08,450 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 14:26:08,450 INFO: Initializing the initial population.
2023-08-01 14:26:08,456 INFO: Running the evolutionary algorithm.
2023-08-01 14:26:08,456 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 14:26:08,456 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 14:26:08,457 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.3793910665223593, momentum=0.5830072805159547 ,nesterov=True ,amsgrad=False ,weight_decay=0.009367150129303888 ,use_ema=False ,ema_momentum=0.2652913981117031 ,rho=0.23314783892564028 ,epsilon=0.0006904499007781096 ,centered=True ,beta_1=0.26221593779433217 ,beta_2=0.09878031559426537 ,learning_rate_power=-0.882301294433799 ,initial_accumulator_value=0.16509984600758953 ,l1_regularization_strength=0.7617468467034191 ,l2_regularization_strength=0.658003585642471 ,l2_shrinkage_regularization_strength=0.050203440514947295 ,beta=0.09837135738924718].
2023-08-01 14:26:08,457 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.3793910665223593, momentum=0.5830072805159547 ,nesterov=True ,amsgrad=False ,weight_decay=0.009367150129303888 ,use_ema=False ,ema_momentum=0.2652913981117031 ,rho=0.23314783892564028 ,epsilon=0.0006904499007781096 ,centered=True ,beta_1=0.26221593779433217 ,beta_2=0.09878031559426537 ,learning_rate_power=-0.882301294433799 ,initial_accumulator_value=0.16509984600758953 ,l1_regularization_strength=0.7617468467034191 ,l2_regularization_strength=0.658003585642471 ,l2_shrinkage_regularization_strength=0.050203440514947295 ,beta=0.09837135738924718].
2023-08-01 14:26:08,541 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:26:08,546 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:26:08,583 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:26:08,586 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:26:08,623 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:26:08,627 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:26:08,656 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:26:08,660 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:26:08,689 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:26:08,692 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:26:09,974 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:26:11,356 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:26:38,373 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.051120310723679285, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8451375717203238 ,rho=0.707086180114632 ,epsilon=0.0007800625297525084 ,centered=False ,beta_1=0.025172611668176748 ,beta_2=0.4061705348062866 ,learning_rate_power=-0.1268475829225849 ,initial_accumulator_value=0.00013752783874820818 ,l1_regularization_strength=0.01987301276964537 ,l2_regularization_strength=0.09486660245776202 ,l2_shrinkage_regularization_strength=0.4110227211817291 ,beta=0.597409114188568].
2023-08-01 14:26:38,373 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.051120310723679285, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8451375717203238 ,rho=0.707086180114632 ,epsilon=0.0007800625297525084 ,centered=False ,beta_1=0.025172611668176748 ,beta_2=0.4061705348062866 ,learning_rate_power=-0.1268475829225849 ,initial_accumulator_value=0.00013752783874820818 ,l1_regularization_strength=0.01987301276964537 ,l2_regularization_strength=0.09486660245776202 ,l2_shrinkage_regularization_strength=0.4110227211817291 ,beta=0.597409114188568].
2023-08-01 14:26:39,549 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:26:41,204 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:27:02,496 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6702345600834907, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0031822101813669557 ,use_ema=False ,ema_momentum=0.7415215177352918 ,rho=0.6521327110721173 ,epsilon=0.0004497543285594625 ,centered=False ,beta_1=0.28614294476568325 ,beta_2=0.9942285657988178 ,learning_rate_power=-0.7738772238327366 ,initial_accumulator_value=0.29168161517552493 ,l1_regularization_strength=0.07958555531246247 ,l2_regularization_strength=0.36534377729380485 ,l2_shrinkage_regularization_strength=0.40894378439813894 ,beta=0.9078326916065143].
2023-08-01 14:27:02,496 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6702345600834907, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0031822101813669557 ,use_ema=False ,ema_momentum=0.7415215177352918 ,rho=0.6521327110721173 ,epsilon=0.0004497543285594625 ,centered=False ,beta_1=0.28614294476568325 ,beta_2=0.9942285657988178 ,learning_rate_power=-0.7738772238327366 ,initial_accumulator_value=0.29168161517552493 ,l1_regularization_strength=0.07958555531246247 ,l2_regularization_strength=0.36534377729380485 ,l2_shrinkage_regularization_strength=0.40894378439813894 ,beta=0.9078326916065143].
2023-08-01 14:27:03,937 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:27:05,394 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:27:26,037 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.46043914296457067, momentum=0.0335553552394966 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9120485053164369 ,rho=0.3231163810735046 ,epsilon=1.744920511567462e-06 ,centered=False ,beta_1=0.3805384512751595 ,beta_2=0.8796018010470213 ,learning_rate_power=-0.4590044496450141 ,initial_accumulator_value=0.7329280349633744 ,l1_regularization_strength=0.45841367988320714 ,l2_regularization_strength=0.5758350855176683 ,l2_shrinkage_regularization_strength=0.7992363276250174 ,beta=0.21058074799068216].
2023-08-01 14:27:26,038 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.46043914296457067, momentum=0.0335553552394966 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9120485053164369 ,rho=0.3231163810735046 ,epsilon=1.744920511567462e-06 ,centered=False ,beta_1=0.3805384512751595 ,beta_2=0.8796018010470213 ,learning_rate_power=-0.4590044496450141 ,initial_accumulator_value=0.7329280349633744 ,l1_regularization_strength=0.45841367988320714 ,l2_regularization_strength=0.5758350855176683 ,l2_shrinkage_regularization_strength=0.7992363276250174 ,beta=0.21058074799068216].
2023-08-01 14:27:27,199 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:27:30,653 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:27:59,810 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.3572148071058192, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0006054043731187586 ,use_ema=False ,ema_momentum=0.06892455457281998 ,rho=0.06262486956831126 ,epsilon=0.0008066571293801032 ,centered=True ,beta_1=0.5552914559547839 ,beta_2=0.28487789013346587 ,learning_rate_power=-0.9884545131115026 ,initial_accumulator_value=0.9895037326228412 ,l1_regularization_strength=0.839704230885429 ,l2_regularization_strength=0.25838916597487815 ,l2_shrinkage_regularization_strength=0.3809804387220971 ,beta=0.6524120974008802].
2023-08-01 14:27:59,810 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.3572148071058192, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0006054043731187586 ,use_ema=False ,ema_momentum=0.06892455457281998 ,rho=0.06262486956831126 ,epsilon=0.0008066571293801032 ,centered=True ,beta_1=0.5552914559547839 ,beta_2=0.28487789013346587 ,learning_rate_power=-0.9884545131115026 ,initial_accumulator_value=0.9895037326228412 ,l1_regularization_strength=0.839704230885429 ,l2_regularization_strength=0.25838916597487815 ,l2_shrinkage_regularization_strength=0.3809804387220971 ,beta=0.6524120974008802].
2023-08-01 14:28:00,983 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:28:02,317 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 14:28:24,877 INFO: Applying selection operators for generation 1.
2023-08-01 14:28:24,877 INFO: Applying genetic operators for generation 1.
2023-08-01 14:28:24,878 INFO: Evaluating fitness for for generation 1.
2023-08-01 14:28:24,878 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 14:28:24,878 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.3793910665223593, momentum=0.5830072805159547 ,nesterov=True ,amsgrad=False ,weight_decay=0.009367150129303888 ,use_ema=False ,ema_momentum=0.7415215177352918 ,rho=0.6521327110721173 ,epsilon=0.0004497543285594625 ,centered=False ,beta_1=0.28614294476568325 ,beta_2=0.9942285657988178 ,learning_rate_power=-0.7738772238327366 ,initial_accumulator_value=0.29168161517552493 ,l1_regularization_strength=0.07958555531246247 ,l2_regularization_strength=0.36534377729380485 ,l2_shrinkage_regularization_strength=0.050203440514947295 ,beta=0.09837135738924718].
2023-08-01 14:28:24,879 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.3793910665223593, momentum=0.5830072805159547 ,nesterov=True ,amsgrad=False ,weight_decay=0.009367150129303888 ,use_ema=False ,ema_momentum=0.7415215177352918 ,rho=0.6521327110721173 ,epsilon=0.0004497543285594625 ,centered=False ,beta_1=0.28614294476568325 ,beta_2=0.9942285657988178 ,learning_rate_power=-0.7738772238327366 ,initial_accumulator_value=0.29168161517552493 ,l1_regularization_strength=0.07958555531246247 ,l2_regularization_strength=0.36534377729380485 ,l2_shrinkage_regularization_strength=0.050203440514947295 ,beta=0.09837135738924718].
2023-08-01 14:28:49,919 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6702345600834907, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0031822101813669557 ,use_ema=False ,ema_momentum=0.2652913981117031 ,rho=0.23314783892564028 ,epsilon=0.0006904499007781096 ,centered=True ,beta_1=0.26221593779433217 ,beta_2=0.09878031559426537 ,learning_rate_power=-0.882301294433799 ,initial_accumulator_value=0.16509984600758953 ,l1_regularization_strength=0.7617468467034191 ,l2_regularization_strength=0.658003585642471 ,l2_shrinkage_regularization_strength=0.40894378439813894 ,beta=0.9078326916065143].
2023-08-01 14:28:49,919 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6702345600834907, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0031822101813669557 ,use_ema=False ,ema_momentum=0.2652913981117031 ,rho=0.23314783892564028 ,epsilon=0.0006904499007781096 ,centered=True ,beta_1=0.26221593779433217 ,beta_2=0.09878031559426537 ,learning_rate_power=-0.882301294433799 ,initial_accumulator_value=0.16509984600758953 ,l1_regularization_strength=0.7617468467034191 ,l2_regularization_strength=0.658003585642471 ,l2_shrinkage_regularization_strength=0.40894378439813894 ,beta=0.9078326916065143].
2023-08-01 14:29:12,590 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.3644087600620438, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9009551173890138 ,rho=0.707086180114632 ,epsilon=0.0007800625297525084 ,centered=False ,beta_1=0.8789086376427995 ,beta_2=0.22107243511440744 ,learning_rate_power=-0.1268475829225849 ,initial_accumulator_value=0.00013752783874820818 ,l1_regularization_strength=0.40404008312860706 ,l2_regularization_strength=0.09486660245776202 ,l2_shrinkage_regularization_strength=0.4976698799163092 ,beta=0.6447064446106531].
2023-08-01 14:29:12,590 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.3644087600620438, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9009551173890138 ,rho=0.707086180114632 ,epsilon=0.0007800625297525084 ,centered=False ,beta_1=0.8789086376427995 ,beta_2=0.22107243511440744 ,learning_rate_power=-0.1268475829225849 ,initial_accumulator_value=0.00013752783874820818 ,l1_regularization_strength=0.40404008312860706 ,l2_regularization_strength=0.09486660245776202 ,l2_shrinkage_regularization_strength=0.4976698799163092 ,beta=0.6447064446106531].
2023-08-01 14:29:38,557 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.051120310723679285, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8451375717203238 ,rho=0.707086180114632 ,epsilon=0.0007800625297525084 ,centered=False ,beta_1=0.025172611668176748 ,beta_2=0.4061705348062866 ,learning_rate_power=-0.1268475829225849 ,initial_accumulator_value=0.00013752783874820818 ,l1_regularization_strength=0.01987301276964537 ,l2_regularization_strength=0.09486660245776202 ,l2_shrinkage_regularization_strength=0.4110227211817291 ,beta=0.597409114188568].
2023-08-01 14:29:38,557 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.051120310723679285, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8451375717203238 ,rho=0.707086180114632 ,epsilon=0.0007800625297525084 ,centered=False ,beta_1=0.025172611668176748 ,beta_2=0.4061705348062866 ,learning_rate_power=-0.1268475829225849 ,initial_accumulator_value=0.00013752783874820818 ,l1_regularization_strength=0.01987301276964537 ,l2_regularization_strength=0.09486660245776202 ,l2_shrinkage_regularization_strength=0.4110227211817291 ,beta=0.597409114188568].
2023-08-01 14:30:01,752 INFO: Applying selection operators for generation 2.
2023-08-01 14:30:01,753 INFO: Applying genetic operators for generation 2.
2023-08-01 14:30:01,753 INFO: Evaluating fitness for for generation 2.
2023-08-01 14:30:01,753 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 14:30:01,753 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.051120310723679285, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8451375717203238 ,rho=0.707086180114632 ,epsilon=0.0007800625297525084 ,centered=False ,beta_1=0.025172611668176748 ,beta_2=0.4061705348062866 ,learning_rate_power=-0.1268475829225849 ,initial_accumulator_value=0.00013752783874820818 ,l1_regularization_strength=0.01987301276964537 ,l2_regularization_strength=0.09486660245776202 ,l2_shrinkage_regularization_strength=0.4110227211817291 ,beta=0.597409114188568].
2023-08-01 14:30:01,753 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.051120310723679285, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8451375717203238 ,rho=0.707086180114632 ,epsilon=0.0007800625297525084 ,centered=False ,beta_1=0.025172611668176748 ,beta_2=0.4061705348062866 ,learning_rate_power=-0.1268475829225849 ,initial_accumulator_value=0.00013752783874820818 ,l1_regularization_strength=0.01987301276964537 ,l2_regularization_strength=0.09486660245776202 ,l2_shrinkage_regularization_strength=0.4110227211817291 ,beta=0.597409114188568].
2023-08-01 14:30:36,603 INFO: Starting experiment with the arguments logged below.
2023-08-01 14:30:36,603 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 14:30:36,603 INFO: Setting the random number generator seed for this experiment.
2023-08-01 14:30:36,603 INFO: Loading dataset for the experiment.
2023-08-01 14:30:37,129 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 14:30:37,129 INFO: Setting up DEAP toolbox.
2023-08-01 14:30:37,129 INFO: Registering individual initialization method.
2023-08-01 14:30:37,129 INFO: Registering population initialization method.
2023-08-01 14:30:37,129 INFO: Registering the selection method.
2023-08-01 14:30:37,129 INFO: Registering the evaluation method.
2023-08-01 14:30:37,129 INFO: Registering the genetic operators.
2023-08-01 14:30:37,129 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 14:30:37,129 INFO: Initializing the initial population.
2023-08-01 14:30:37,136 INFO: Running the evolutionary algorithm.
2023-08-01 14:30:37,136 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 14:30:37,136 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 14:30:37,136 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.6383996221769521, momentum=0.8224939145845986 ,nesterov=True ,amsgrad=False ,weight_decay=0.005313731903443316 ,use_ema=False ,ema_momentum=0.008595247007774698 ,rho=0.6079038461973213 ,epsilon=0.0003579279962072253 ,centered=True ,beta_1=0.019303195267499795 ,beta_2=0.8438201045454315 ,learning_rate_power=-0.6668185228348357 ,initial_accumulator_value=0.4463253403413504 ,l1_regularization_strength=0.4320398747323363 ,l2_regularization_strength=0.7794172679325498 ,l2_shrinkage_regularization_strength=0.8467998900979319 ,beta=0.4558829782963951].
2023-08-01 14:30:37,136 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.6383996221769521, momentum=0.8224939145845986 ,nesterov=True ,amsgrad=False ,weight_decay=0.005313731903443316 ,use_ema=False ,ema_momentum=0.008595247007774698 ,rho=0.6079038461973213 ,epsilon=0.0003579279962072253 ,centered=True ,beta_1=0.019303195267499795 ,beta_2=0.8438201045454315 ,learning_rate_power=-0.6668185228348357 ,initial_accumulator_value=0.4463253403413504 ,l1_regularization_strength=0.4320398747323363 ,l2_regularization_strength=0.7794172679325498 ,l2_shrinkage_regularization_strength=0.8467998900979319 ,beta=0.4558829782963951].
2023-08-01 14:30:37,217 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:30:37,222 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:30:37,257 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:30:37,261 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:30:37,296 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:30:37,300 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:30:37,327 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:30:37,331 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:30:37,358 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:30:37,361 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:30:38,563 INFO: Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
2023-08-01 14:30:38,565 INFO: Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
2023-08-01 14:30:38,566 INFO: Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
2023-08-01 14:30:38,570 INFO: Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
2023-08-01 14:30:38,571 INFO: Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
2023-08-01 14:30:38,573 INFO: Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
2023-08-01 14:30:38,578 INFO: Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
2023-08-01 14:30:38,579 INFO: Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
2023-08-01 14:30:38,581 INFO: Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
2023-08-01 14:30:38,582 INFO: Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
2023-08-01 14:31:04,509 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.6749654027832934, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.9308678992293273 ,rho=0.008064189098185959 ,epsilon=0.0009392438522270688 ,centered=False ,beta_1=0.17935162562073304 ,beta_2=0.46087162005312676 ,learning_rate_power=-0.20848998966762078 ,initial_accumulator_value=0.1912930762227384 ,l1_regularization_strength=0.43969443757583127 ,l2_regularization_strength=0.87174952585183 ,l2_shrinkage_regularization_strength=0.3775145478025458 ,beta=0.3590600309996551].
2023-08-01 14:31:04,510 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.6749654027832934, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.9308678992293273 ,rho=0.008064189098185959 ,epsilon=0.0009392438522270688 ,centered=False ,beta_1=0.17935162562073304 ,beta_2=0.46087162005312676 ,learning_rate_power=-0.20848998966762078 ,initial_accumulator_value=0.1912930762227384 ,l1_regularization_strength=0.43969443757583127 ,l2_regularization_strength=0.87174952585183 ,l2_shrinkage_regularization_strength=0.3775145478025458 ,beta=0.3590600309996551].
2023-08-01 14:31:28,747 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.3059547001196693, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.00686830420427103 ,use_ema=False ,ema_momentum=0.7788899940312122 ,rho=0.3560263780184191 ,epsilon=0.0008534765792860641 ,centered=False ,beta_1=0.6706495074668049 ,beta_2=0.7173483422036026 ,learning_rate_power=-0.9506890480949989 ,initial_accumulator_value=0.5447268497371063 ,l1_regularization_strength=0.5203576180131393 ,l2_regularization_strength=0.004788576493884089 ,l2_shrinkage_regularization_strength=0.6301982572428312 ,beta=0.1825364006484883].
2023-08-01 14:31:28,748 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.3059547001196693, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.00686830420427103 ,use_ema=False ,ema_momentum=0.7788899940312122 ,rho=0.3560263780184191 ,epsilon=0.0008534765792860641 ,centered=False ,beta_1=0.6706495074668049 ,beta_2=0.7173483422036026 ,learning_rate_power=-0.9506890480949989 ,initial_accumulator_value=0.5447268497371063 ,l1_regularization_strength=0.5203576180131393 ,l2_regularization_strength=0.004788576493884089 ,l2_shrinkage_regularization_strength=0.6301982572428312 ,beta=0.1825364006484883].
2023-08-01 14:31:51,288 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.49443721030865717, momentum=0.16788931794901707 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.6360705955272722 ,rho=0.3605943236879682 ,epsilon=0.0008537297924627903 ,centered=False ,beta_1=0.7754834300086282 ,beta_2=0.3291871890757718 ,learning_rate_power=-0.5823543309083669 ,initial_accumulator_value=0.942815149069155 ,l1_regularization_strength=0.9516203549924852 ,l2_regularization_strength=0.5520560862957813 ,l2_shrinkage_regularization_strength=0.71048249670975 ,beta=0.33352770628076867].
2023-08-01 14:31:51,288 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.49443721030865717, momentum=0.16788931794901707 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.6360705955272722 ,rho=0.3605943236879682 ,epsilon=0.0008537297924627903 ,centered=False ,beta_1=0.7754834300086282 ,beta_2=0.3291871890757718 ,learning_rate_power=-0.5823543309083669 ,initial_accumulator_value=0.942815149069155 ,l1_regularization_strength=0.9516203549924852 ,l2_regularization_strength=0.5520560862957813 ,l2_shrinkage_regularization_strength=0.71048249670975 ,beta=0.33352770628076867].
2023-08-01 14:32:25,602 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.779487563977484, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0008331725297189893 ,use_ema=False ,ema_momentum=0.015790521390915413 ,rho=0.5915855123793 ,epsilon=0.0002038193215948838 ,centered=True ,beta_1=0.37931869524437667 ,beta_2=0.34032447759967976 ,learning_rate_power=-0.8548232657605064 ,initial_accumulator_value=0.4585684750399739 ,l1_regularization_strength=0.42973026017389104 ,l2_regularization_strength=0.802994341171008 ,l2_shrinkage_regularization_strength=0.7215104363128653 ,beta=0.4609422971899255].
2023-08-01 14:32:25,602 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.779487563977484, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0008331725297189893 ,use_ema=False ,ema_momentum=0.015790521390915413 ,rho=0.5915855123793 ,epsilon=0.0002038193215948838 ,centered=True ,beta_1=0.37931869524437667 ,beta_2=0.34032447759967976 ,learning_rate_power=-0.8548232657605064 ,initial_accumulator_value=0.4585684750399739 ,l1_regularization_strength=0.42973026017389104 ,l2_regularization_strength=0.802994341171008 ,l2_shrinkage_regularization_strength=0.7215104363128653 ,beta=0.4609422971899255].
2023-08-01 14:32:48,736 INFO: Applying selection operators for generation 1.
2023-08-01 14:32:48,736 INFO: Applying genetic operators for generation 1.
2023-08-01 14:32:48,738 INFO: Evaluating fitness for for generation 1.
2023-08-01 14:32:48,738 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 14:32:48,738 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.6383996221769521, momentum=0.8224939145845986 ,nesterov=True ,amsgrad=False ,weight_decay=0.005313731903443316 ,use_ema=True ,ema_momentum=0.6360705955272722 ,rho=0.3605943236879682 ,epsilon=0.0008537297924627903 ,centered=False ,beta_1=0.7754834300086282 ,beta_2=0.3291871890757718 ,learning_rate_power=-0.5823543309083669 ,initial_accumulator_value=0.942815149069155 ,l1_regularization_strength=0.9516203549924852 ,l2_regularization_strength=0.5520560862957813 ,l2_shrinkage_regularization_strength=0.8467998900979319 ,beta=0.4558829782963951].
2023-08-01 14:32:48,738 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.6383996221769521, momentum=0.8224939145845986 ,nesterov=True ,amsgrad=False ,weight_decay=0.005313731903443316 ,use_ema=True ,ema_momentum=0.6360705955272722 ,rho=0.3605943236879682 ,epsilon=0.0008537297924627903 ,centered=False ,beta_1=0.7754834300086282 ,beta_2=0.3291871890757718 ,learning_rate_power=-0.5823543309083669 ,initial_accumulator_value=0.942815149069155 ,l1_regularization_strength=0.9516203549924852 ,l2_regularization_strength=0.5520560862957813 ,l2_shrinkage_regularization_strength=0.8467998900979319 ,beta=0.4558829782963951].
2023-08-01 14:33:12,573 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.49443721030865717, momentum=0.16788931794901707 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.008595247007774698 ,rho=0.6079038461973213 ,epsilon=0.0003579279962072253 ,centered=True ,beta_1=0.019303195267499795 ,beta_2=0.8438201045454315 ,learning_rate_power=-0.6668185228348357 ,initial_accumulator_value=0.4463253403413504 ,l1_regularization_strength=0.4320398747323363 ,l2_regularization_strength=0.7794172679325498 ,l2_shrinkage_regularization_strength=0.71048249670975 ,beta=0.33352770628076867].
2023-08-01 14:33:12,574 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.49443721030865717, momentum=0.16788931794901707 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.008595247007774698 ,rho=0.6079038461973213 ,epsilon=0.0003579279962072253 ,centered=True ,beta_1=0.019303195267499795 ,beta_2=0.8438201045454315 ,learning_rate_power=-0.6668185228348357 ,initial_accumulator_value=0.4463253403413504 ,l1_regularization_strength=0.4320398747323363 ,l2_regularization_strength=0.7794172679325498 ,l2_shrinkage_regularization_strength=0.71048249670975 ,beta=0.33352770628076867].
2023-08-01 14:33:46,804 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.6323660846367403, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.5692331067790768 ,rho=0.008064189098185959 ,epsilon=0.0009392438522270688 ,centered=False ,beta_1=0.7024604295598021 ,beta_2=0.10016481628655216 ,learning_rate_power=-0.8548232657605064 ,initial_accumulator_value=0.4585684750399739 ,l1_regularization_strength=0.6168109576518562 ,l2_regularization_strength=0.87174952585183 ,l2_shrinkage_regularization_strength=0.13740213114422384 ,beta=0.19588396609222491].
2023-08-01 14:33:46,804 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.6323660846367403, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.5692331067790768 ,rho=0.008064189098185959 ,epsilon=0.0009392438522270688 ,centered=False ,beta_1=0.7024604295598021 ,beta_2=0.10016481628655216 ,learning_rate_power=-0.8548232657605064 ,initial_accumulator_value=0.4585684750399739 ,l1_regularization_strength=0.6168109576518562 ,l2_regularization_strength=0.87174952585183 ,l2_shrinkage_regularization_strength=0.13740213114422384 ,beta=0.19588396609222491].
2023-08-01 14:34:10,822 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.779487563977484, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0008331725297189893 ,use_ema=False ,ema_momentum=0.015790521390915413 ,rho=0.5915855123793 ,epsilon=0.0002038193215948838 ,centered=False ,beta_1=0.17935162562073304 ,beta_2=0.46087162005312676 ,learning_rate_power=-0.20848998966762078 ,initial_accumulator_value=0.1912930762227384 ,l1_regularization_strength=0.43969443757583127 ,l2_regularization_strength=0.802994341171008 ,l2_shrinkage_regularization_strength=0.7215104363128653 ,beta=0.4609422971899255].
2023-08-01 14:34:10,822 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.779487563977484, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0008331725297189893 ,use_ema=False ,ema_momentum=0.015790521390915413 ,rho=0.5915855123793 ,epsilon=0.0002038193215948838 ,centered=False ,beta_1=0.17935162562073304 ,beta_2=0.46087162005312676 ,learning_rate_power=-0.20848998966762078 ,initial_accumulator_value=0.1912930762227384 ,l1_regularization_strength=0.43969443757583127 ,l2_regularization_strength=0.802994341171008 ,l2_shrinkage_regularization_strength=0.7215104363128653 ,beta=0.4609422971899255].
2023-08-01 14:34:32,997 INFO: Applying selection operators for generation 2.
2023-08-01 14:34:32,998 INFO: Applying genetic operators for generation 2.
2023-08-01 14:34:32,998 INFO: Evaluating fitness for for generation 2.
2023-08-01 14:34:32,998 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 14:34:32,998 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.779487563977484, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.005313731903443316 ,use_ema=True ,ema_momentum=0.6360705955272722 ,rho=0.3605943236879682 ,epsilon=0.0008537297924627903 ,centered=False ,beta_1=0.17935162562073304 ,beta_2=0.46087162005312676 ,learning_rate_power=-0.20848998966762078 ,initial_accumulator_value=0.1912930762227384 ,l1_regularization_strength=0.43969443757583127 ,l2_regularization_strength=0.802994341171008 ,l2_shrinkage_regularization_strength=0.7215104363128653 ,beta=0.4609422971899255].
2023-08-01 14:34:32,998 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.779487563977484, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.005313731903443316 ,use_ema=True ,ema_momentum=0.6360705955272722 ,rho=0.3605943236879682 ,epsilon=0.0008537297924627903 ,centered=False ,beta_1=0.17935162562073304 ,beta_2=0.46087162005312676 ,learning_rate_power=-0.20848998966762078 ,initial_accumulator_value=0.1912930762227384 ,l1_regularization_strength=0.43969443757583127 ,l2_regularization_strength=0.802994341171008 ,l2_shrinkage_regularization_strength=0.7215104363128653 ,beta=0.4609422971899255].
2023-08-01 14:56:09,606 INFO: Starting experiment with the arguments logged below.
2023-08-01 14:56:09,606 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=5, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 14:56:09,606 INFO: Setting the random number generator seed for this experiment.
2023-08-01 14:56:09,606 INFO: Loading dataset for the experiment.
2023-08-01 14:56:10,114 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 14:56:10,114 INFO: Setting up DEAP toolbox.
2023-08-01 14:56:10,114 INFO: Registering individual initialization method.
2023-08-01 14:56:10,114 INFO: Registering population initialization method.
2023-08-01 14:56:10,114 INFO: Registering the selection method.
2023-08-01 14:56:10,114 INFO: Registering the evaluation method.
2023-08-01 14:56:10,114 INFO: Registering the genetic operators.
2023-08-01 14:56:10,114 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 14:56:10,114 INFO: Initializing the initial population.
2023-08-01 14:56:10,120 INFO: Running the evolutionary algorithm.
2023-08-01 14:56:10,120 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 14:56:10,120 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 14:56:10,121 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8182300622843633, momentum=0.7454872339630445 ,nesterov=True ,amsgrad=False ,weight_decay=0.008321451550783338 ,use_ema=False ,ema_momentum=0.5436689750806866 ,rho=0.7602202238882952 ,epsilon=0.0005125300666196568 ,centered=True ,beta_1=0.5019275180737421 ,beta_2=0.6197099425960942 ,learning_rate_power=-0.2152065436093461 ,initial_accumulator_value=0.19327873711784627 ,l1_regularization_strength=0.8438620214916965 ,l2_regularization_strength=0.777895484482472 ,l2_shrinkage_regularization_strength=0.4937306897799454 ,beta=0.037678860782756285].
2023-08-01 14:56:10,121 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8182300622843633, momentum=0.7454872339630445 ,nesterov=True ,amsgrad=False ,weight_decay=0.008321451550783338 ,use_ema=False ,ema_momentum=0.5436689750806866 ,rho=0.7602202238882952 ,epsilon=0.0005125300666196568 ,centered=True ,beta_1=0.5019275180737421 ,beta_2=0.6197099425960942 ,learning_rate_power=-0.2152065436093461 ,initial_accumulator_value=0.19327873711784627 ,l1_regularization_strength=0.8438620214916965 ,l2_regularization_strength=0.777895484482472 ,l2_shrinkage_regularization_strength=0.4937306897799454 ,beta=0.037678860782756285].
2023-08-01 14:56:10,215 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:56:10,221 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:56:10,269 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:56:10,273 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:56:10,318 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:56:10,322 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:56:10,357 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:56:10,361 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:56:10,395 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:56:10,399 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:58:02,522 INFO: Starting experiment with the arguments logged below.
2023-08-01 14:58:02,522 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=5, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 14:58:02,522 INFO: Setting the random number generator seed for this experiment.
2023-08-01 14:58:02,522 INFO: Loading dataset for the experiment.
2023-08-01 14:58:03,031 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 14:58:03,032 INFO: Setting up DEAP toolbox.
2023-08-01 14:58:03,032 INFO: Registering individual initialization method.
2023-08-01 14:58:03,032 INFO: Registering population initialization method.
2023-08-01 14:58:03,032 INFO: Registering the selection method.
2023-08-01 14:58:03,032 INFO: Registering the evaluation method.
2023-08-01 14:58:03,032 INFO: Registering the genetic operators.
2023-08-01 14:58:03,032 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 14:58:03,032 INFO: Initializing the initial population.
2023-08-01 14:58:03,038 INFO: Running the evolutionary algorithm.
2023-08-01 14:58:03,039 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 14:58:03,039 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 14:58:03,039 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.06275812703740191, momentum=0.5184626052935326 ,nesterov=True ,amsgrad=False ,weight_decay=0.004787933117496646 ,use_ema=False ,ema_momentum=0.9532734730331078 ,rho=0.6164759039763433 ,epsilon=0.0006424230079463392 ,centered=True ,beta_1=0.595995864530722 ,beta_2=0.7520078026608514 ,learning_rate_power=-0.04002528090866342 ,initial_accumulator_value=0.8813438153920908 ,l1_regularization_strength=0.5251501403141488 ,l2_regularization_strength=0.9984993321575505 ,l2_shrinkage_regularization_strength=0.7688858911326935 ,beta=0.9634086767905611].
2023-08-01 14:58:03,040 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.06275812703740191, momentum=0.5184626052935326 ,nesterov=True ,amsgrad=False ,weight_decay=0.004787933117496646 ,use_ema=False ,ema_momentum=0.9532734730331078 ,rho=0.6164759039763433 ,epsilon=0.0006424230079463392 ,centered=True ,beta_1=0.595995864530722 ,beta_2=0.7520078026608514 ,learning_rate_power=-0.04002528090866342 ,initial_accumulator_value=0.8813438153920908 ,l1_regularization_strength=0.5251501403141488 ,l2_regularization_strength=0.9984993321575505 ,l2_shrinkage_regularization_strength=0.7688858911326935 ,beta=0.9634086767905611].
2023-08-01 14:58:03,138 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:58:03,143 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:58:03,187 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:58:03,191 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:58:03,234 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:58:03,238 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:58:03,273 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:58:03,276 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:58:03,310 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 14:58:03,314 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:02:19,403 INFO: Starting experiment with the arguments logged below.
2023-08-01 15:02:19,404 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=5, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 15:02:19,404 INFO: Setting the random number generator seed for this experiment.
2023-08-01 15:02:19,404 INFO: Loading dataset for the experiment.
2023-08-01 15:02:19,927 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 15:02:19,927 INFO: Setting up DEAP toolbox.
2023-08-01 15:02:19,927 INFO: Registering individual initialization method.
2023-08-01 15:02:19,927 INFO: Registering population initialization method.
2023-08-01 15:02:19,927 INFO: Registering the selection method.
2023-08-01 15:02:19,927 INFO: Registering the evaluation method.
2023-08-01 15:02:19,928 INFO: Registering the genetic operators.
2023-08-01 15:02:19,928 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 15:02:19,928 INFO: Initializing the initial population.
2023-08-01 15:02:19,934 INFO: Running the evolutionary algorithm.
2023-08-01 15:02:19,934 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 15:02:19,934 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 15:02:19,934 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.4684252666371663, momentum=0.5329000617177342 ,nesterov=True ,amsgrad=False ,weight_decay=0.006018833979601069 ,use_ema=False ,ema_momentum=0.6508138668128942 ,rho=0.11583496909823265 ,epsilon=0.00032708054633597 ,centered=True ,beta_1=0.014077894856209583 ,beta_2=0.5875242239090669 ,learning_rate_power=-0.1282979501906889 ,initial_accumulator_value=0.8132986411508959 ,l1_regularization_strength=0.43646709079099977 ,l2_regularization_strength=0.7998907577268439 ,l2_shrinkage_regularization_strength=0.42549654197327635 ,beta=0.260367758317643].
2023-08-01 15:02:19,934 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.4684252666371663, momentum=0.5329000617177342 ,nesterov=True ,amsgrad=False ,weight_decay=0.006018833979601069 ,use_ema=False ,ema_momentum=0.6508138668128942 ,rho=0.11583496909823265 ,epsilon=0.00032708054633597 ,centered=True ,beta_1=0.014077894856209583 ,beta_2=0.5875242239090669 ,learning_rate_power=-0.1282979501906889 ,initial_accumulator_value=0.8132986411508959 ,l1_regularization_strength=0.43646709079099977 ,l2_regularization_strength=0.7998907577268439 ,l2_shrinkage_regularization_strength=0.42549654197327635 ,beta=0.260367758317643].
2023-08-01 15:02:20,022 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:02:20,026 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:02:20,061 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:02:20,064 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:02:20,098 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:02:20,101 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:02:20,129 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:02:20,132 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:02:20,159 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:02:20,162 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:02:21,323 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:02:22,705 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:02:42,018 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.41175165654056245, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.92140882860277 ,rho=0.5433381215863073 ,epsilon=2.7287648853796425e-05 ,centered=False ,beta_1=0.8942471164045138 ,beta_2=0.5673273845735354 ,learning_rate_power=-0.951358147060555 ,initial_accumulator_value=0.26066636141493993 ,l1_regularization_strength=0.36030247908175206 ,l2_regularization_strength=0.8429115355578267 ,l2_shrinkage_regularization_strength=0.5968053203809346 ,beta=0.44719362198900137].
2023-08-01 15:02:42,018 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.41175165654056245, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.92140882860277 ,rho=0.5433381215863073 ,epsilon=2.7287648853796425e-05 ,centered=False ,beta_1=0.8942471164045138 ,beta_2=0.5673273845735354 ,learning_rate_power=-0.951358147060555 ,initial_accumulator_value=0.26066636141493993 ,l1_regularization_strength=0.36030247908175206 ,l2_regularization_strength=0.8429115355578267 ,l2_shrinkage_regularization_strength=0.5968053203809346 ,beta=0.44719362198900137].
2023-08-01 15:02:43,173 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:02:44,469 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:02:57,296 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.5000986686958209, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.008688459904135342 ,use_ema=False ,ema_momentum=0.43817818709341694 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.23503317245515187 ,beta_2=0.18273962608591154 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.08234801313948537 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.5301422689238604 ,beta=0.0035208303099979466].
2023-08-01 15:02:57,296 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.5000986686958209, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.008688459904135342 ,use_ema=False ,ema_momentum=0.43817818709341694 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.23503317245515187 ,beta_2=0.18273962608591154 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.08234801313948537 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.5301422689238604 ,beta=0.0035208303099979466].
2023-08-01 15:02:58,445 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:03:00,184 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:03:12,140 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.09206064591397789, momentum=0.46587622806910967 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.5924956378813113 ,rho=0.890062719219267 ,epsilon=0.0003778354466862316 ,centered=False ,beta_1=0.6810168932708113 ,beta_2=0.6759576332088265 ,learning_rate_power=-0.7996010320212077 ,initial_accumulator_value=0.9440524440423377 ,l1_regularization_strength=0.8632859742275868 ,l2_regularization_strength=0.6302985368188474 ,l2_shrinkage_regularization_strength=0.13188478740762422 ,beta=0.3204128002470231].
2023-08-01 15:03:12,141 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.09206064591397789, momentum=0.46587622806910967 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.5924956378813113 ,rho=0.890062719219267 ,epsilon=0.0003778354466862316 ,centered=False ,beta_1=0.6810168932708113 ,beta_2=0.6759576332088265 ,learning_rate_power=-0.7996010320212077 ,initial_accumulator_value=0.9440524440423377 ,l1_regularization_strength=0.8632859742275868 ,l2_regularization_strength=0.6302985368188474 ,l2_shrinkage_regularization_strength=0.13188478740762422 ,beta=0.3204128002470231].
2023-08-01 15:03:13,281 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:03:16,581 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:03:36,084 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.538162824259468, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0069872390549022495 ,use_ema=False ,ema_momentum=0.5161753566684104 ,rho=0.779295951267342 ,epsilon=0.00015534108958095637 ,centered=True ,beta_1=0.14719506365553792 ,beta_2=0.6475414852776401 ,learning_rate_power=-0.09934787341352402 ,initial_accumulator_value=0.006505548923221416 ,l1_regularization_strength=0.8523449801314199 ,l2_regularization_strength=0.4276747104512416 ,l2_shrinkage_regularization_strength=0.6571496771080255 ,beta=0.7731466899961443].
2023-08-01 15:03:36,084 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.538162824259468, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0069872390549022495 ,use_ema=False ,ema_momentum=0.5161753566684104 ,rho=0.779295951267342 ,epsilon=0.00015534108958095637 ,centered=True ,beta_1=0.14719506365553792 ,beta_2=0.6475414852776401 ,learning_rate_power=-0.09934787341352402 ,initial_accumulator_value=0.006505548923221416 ,l1_regularization_strength=0.8523449801314199 ,l2_regularization_strength=0.4276747104512416 ,l2_shrinkage_regularization_strength=0.6571496771080255 ,beta=0.7731466899961443].
2023-08-01 15:03:37,246 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:03:38,586 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:03:52,126 INFO: Applying selection operators for generation 1.
2023-08-01 15:03:52,126 INFO: Applying genetic operators for generation 1.
2023-08-01 15:03:52,128 INFO: Evaluating fitness for for generation 1.
2023-08-01 15:03:52,128 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 15:03:52,128 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.4684252666371663, momentum=0.5329000617177342 ,nesterov=True ,amsgrad=False ,weight_decay=0.006018833979601069 ,use_ema=False ,ema_momentum=0.43817818709341694 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.23503317245515187 ,beta_2=0.18273962608591154 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.08234801313948537 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.42549654197327635 ,beta=0.260367758317643].
2023-08-01 15:03:52,128 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.4684252666371663, momentum=0.5329000617177342 ,nesterov=True ,amsgrad=False ,weight_decay=0.006018833979601069 ,use_ema=False ,ema_momentum=0.43817818709341694 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.23503317245515187 ,beta_2=0.18273962608591154 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.08234801313948537 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.42549654197327635 ,beta=0.260367758317643].
2023-08-01 15:04:08,014 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.5000986686958209, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.008688459904135342 ,use_ema=False ,ema_momentum=0.6508138668128942 ,rho=0.11583496909823265 ,epsilon=0.00032708054633597 ,centered=True ,beta_1=0.014077894856209583 ,beta_2=0.5875242239090669 ,learning_rate_power=-0.1282979501906889 ,initial_accumulator_value=0.8132986411508959 ,l1_regularization_strength=0.43646709079099977 ,l2_regularization_strength=0.7998907577268439 ,l2_shrinkage_regularization_strength=0.5301422689238604 ,beta=0.0035208303099979466].
2023-08-01 15:04:08,015 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.5000986686958209, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.008688459904135342 ,use_ema=False ,ema_momentum=0.6508138668128942 ,rho=0.11583496909823265 ,epsilon=0.00032708054633597 ,centered=True ,beta_1=0.014077894856209583 ,beta_2=0.5875242239090669 ,learning_rate_power=-0.1282979501906889 ,initial_accumulator_value=0.8132986411508959 ,l1_regularization_strength=0.43646709079099977 ,l2_regularization_strength=0.7998907577268439 ,l2_shrinkage_regularization_strength=0.5301422689238604 ,beta=0.0035208303099979466].
2023-08-01 15:04:22,076 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.6032916912162115, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.6657546286544577 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.7226636195965982 ,beta_2=0.6866097172588499 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.6001383076265943 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.44140014986456766 ,beta=0.6319236745191268].
2023-08-01 15:04:22,076 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.6032916912162115, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.6657546286544577 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.7226636195965982 ,beta_2=0.6866097172588499 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.6001383076265943 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.44140014986456766 ,beta=0.6319236745191268].
2023-08-01 15:04:39,011 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.5000986686958209, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.008688459904135342 ,use_ema=False ,ema_momentum=0.43817818709341694 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.23503317245515187 ,beta_2=0.18273962608591154 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.08234801313948537 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.5301422689238604 ,beta=0.0035208303099979466].
2023-08-01 15:04:39,012 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.5000986686958209, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.008688459904135342 ,use_ema=False ,ema_momentum=0.43817818709341694 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.23503317245515187 ,beta_2=0.18273962608591154 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.08234801313948537 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.5301422689238604 ,beta=0.0035208303099979466].
2023-08-01 15:04:53,145 INFO: Applying selection operators for generation 2.
2023-08-01 15:04:53,145 INFO: Applying genetic operators for generation 2.
2023-08-01 15:04:53,146 INFO: Evaluating fitness for for generation 2.
2023-08-01 15:04:53,146 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 15:04:53,146 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.5000986686958209, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.006018833979601069 ,use_ema=False ,ema_momentum=0.43817818709341694 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.23503317245515187 ,beta_2=0.18273962608591154 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.08234801313948537 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.5301422689238604 ,beta=0.0035208303099979466].
2023-08-01 15:04:53,146 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.5000986686958209, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.006018833979601069 ,use_ema=False ,ema_momentum=0.43817818709341694 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.23503317245515187 ,beta_2=0.18273962608591154 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.08234801313948537 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.5301422689238604 ,beta=0.0035208303099979466].
2023-08-01 15:05:07,180 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.4684252666371663, momentum=0.5329000617177342 ,nesterov=True ,amsgrad=False ,weight_decay=0.008688459904135342 ,use_ema=False ,ema_momentum=0.43817818709341694 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.23503317245515187 ,beta_2=0.18273962608591154 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.08234801313948537 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.42549654197327635 ,beta=0.260367758317643].
2023-08-01 15:05:07,181 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.4684252666371663, momentum=0.5329000617177342 ,nesterov=True ,amsgrad=False ,weight_decay=0.008688459904135342 ,use_ema=False ,ema_momentum=0.43817818709341694 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.23503317245515187 ,beta_2=0.18273962608591154 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.08234801313948537 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.42549654197327635 ,beta=0.260367758317643].
2023-08-01 15:05:22,081 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.5000986686958209, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.008688459904135342 ,use_ema=False ,ema_momentum=0.43817818709341694 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.23503317245515187 ,beta_2=0.18273962608591154 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.08234801313948537 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.5301422689238604 ,beta=0.0035208303099979466].
2023-08-01 15:05:22,081 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.5000986686958209, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.008688459904135342 ,use_ema=False ,ema_momentum=0.43817818709341694 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.23503317245515187 ,beta_2=0.18273962608591154 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.08234801313948537 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.5301422689238604 ,beta=0.0035208303099979466].
2023-08-01 15:05:36,182 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.5000986686958209, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.008688459904135342 ,use_ema=False ,ema_momentum=0.43817818709341694 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.23503317245515187 ,beta_2=0.18273962608591154 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.08234801313948537 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.5301422689238604 ,beta=0.0035208303099979466].
2023-08-01 15:05:36,182 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.5000986686958209, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.008688459904135342 ,use_ema=False ,ema_momentum=0.43817818709341694 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.23503317245515187 ,beta_2=0.18273962608591154 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.08234801313948537 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.5301422689238604 ,beta=0.0035208303099979466].
2023-08-01 15:05:50,311 INFO: Applying selection operators for generation 3.
2023-08-01 15:05:50,312 INFO: Applying genetic operators for generation 3.
2023-08-01 15:05:50,313 INFO: Evaluating fitness for for generation 3.
2023-08-01 15:05:50,313 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 15:05:50,313 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.4684252666371663, momentum=0.5329000617177342 ,nesterov=True ,amsgrad=False ,weight_decay=0.006018833979601069 ,use_ema=False ,ema_momentum=0.43817818709341694 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.23503317245515187 ,beta_2=0.18273962608591154 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.08234801313948537 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.42549654197327635 ,beta=0.260367758317643].
2023-08-01 15:05:50,313 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.4684252666371663, momentum=0.5329000617177342 ,nesterov=True ,amsgrad=False ,weight_decay=0.006018833979601069 ,use_ema=False ,ema_momentum=0.43817818709341694 ,rho=0.9180978418946779 ,epsilon=0.0001881962233637588 ,centered=False ,beta_1=0.23503317245515187 ,beta_2=0.18273962608591154 ,learning_rate_power=-0.4372461551786365 ,initial_accumulator_value=0.45065749871038907 ,l1_regularization_strength=0.08234801313948537 ,l2_regularization_strength=0.19347297765231453 ,l2_shrinkage_regularization_strength=0.42549654197327635 ,beta=0.260367758317643].
2023-08-01 15:06:23,384 INFO: Starting experiment with the arguments logged below.
2023-08-01 15:06:23,384 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=5, batch_size=2048, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 15:06:23,384 INFO: Setting the random number generator seed for this experiment.
2023-08-01 15:06:23,384 INFO: Loading dataset for the experiment.
2023-08-01 15:06:23,887 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 15:06:23,887 INFO: Setting up DEAP toolbox.
2023-08-01 15:06:23,887 INFO: Registering individual initialization method.
2023-08-01 15:06:23,887 INFO: Registering population initialization method.
2023-08-01 15:06:23,887 INFO: Registering the selection method.
2023-08-01 15:06:23,887 INFO: Registering the evaluation method.
2023-08-01 15:06:23,887 INFO: Registering the genetic operators.
2023-08-01 15:06:23,887 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 15:06:23,887 INFO: Initializing the initial population.
2023-08-01 15:06:23,893 INFO: Running the evolutionary algorithm.
2023-08-01 15:06:23,893 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 15:06:23,894 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 15:06:23,894 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.05004227745498735, momentum=0.19196130240253706 ,nesterov=True ,amsgrad=False ,weight_decay=0.001929141669870598 ,use_ema=False ,ema_momentum=0.7846454093407 ,rho=0.711564215958096 ,epsilon=0.00026867609545596385 ,centered=True ,beta_1=0.1876901488196968 ,beta_2=0.7360430070806566 ,learning_rate_power=-0.30578327816939044 ,initial_accumulator_value=0.11119422662514533 ,l1_regularization_strength=0.14623020902861805 ,l2_regularization_strength=0.7863759559180853 ,l2_shrinkage_regularization_strength=0.7184219988800788 ,beta=0.1975340906967078].
2023-08-01 15:06:23,894 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.05004227745498735, momentum=0.19196130240253706 ,nesterov=True ,amsgrad=False ,weight_decay=0.001929141669870598 ,use_ema=False ,ema_momentum=0.7846454093407 ,rho=0.711564215958096 ,epsilon=0.00026867609545596385 ,centered=True ,beta_1=0.1876901488196968 ,beta_2=0.7360430070806566 ,learning_rate_power=-0.30578327816939044 ,initial_accumulator_value=0.11119422662514533 ,l1_regularization_strength=0.14623020902861805 ,l2_regularization_strength=0.7863759559180853 ,l2_shrinkage_regularization_strength=0.7184219988800788 ,beta=0.1975340906967078].
2023-08-01 15:06:23,975 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:06:23,979 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:06:24,014 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:06:24,017 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:06:24,052 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:06:24,055 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:06:24,082 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:06:24,085 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:06:24,112 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:06:24,115 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:06:25,273 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:06:26,646 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:06:45,944 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.04724967138095859, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.9046442045196693 ,rho=0.7714703679947358 ,epsilon=0.0006092344654480736 ,centered=False ,beta_1=0.7825599816381521 ,beta_2=0.4132829459802567 ,learning_rate_power=-0.03723395250910633 ,initial_accumulator_value=0.9777408159822472 ,l1_regularization_strength=0.09465721932138882 ,l2_regularization_strength=0.7827362510189894 ,l2_shrinkage_regularization_strength=0.04480031371117987 ,beta=0.9645818737906738].
2023-08-01 15:06:45,945 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.04724967138095859, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.9046442045196693 ,rho=0.7714703679947358 ,epsilon=0.0006092344654480736 ,centered=False ,beta_1=0.7825599816381521 ,beta_2=0.4132829459802567 ,learning_rate_power=-0.03723395250910633 ,initial_accumulator_value=0.9777408159822472 ,l1_regularization_strength=0.09465721932138882 ,l2_regularization_strength=0.7827362510189894 ,l2_shrinkage_regularization_strength=0.04480031371117987 ,beta=0.9645818737906738].
2023-08-01 15:06:47,097 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:06:48,472 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:07:00,823 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.5173714471191014, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.00876406191384682 ,use_ema=False ,ema_momentum=0.6877338690809355 ,rho=0.5347340812990724 ,epsilon=0.0008689728567095245 ,centered=False ,beta_1=0.2598220468713426 ,beta_2=0.23787504467640008 ,learning_rate_power=-0.3692618830817914 ,initial_accumulator_value=0.6097454637300154 ,l1_regularization_strength=0.6099526552117127 ,l2_regularization_strength=0.8949344971082497 ,l2_shrinkage_regularization_strength=0.5608614539863396 ,beta=0.3041850563220828].
2023-08-01 15:07:00,824 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.5173714471191014, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.00876406191384682 ,use_ema=False ,ema_momentum=0.6877338690809355 ,rho=0.5347340812990724 ,epsilon=0.0008689728567095245 ,centered=False ,beta_1=0.2598220468713426 ,beta_2=0.23787504467640008 ,learning_rate_power=-0.3692618830817914 ,initial_accumulator_value=0.6097454637300154 ,l1_regularization_strength=0.6099526552117127 ,l2_regularization_strength=0.8949344971082497 ,l2_shrinkage_regularization_strength=0.5608614539863396 ,beta=0.3041850563220828].
2023-08-01 15:07:01,970 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:07:03,638 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:07:14,754 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.4895115747202301, momentum=0.08154415734849862 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.38686489672821067 ,rho=0.3182100828213602 ,epsilon=0.00015768898244828355 ,centered=False ,beta_1=0.9564931843621358 ,beta_2=0.49536365723373654 ,learning_rate_power=-0.00459498396993141 ,initial_accumulator_value=0.5808471383963625 ,l1_regularization_strength=0.45119133088318875 ,l2_regularization_strength=0.4061312699656653 ,l2_shrinkage_regularization_strength=0.6696282884387836 ,beta=0.8205662782653603].
2023-08-01 15:07:14,755 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.4895115747202301, momentum=0.08154415734849862 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.38686489672821067 ,rho=0.3182100828213602 ,epsilon=0.00015768898244828355 ,centered=False ,beta_1=0.9564931843621358 ,beta_2=0.49536365723373654 ,learning_rate_power=-0.00459498396993141 ,initial_accumulator_value=0.5808471383963625 ,l1_regularization_strength=0.45119133088318875 ,l2_regularization_strength=0.4061312699656653 ,l2_shrinkage_regularization_strength=0.6696282884387836 ,beta=0.8205662782653603].
2023-08-01 15:07:15,893 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:07:19,258 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:07:38,305 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9395923695262671, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006144509203790937 ,use_ema=False ,ema_momentum=0.44605401774510633 ,rho=0.651814934315625 ,epsilon=3.380189932118019e-05 ,centered=True ,beta_1=0.1363849046284391 ,beta_2=0.6086968172811122 ,learning_rate_power=-0.5701277645778285 ,initial_accumulator_value=0.0628618749447265 ,l1_regularization_strength=0.8558969693834362 ,l2_regularization_strength=0.6495374010252726 ,l2_shrinkage_regularization_strength=0.4635306269751449 ,beta=0.4407273804113949].
2023-08-01 15:07:38,305 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9395923695262671, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.006144509203790937 ,use_ema=False ,ema_momentum=0.44605401774510633 ,rho=0.651814934315625 ,epsilon=3.380189932118019e-05 ,centered=True ,beta_1=0.1363849046284391 ,beta_2=0.6086968172811122 ,learning_rate_power=-0.5701277645778285 ,initial_accumulator_value=0.0628618749447265 ,l1_regularization_strength=0.8558969693834362 ,l2_regularization_strength=0.6495374010252726 ,l2_shrinkage_regularization_strength=0.4635306269751449 ,beta=0.4407273804113949].
2023-08-01 15:07:39,454 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:07:40,813 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:07:53,904 INFO: Applying selection operators for generation 1.
2023-08-01 15:07:53,905 INFO: Applying genetic operators for generation 1.
2023-08-01 15:07:53,906 INFO: Evaluating fitness for for generation 1.
2023-08-01 15:07:53,906 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 15:07:53,906 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.05004227745498735, momentum=0.19196130240253706 ,nesterov=True ,amsgrad=False ,weight_decay=0.001929141669870598 ,use_ema=False ,ema_momentum=0.7846454093407 ,rho=0.711564215958096 ,epsilon=0.00026867609545596385 ,centered=True ,beta_1=0.1876901488196968 ,beta_2=0.7360430070806566 ,learning_rate_power=-0.30578327816939044 ,initial_accumulator_value=0.11119422662514533 ,l1_regularization_strength=0.14623020902861805 ,l2_regularization_strength=0.7863759559180853 ,l2_shrinkage_regularization_strength=0.7184219988800788 ,beta=0.1975340906967078].
2023-08-01 15:07:53,906 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.05004227745498735, momentum=0.19196130240253706 ,nesterov=True ,amsgrad=False ,weight_decay=0.001929141669870598 ,use_ema=False ,ema_momentum=0.7846454093407 ,rho=0.711564215958096 ,epsilon=0.00026867609545596385 ,centered=True ,beta_1=0.1876901488196968 ,beta_2=0.7360430070806566 ,learning_rate_power=-0.30578327816939044 ,initial_accumulator_value=0.11119422662514533 ,l1_regularization_strength=0.14623020902861805 ,l2_regularization_strength=0.7863759559180853 ,l2_shrinkage_regularization_strength=0.7184219988800788 ,beta=0.1975340906967078].
2023-08-01 15:08:08,153 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.05004227745498735, momentum=0.19196130240253706 ,nesterov=True ,amsgrad=False ,weight_decay=0.001929141669870598 ,use_ema=False ,ema_momentum=0.7846454093407 ,rho=0.711564215958096 ,epsilon=0.00026867609545596385 ,centered=True ,beta_1=0.1876901488196968 ,beta_2=0.7360430070806566 ,learning_rate_power=-0.30578327816939044 ,initial_accumulator_value=0.11119422662514533 ,l1_regularization_strength=0.14623020902861805 ,l2_regularization_strength=0.7863759559180853 ,l2_shrinkage_regularization_strength=0.7184219988800788 ,beta=0.1975340906967078].
2023-08-01 15:08:08,153 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.05004227745498735, momentum=0.19196130240253706 ,nesterov=True ,amsgrad=False ,weight_decay=0.001929141669870598 ,use_ema=False ,ema_momentum=0.7846454093407 ,rho=0.711564215958096 ,epsilon=0.00026867609545596385 ,centered=True ,beta_1=0.1876901488196968 ,beta_2=0.7360430070806566 ,learning_rate_power=-0.30578327816939044 ,initial_accumulator_value=0.11119422662514533 ,l1_regularization_strength=0.14623020902861805 ,l2_regularization_strength=0.7863759559180853 ,l2_shrinkage_regularization_strength=0.7184219988800788 ,beta=0.1975340906967078].
2023-08-01 15:08:41,360 INFO: Starting experiment with the arguments logged below.
2023-08-01 15:08:41,360 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=5, batch_size=512, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 15:08:41,360 INFO: Setting the random number generator seed for this experiment.
2023-08-01 15:08:41,360 INFO: Loading dataset for the experiment.
2023-08-01 15:08:41,855 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 15:08:41,855 INFO: Setting up DEAP toolbox.
2023-08-01 15:08:41,855 INFO: Registering individual initialization method.
2023-08-01 15:08:41,855 INFO: Registering population initialization method.
2023-08-01 15:08:41,855 INFO: Registering the selection method.
2023-08-01 15:08:41,855 INFO: Registering the evaluation method.
2023-08-01 15:08:41,855 INFO: Registering the genetic operators.
2023-08-01 15:08:41,855 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 15:08:41,855 INFO: Initializing the initial population.
2023-08-01 15:08:41,860 INFO: Running the evolutionary algorithm.
2023-08-01 15:08:41,860 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 15:08:41,860 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 15:08:41,861 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.7425300530075277, momentum=0.9050867209528138 ,nesterov=True ,amsgrad=False ,weight_decay=0.006082049758046974 ,use_ema=False ,ema_momentum=0.9912716680067867 ,rho=0.8075647151177094 ,epsilon=0.0005441845247346118 ,centered=True ,beta_1=0.8353954234388336 ,beta_2=0.2425674210795139 ,learning_rate_power=-0.3775960112375678 ,initial_accumulator_value=0.03818884532789191 ,l1_regularization_strength=0.9342239903920753 ,l2_regularization_strength=0.721089861005094 ,l2_shrinkage_regularization_strength=0.8248657552157739 ,beta=0.12005850977012733].
2023-08-01 15:08:41,861 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.7425300530075277, momentum=0.9050867209528138 ,nesterov=True ,amsgrad=False ,weight_decay=0.006082049758046974 ,use_ema=False ,ema_momentum=0.9912716680067867 ,rho=0.8075647151177094 ,epsilon=0.0005441845247346118 ,centered=True ,beta_1=0.8353954234388336 ,beta_2=0.2425674210795139 ,learning_rate_power=-0.3775960112375678 ,initial_accumulator_value=0.03818884532789191 ,l1_regularization_strength=0.9342239903920753 ,l2_regularization_strength=0.721089861005094 ,l2_shrinkage_regularization_strength=0.8248657552157739 ,beta=0.12005850977012733].
2023-08-01 15:08:41,938 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:08:41,942 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:08:41,976 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:08:41,979 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:08:42,013 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:08:42,016 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:08:42,042 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:08:42,045 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:08:42,071 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:08:42,074 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:08:43,228 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:08:44,601 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:09:04,181 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.6912743930284007, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.5963855038348723 ,rho=0.525957003552994 ,epsilon=0.0005804474933624506 ,centered=False ,beta_1=0.004455105041929919 ,beta_2=0.549006559881552 ,learning_rate_power=-0.3689347804055705 ,initial_accumulator_value=0.940257480615582 ,l1_regularization_strength=0.12942889551035253 ,l2_regularization_strength=0.5774308338175362 ,l2_shrinkage_regularization_strength=0.6941077765293133 ,beta=0.41762604170171236].
2023-08-01 15:09:04,181 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.6912743930284007, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.5963855038348723 ,rho=0.525957003552994 ,epsilon=0.0005804474933624506 ,centered=False ,beta_1=0.004455105041929919 ,beta_2=0.549006559881552 ,learning_rate_power=-0.3689347804055705 ,initial_accumulator_value=0.940257480615582 ,l1_regularization_strength=0.12942889551035253 ,l2_regularization_strength=0.5774308338175362 ,l2_shrinkage_regularization_strength=0.6941077765293133 ,beta=0.41762604170171236].
2023-08-01 15:09:05,329 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:09:06,661 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:09:22,220 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.028277429901295337, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.00022120660890532966 ,use_ema=False ,ema_momentum=0.33343226263928694 ,rho=0.8458726695218063 ,epsilon=0.0009402586347640147 ,centered=False ,beta_1=0.9383194184782317 ,beta_2=0.8625990785777303 ,learning_rate_power=-0.8028466419269409 ,initial_accumulator_value=0.9780859237761315 ,l1_regularization_strength=0.1975725493308753 ,l2_regularization_strength=0.5203385327799472 ,l2_shrinkage_regularization_strength=0.21398787898385052 ,beta=0.05673472240337518].
2023-08-01 15:09:22,220 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.028277429901295337, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.00022120660890532966 ,use_ema=False ,ema_momentum=0.33343226263928694 ,rho=0.8458726695218063 ,epsilon=0.0009402586347640147 ,centered=False ,beta_1=0.9383194184782317 ,beta_2=0.8625990785777303 ,learning_rate_power=-0.8028466419269409 ,initial_accumulator_value=0.9780859237761315 ,l1_regularization_strength=0.1975725493308753 ,l2_regularization_strength=0.5203385327799472 ,l2_shrinkage_regularization_strength=0.21398787898385052 ,beta=0.05673472240337518].
2023-08-01 15:09:23,387 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:09:24,841 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:09:39,639 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.35415973996804984, momentum=0.6595263151529402 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.47722593460392215 ,rho=0.24943796264935547 ,epsilon=0.0009129539853162154 ,centered=False ,beta_1=0.22991356608473112 ,beta_2=0.05980486200688706 ,learning_rate_power=-0.3415648047951412 ,initial_accumulator_value=0.4501142471215268 ,l1_regularization_strength=0.006128526183984184 ,l2_regularization_strength=0.8713370039690518 ,l2_shrinkage_regularization_strength=0.5336279145461322 ,beta=0.5625264481828263].
2023-08-01 15:09:39,640 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.35415973996804984, momentum=0.6595263151529402 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.47722593460392215 ,rho=0.24943796264935547 ,epsilon=0.0009129539853162154 ,centered=False ,beta_1=0.22991356608473112 ,beta_2=0.05980486200688706 ,learning_rate_power=-0.3415648047951412 ,initial_accumulator_value=0.4501142471215268 ,l1_regularization_strength=0.006128526183984184 ,l2_regularization_strength=0.8713370039690518 ,l2_shrinkage_regularization_strength=0.5336279145461322 ,beta=0.5625264481828263].
2023-08-01 15:09:40,816 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:09:44,354 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:10:07,363 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.20065623579875302, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.00029129295662495163 ,use_ema=False ,ema_momentum=0.9664234245549133 ,rho=0.25586338588102453 ,epsilon=0.00011372205779865872 ,centered=True ,beta_1=0.8369823221541979 ,beta_2=0.0685922175192063 ,learning_rate_power=-0.10668207708692323 ,initial_accumulator_value=0.6707502061757573 ,l1_regularization_strength=0.5353542081092592 ,l2_regularization_strength=0.39935875122453124 ,l2_shrinkage_regularization_strength=0.5261077474079624 ,beta=0.2553134545528608].
2023-08-01 15:10:07,363 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.20065623579875302, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.00029129295662495163 ,use_ema=False ,ema_momentum=0.9664234245549133 ,rho=0.25586338588102453 ,epsilon=0.00011372205779865872 ,centered=True ,beta_1=0.8369823221541979 ,beta_2=0.0685922175192063 ,learning_rate_power=-0.10668207708692323 ,initial_accumulator_value=0.6707502061757573 ,l1_regularization_strength=0.5353542081092592 ,l2_regularization_strength=0.39935875122453124 ,l2_shrinkage_regularization_strength=0.5261077474079624 ,beta=0.2553134545528608].
2023-08-01 15:10:08,537 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:10:09,886 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:10:26,284 INFO: Applying selection operators for generation 1.
2023-08-01 15:10:26,285 INFO: Applying genetic operators for generation 1.
2023-08-01 15:10:26,286 INFO: Evaluating fitness for for generation 1.
2023-08-01 15:10:26,287 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 15:10:26,287 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.7425300530075277, momentum=0.9050867209528138 ,nesterov=True ,amsgrad=False ,weight_decay=0.006082049758046974 ,use_ema=True ,ema_momentum=0.47722593460392215 ,rho=0.24943796264935547 ,epsilon=0.0009129539853162154 ,centered=False ,beta_1=0.22991356608473112 ,beta_2=0.05980486200688706 ,learning_rate_power=-0.3415648047951412 ,initial_accumulator_value=0.4501142471215268 ,l1_regularization_strength=0.006128526183984184 ,l2_regularization_strength=0.8713370039690518 ,l2_shrinkage_regularization_strength=0.8248657552157739 ,beta=0.12005850977012733].
2023-08-01 15:10:26,287 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.7425300530075277, momentum=0.9050867209528138 ,nesterov=True ,amsgrad=False ,weight_decay=0.006082049758046974 ,use_ema=True ,ema_momentum=0.47722593460392215 ,rho=0.24943796264935547 ,epsilon=0.0009129539853162154 ,centered=False ,beta_1=0.22991356608473112 ,beta_2=0.05980486200688706 ,learning_rate_power=-0.3415648047951412 ,initial_accumulator_value=0.4501142471215268 ,l1_regularization_strength=0.006128526183984184 ,l2_regularization_strength=0.8713370039690518 ,l2_shrinkage_regularization_strength=0.8248657552157739 ,beta=0.12005850977012733].
2023-08-01 15:11:09,476 INFO: Starting experiment with the arguments logged below.
2023-08-01 15:11:09,476 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 15:11:09,476 INFO: Setting the random number generator seed for this experiment.
2023-08-01 15:11:09,476 INFO: Loading dataset for the experiment.
2023-08-01 15:11:09,974 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 15:11:09,975 INFO: Setting up DEAP toolbox.
2023-08-01 15:11:09,975 INFO: Registering individual initialization method.
2023-08-01 15:11:09,975 INFO: Registering population initialization method.
2023-08-01 15:11:09,975 INFO: Registering the selection method.
2023-08-01 15:11:09,975 INFO: Registering the evaluation method.
2023-08-01 15:11:09,975 INFO: Registering the genetic operators.
2023-08-01 15:11:09,975 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 15:11:09,975 INFO: Initializing the initial population.
2023-08-01 15:11:09,981 INFO: Running the evolutionary algorithm.
2023-08-01 15:11:09,981 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 15:11:09,981 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 15:11:09,981 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.5246049775814371, momentum=0.9825727392362238 ,nesterov=True ,amsgrad=False ,weight_decay=0.0002768727400031105 ,use_ema=False ,ema_momentum=0.11800702129142426 ,rho=0.10503235518496767 ,epsilon=0.0007122465871924696 ,centered=True ,beta_1=0.15657680593288859 ,beta_2=0.7692247466804153 ,learning_rate_power=-0.03869767232546506 ,initial_accumulator_value=0.24290143499956562 ,l1_regularization_strength=0.573982218531038 ,l2_regularization_strength=0.8491985899399194 ,l2_shrinkage_regularization_strength=0.9826945987805817 ,beta=0.300153063606587].
2023-08-01 15:11:09,982 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.5246049775814371, momentum=0.9825727392362238 ,nesterov=True ,amsgrad=False ,weight_decay=0.0002768727400031105 ,use_ema=False ,ema_momentum=0.11800702129142426 ,rho=0.10503235518496767 ,epsilon=0.0007122465871924696 ,centered=True ,beta_1=0.15657680593288859 ,beta_2=0.7692247466804153 ,learning_rate_power=-0.03869767232546506 ,initial_accumulator_value=0.24290143499956562 ,l1_regularization_strength=0.573982218531038 ,l2_regularization_strength=0.8491985899399194 ,l2_shrinkage_regularization_strength=0.9826945987805817 ,beta=0.300153063606587].
2023-08-01 15:11:10,060 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:11:10,064 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:11:10,099 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:11:10,102 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:11:10,137 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:11:10,140 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:11:10,168 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:11:10,171 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:11:10,199 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:11:10,202 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:11:11,368 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:11:12,763 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:11:40,179 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.6457406746861795, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.922002039883726 ,rho=0.77196155606432 ,epsilon=0.0008163423555156741 ,centered=False ,beta_1=0.9904960686764337 ,beta_2=0.27820545463385904 ,learning_rate_power=-0.6561248985085497 ,initial_accumulator_value=0.11736991667028007 ,l1_regularization_strength=0.032931247471038594 ,l2_regularization_strength=0.6993462766427089 ,l2_shrinkage_regularization_strength=0.9274318728564154 ,beta=0.5102962127800617].
2023-08-01 15:11:40,179 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.6457406746861795, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.922002039883726 ,rho=0.77196155606432 ,epsilon=0.0008163423555156741 ,centered=False ,beta_1=0.9904960686764337 ,beta_2=0.27820545463385904 ,learning_rate_power=-0.6561248985085497 ,initial_accumulator_value=0.11736991667028007 ,l1_regularization_strength=0.032931247471038594 ,l2_regularization_strength=0.6993462766427089 ,l2_shrinkage_regularization_strength=0.9274318728564154 ,beta=0.5102962127800617].
2023-08-01 15:11:41,335 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:11:43,007 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:12:05,046 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.5000500346292096, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.009992188417984549 ,use_ema=False ,ema_momentum=0.8863874428308612 ,rho=0.1844422421914922 ,epsilon=8.142120328965274e-05 ,centered=False ,beta_1=0.38052100922892707 ,beta_2=0.522244641704116 ,learning_rate_power=-0.5665645452154798 ,initial_accumulator_value=0.4777999337934449 ,l1_regularization_strength=0.4716419068597686 ,l2_regularization_strength=0.5713170042222024 ,l2_shrinkage_regularization_strength=0.5880625386851432 ,beta=0.6173745453768982].
2023-08-01 15:12:05,046 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.5000500346292096, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.009992188417984549 ,use_ema=False ,ema_momentum=0.8863874428308612 ,rho=0.1844422421914922 ,epsilon=8.142120328965274e-05 ,centered=False ,beta_1=0.38052100922892707 ,beta_2=0.522244641704116 ,learning_rate_power=-0.5665645452154798 ,initial_accumulator_value=0.4777999337934449 ,l1_regularization_strength=0.4716419068597686 ,l2_regularization_strength=0.5713170042222024 ,l2_shrinkage_regularization_strength=0.5880625386851432 ,beta=0.6173745453768982].
2023-08-01 15:12:06,500 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:12:07,949 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:12:29,474 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.989124784168335, momentum=0.010086508852938936 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.5670297035334338 ,rho=0.8837306955594589 ,epsilon=0.000897301750986267 ,centered=False ,beta_1=0.5013798941763055 ,beta_2=0.5901700928166996 ,learning_rate_power=-0.12430635154510716 ,initial_accumulator_value=0.7048615749709893 ,l1_regularization_strength=0.8663865209389617 ,l2_regularization_strength=0.437254622678245 ,l2_shrinkage_regularization_strength=0.11850436678438181 ,beta=0.757101642166868].
2023-08-01 15:12:29,475 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.989124784168335, momentum=0.010086508852938936 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.5670297035334338 ,rho=0.8837306955594589 ,epsilon=0.000897301750986267 ,centered=False ,beta_1=0.5013798941763055 ,beta_2=0.5901700928166996 ,learning_rate_power=-0.12430635154510716 ,initial_accumulator_value=0.7048615749709893 ,l1_regularization_strength=0.8663865209389617 ,l2_regularization_strength=0.437254622678245 ,l2_shrinkage_regularization_strength=0.11850436678438181 ,beta=0.757101642166868].
2023-08-01 15:12:30,657 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:12:34,116 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:13:03,967 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.38482769505572745, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0076972446431620364 ,use_ema=False ,ema_momentum=0.6783133771141284 ,rho=0.9815812364443961 ,epsilon=0.0005086265302774054 ,centered=True ,beta_1=0.27039219580891005 ,beta_2=0.6743135865031016 ,learning_rate_power=-0.31589530544553057 ,initial_accumulator_value=0.4275438513284453 ,l1_regularization_strength=0.8985003365359837 ,l2_regularization_strength=0.9010944829041844 ,l2_shrinkage_regularization_strength=0.6621850174477079 ,beta=0.10326185624465944].
2023-08-01 15:13:03,967 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.38482769505572745, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0076972446431620364 ,use_ema=False ,ema_momentum=0.6783133771141284 ,rho=0.9815812364443961 ,epsilon=0.0005086265302774054 ,centered=True ,beta_1=0.27039219580891005 ,beta_2=0.6743135865031016 ,learning_rate_power=-0.31589530544553057 ,initial_accumulator_value=0.4275438513284453 ,l1_regularization_strength=0.8985003365359837 ,l2_regularization_strength=0.9010944829041844 ,l2_shrinkage_regularization_strength=0.6621850174477079 ,beta=0.10326185624465944].
2023-08-01 15:13:05,114 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:13:06,460 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:13:29,476 INFO: Applying selection operators for generation 1.
2023-08-01 15:13:29,476 INFO: Applying genetic operators for generation 1.
2023-08-01 15:13:29,478 INFO: Evaluating fitness for for generation 1.
2023-08-01 15:13:29,478 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 15:13:29,478 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.5246049775814371, momentum=0.9825727392362238 ,nesterov=True ,amsgrad=False ,weight_decay=0.0002768727400031105 ,use_ema=False ,ema_momentum=0.11800702129142426 ,rho=0.10503235518496767 ,epsilon=0.0007122465871924696 ,centered=True ,beta_1=0.15657680593288859 ,beta_2=0.7692247466804153 ,learning_rate_power=-0.03869767232546506 ,initial_accumulator_value=0.24290143499956562 ,l1_regularization_strength=0.573982218531038 ,l2_regularization_strength=0.8491985899399194 ,l2_shrinkage_regularization_strength=0.9826945987805817 ,beta=0.300153063606587].
2023-08-01 15:13:29,478 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.5246049775814371, momentum=0.9825727392362238 ,nesterov=True ,amsgrad=False ,weight_decay=0.0002768727400031105 ,use_ema=False ,ema_momentum=0.11800702129142426 ,rho=0.10503235518496767 ,epsilon=0.0007122465871924696 ,centered=True ,beta_1=0.15657680593288859 ,beta_2=0.7692247466804153 ,learning_rate_power=-0.03869767232546506 ,initial_accumulator_value=0.24290143499956562 ,l1_regularization_strength=0.573982218531038 ,l2_regularization_strength=0.8491985899399194 ,l2_shrinkage_regularization_strength=0.9826945987805817 ,beta=0.300153063606587].
2023-08-01 15:13:53,658 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.5246049775814371, momentum=0.9825727392362238 ,nesterov=True ,amsgrad=False ,weight_decay=0.0002768727400031105 ,use_ema=False ,ema_momentum=0.11800702129142426 ,rho=0.10503235518496767 ,epsilon=0.0007122465871924696 ,centered=True ,beta_1=0.15657680593288859 ,beta_2=0.7692247466804153 ,learning_rate_power=-0.03869767232546506 ,initial_accumulator_value=0.24290143499956562 ,l1_regularization_strength=0.573982218531038 ,l2_regularization_strength=0.8491985899399194 ,l2_shrinkage_regularization_strength=0.9826945987805817 ,beta=0.300153063606587].
2023-08-01 15:13:53,659 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.5246049775814371, momentum=0.9825727392362238 ,nesterov=True ,amsgrad=False ,weight_decay=0.0002768727400031105 ,use_ema=False ,ema_momentum=0.11800702129142426 ,rho=0.10503235518496767 ,epsilon=0.0007122465871924696 ,centered=True ,beta_1=0.15657680593288859 ,beta_2=0.7692247466804153 ,learning_rate_power=-0.03869767232546506 ,initial_accumulator_value=0.24290143499956562 ,l1_regularization_strength=0.573982218531038 ,l2_regularization_strength=0.8491985899399194 ,l2_shrinkage_regularization_strength=0.9826945987805817 ,beta=0.300153063606587].
2023-08-01 15:14:17,868 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.7480246091915047, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.012426932554301051 ,rho=0.10503235518496767 ,epsilon=0.0007122465871924696 ,centered=False ,beta_1=0.2824181144163769 ,beta_2=0.9211958209579958 ,learning_rate_power=-0.31589530544553057 ,initial_accumulator_value=0.4275438513284453 ,l1_regularization_strength=0.08977431346682918 ,l2_regularization_strength=0.8491985899399194 ,l2_shrinkage_regularization_strength=0.4406145411346788 ,beta=0.8983755877517622].
2023-08-01 15:14:17,869 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.7480246091915047, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.012426932554301051 ,rho=0.10503235518496767 ,epsilon=0.0007122465871924696 ,centered=False ,beta_1=0.2824181144163769 ,beta_2=0.9211958209579958 ,learning_rate_power=-0.31589530544553057 ,initial_accumulator_value=0.4275438513284453 ,l1_regularization_strength=0.08977431346682918 ,l2_regularization_strength=0.8491985899399194 ,l2_shrinkage_regularization_strength=0.4406145411346788 ,beta=0.8983755877517622].
2023-08-01 15:14:44,151 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.38482769505572745, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0076972446431620364 ,use_ema=False ,ema_momentum=0.6783133771141284 ,rho=0.9815812364443961 ,epsilon=0.0005086265302774054 ,centered=True ,beta_1=0.15657680593288859 ,beta_2=0.7692247466804153 ,learning_rate_power=-0.03869767232546506 ,initial_accumulator_value=0.24290143499956562 ,l1_regularization_strength=0.573982218531038 ,l2_regularization_strength=0.9010944829041844 ,l2_shrinkage_regularization_strength=0.6621850174477079 ,beta=0.10326185624465944].
2023-08-01 15:14:44,151 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.38482769505572745, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0076972446431620364 ,use_ema=False ,ema_momentum=0.6783133771141284 ,rho=0.9815812364443961 ,epsilon=0.0005086265302774054 ,centered=True ,beta_1=0.15657680593288859 ,beta_2=0.7692247466804153 ,learning_rate_power=-0.03869767232546506 ,initial_accumulator_value=0.24290143499956562 ,l1_regularization_strength=0.573982218531038 ,l2_regularization_strength=0.9010944829041844 ,l2_shrinkage_regularization_strength=0.6621850174477079 ,beta=0.10326185624465944].
2023-08-01 15:15:08,182 INFO: Applying selection operators for generation 2.
2023-08-01 15:15:08,182 INFO: Applying genetic operators for generation 2.
2023-08-01 15:15:08,183 INFO: Evaluating fitness for for generation 2.
2023-08-01 15:15:08,183 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 15:15:08,183 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.5246049775814371, momentum=0.9825727392362238 ,nesterov=True ,amsgrad=False ,weight_decay=0.0002768727400031105 ,use_ema=False ,ema_momentum=0.11800702129142426 ,rho=0.10503235518496767 ,epsilon=0.0007122465871924696 ,centered=True ,beta_1=0.15657680593288859 ,beta_2=0.7692247466804153 ,learning_rate_power=-0.03869767232546506 ,initial_accumulator_value=0.24290143499956562 ,l1_regularization_strength=0.573982218531038 ,l2_regularization_strength=0.8491985899399194 ,l2_shrinkage_regularization_strength=0.9826945987805817 ,beta=0.300153063606587].
2023-08-01 15:15:08,183 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.5246049775814371, momentum=0.9825727392362238 ,nesterov=True ,amsgrad=False ,weight_decay=0.0002768727400031105 ,use_ema=False ,ema_momentum=0.11800702129142426 ,rho=0.10503235518496767 ,epsilon=0.0007122465871924696 ,centered=True ,beta_1=0.15657680593288859 ,beta_2=0.7692247466804153 ,learning_rate_power=-0.03869767232546506 ,initial_accumulator_value=0.24290143499956562 ,l1_regularization_strength=0.573982218531038 ,l2_regularization_strength=0.8491985899399194 ,l2_shrinkage_regularization_strength=0.9826945987805817 ,beta=0.300153063606587].
2023-08-01 15:15:32,354 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.5246049775814371, momentum=0.9825727392362238 ,nesterov=True ,amsgrad=False ,weight_decay=0.0002768727400031105 ,use_ema=False ,ema_momentum=0.11800702129142426 ,rho=0.10503235518496767 ,epsilon=0.0007122465871924696 ,centered=True ,beta_1=0.15657680593288859 ,beta_2=0.7692247466804153 ,learning_rate_power=-0.03869767232546506 ,initial_accumulator_value=0.24290143499956562 ,l1_regularization_strength=0.573982218531038 ,l2_regularization_strength=0.8491985899399194 ,l2_shrinkage_regularization_strength=0.9826945987805817 ,beta=0.300153063606587].
2023-08-01 15:15:32,354 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.5246049775814371, momentum=0.9825727392362238 ,nesterov=True ,amsgrad=False ,weight_decay=0.0002768727400031105 ,use_ema=False ,ema_momentum=0.11800702129142426 ,rho=0.10503235518496767 ,epsilon=0.0007122465871924696 ,centered=True ,beta_1=0.15657680593288859 ,beta_2=0.7692247466804153 ,learning_rate_power=-0.03869767232546506 ,initial_accumulator_value=0.24290143499956562 ,l1_regularization_strength=0.573982218531038 ,l2_regularization_strength=0.8491985899399194 ,l2_shrinkage_regularization_strength=0.9826945987805817 ,beta=0.300153063606587].
2023-08-01 15:17:50,557 INFO: Starting experiment with the arguments logged below.
2023-08-01 15:17:50,557 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 15:17:50,557 INFO: Setting the random number generator seed for this experiment.
2023-08-01 15:17:50,558 INFO: Loading dataset for the experiment.
2023-08-01 15:17:51,076 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 15:17:51,076 INFO: Setting up DEAP toolbox.
2023-08-01 15:17:51,076 INFO: Registering individual initialization method.
2023-08-01 15:17:51,076 INFO: Registering population initialization method.
2023-08-01 15:17:51,076 INFO: Registering the selection method.
2023-08-01 15:17:51,076 INFO: Registering the evaluation method.
2023-08-01 15:17:51,076 INFO: Registering the genetic operators.
2023-08-01 15:17:51,076 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 15:17:51,077 INFO: Initializing the initial population.
2023-08-01 15:17:51,082 INFO: Running the evolutionary algorithm.
2023-08-01 15:17:51,083 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 15:17:51,083 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 15:17:51,083 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.7120625907207598, momentum=0.7062586799186382 ,nesterov=True ,amsgrad=False ,weight_decay=0.008680462582543217 ,use_ema=False ,ema_momentum=0.4818993251194472 ,rho=0.5801074311593375 ,epsilon=8.2372154405051e-05 ,centered=True ,beta_1=0.7839858020868511 ,beta_2=0.11732785793931799 ,learning_rate_power=-0.555006149831324 ,initial_accumulator_value=0.030240577832638338 ,l1_regularization_strength=0.7544319508610037 ,l2_regularization_strength=0.5337062709259258 ,l2_shrinkage_regularization_strength=0.31691064982985184 ,beta=0.7047504294876052].
2023-08-01 15:17:51,083 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.7120625907207598, momentum=0.7062586799186382 ,nesterov=True ,amsgrad=False ,weight_decay=0.008680462582543217 ,use_ema=False ,ema_momentum=0.4818993251194472 ,rho=0.5801074311593375 ,epsilon=8.2372154405051e-05 ,centered=True ,beta_1=0.7839858020868511 ,beta_2=0.11732785793931799 ,learning_rate_power=-0.555006149831324 ,initial_accumulator_value=0.030240577832638338 ,l1_regularization_strength=0.7544319508610037 ,l2_regularization_strength=0.5337062709259258 ,l2_shrinkage_regularization_strength=0.31691064982985184 ,beta=0.7047504294876052].
2023-08-01 15:17:51,173 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:17:51,177 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:17:51,212 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:17:51,215 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:17:51,250 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:17:51,253 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:17:51,281 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:17:51,285 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:17:51,312 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:17:51,315 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:17:52,496 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:17:53,874 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:18:21,845 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.012206658158234185, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.6083444446980455 ,rho=0.41310347309106443 ,epsilon=0.0008597473509319715 ,centered=False ,beta_1=0.5411913568174204 ,beta_2=0.5245575819397086 ,learning_rate_power=-0.05126902297020375 ,initial_accumulator_value=0.02245264872257724 ,l1_regularization_strength=0.4491007286892261 ,l2_regularization_strength=0.9056161455433214 ,l2_shrinkage_regularization_strength=0.41556150268722214 ,beta=0.8633505878964868].
2023-08-01 15:18:21,845 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.012206658158234185, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.6083444446980455 ,rho=0.41310347309106443 ,epsilon=0.0008597473509319715 ,centered=False ,beta_1=0.5411913568174204 ,beta_2=0.5245575819397086 ,learning_rate_power=-0.05126902297020375 ,initial_accumulator_value=0.02245264872257724 ,l1_regularization_strength=0.4491007286892261 ,l2_regularization_strength=0.9056161455433214 ,l2_shrinkage_regularization_strength=0.41556150268722214 ,beta=0.8633505878964868].
2023-08-01 15:18:22,992 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:18:24,624 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:18:45,748 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.8315622633797638, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.004351050638375632 ,use_ema=False ,ema_momentum=0.9560638110030051 ,rho=0.38761923489893313 ,epsilon=0.0005602475301686276 ,centered=False ,beta_1=0.9799509521871609 ,beta_2=0.46722166199841564 ,learning_rate_power=-0.06586023068867342 ,initial_accumulator_value=0.2550389878286474 ,l1_regularization_strength=0.7070219033018943 ,l2_regularization_strength=0.029050490142156327 ,l2_shrinkage_regularization_strength=0.16529930684531857 ,beta=0.10467641181703435].
2023-08-01 15:18:45,748 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.8315622633797638, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.004351050638375632 ,use_ema=False ,ema_momentum=0.9560638110030051 ,rho=0.38761923489893313 ,epsilon=0.0005602475301686276 ,centered=False ,beta_1=0.9799509521871609 ,beta_2=0.46722166199841564 ,learning_rate_power=-0.06586023068867342 ,initial_accumulator_value=0.2550389878286474 ,l1_regularization_strength=0.7070219033018943 ,l2_regularization_strength=0.029050490142156327 ,l2_shrinkage_regularization_strength=0.16529930684531857 ,beta=0.10467641181703435].
2023-08-01 15:18:47,189 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:18:48,658 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:19:09,612 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.4073551772798416, momentum=0.8903082656063777 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.43219151520354615 ,rho=0.5603625326609483 ,epsilon=0.00030731026375946754 ,centered=False ,beta_1=0.22579913912999805 ,beta_2=0.2815816582318337 ,learning_rate_power=-0.5588769182753154 ,initial_accumulator_value=0.6167926951737681 ,l1_regularization_strength=0.891168410945824 ,l2_regularization_strength=0.30895693182533357 ,l2_shrinkage_regularization_strength=0.5312070668389929 ,beta=0.6824632149421016].
2023-08-01 15:19:09,612 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.4073551772798416, momentum=0.8903082656063777 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.43219151520354615 ,rho=0.5603625326609483 ,epsilon=0.00030731026375946754 ,centered=False ,beta_1=0.22579913912999805 ,beta_2=0.2815816582318337 ,learning_rate_power=-0.5588769182753154 ,initial_accumulator_value=0.6167926951737681 ,l1_regularization_strength=0.891168410945824 ,l2_regularization_strength=0.30895693182533357 ,l2_shrinkage_regularization_strength=0.5312070668389929 ,beta=0.6824632149421016].
2023-08-01 15:19:10,765 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:19:14,198 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:19:43,364 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.004004976388880754, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.002396572904312202 ,use_ema=False ,ema_momentum=0.29779693182830613 ,rho=0.8149001230469466 ,epsilon=0.00012723633358921463 ,centered=True ,beta_1=0.1726129353594027 ,beta_2=0.9918345658894466 ,learning_rate_power=-0.7343767564490379 ,initial_accumulator_value=0.30022315060068794 ,l1_regularization_strength=0.7298500188388514 ,l2_regularization_strength=0.519599475825614 ,l2_shrinkage_regularization_strength=0.13651769144725656 ,beta=0.14173834180195255].
2023-08-01 15:19:43,364 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.004004976388880754, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.002396572904312202 ,use_ema=False ,ema_momentum=0.29779693182830613 ,rho=0.8149001230469466 ,epsilon=0.00012723633358921463 ,centered=True ,beta_1=0.1726129353594027 ,beta_2=0.9918345658894466 ,learning_rate_power=-0.7343767564490379 ,initial_accumulator_value=0.30022315060068794 ,l1_regularization_strength=0.7298500188388514 ,l2_regularization_strength=0.519599475825614 ,l2_shrinkage_regularization_strength=0.13651769144725656 ,beta=0.14173834180195255].
2023-08-01 15:19:44,523 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:19:45,902 INFO: batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1
2023-08-01 15:20:08,577 INFO: Applying selection operators for generation 1.
2023-08-01 15:20:08,578 INFO: Applying genetic operators for generation 1.
2023-08-01 15:20:08,579 INFO: Evaluating fitness for for generation 1.
2023-08-01 15:20:08,579 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 15:20:08,579 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.7120625907207598, momentum=0.7062586799186382 ,nesterov=True ,amsgrad=False ,weight_decay=0.008680462582543217 ,use_ema=True ,ema_momentum=0.43219151520354615 ,rho=0.5603625326609483 ,epsilon=0.00030731026375946754 ,centered=False ,beta_1=0.22579913912999805 ,beta_2=0.2815816582318337 ,learning_rate_power=-0.5588769182753154 ,initial_accumulator_value=0.6167926951737681 ,l1_regularization_strength=0.891168410945824 ,l2_regularization_strength=0.30895693182533357 ,l2_shrinkage_regularization_strength=0.31691064982985184 ,beta=0.7047504294876052].
2023-08-01 15:20:08,579 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.7120625907207598, momentum=0.7062586799186382 ,nesterov=True ,amsgrad=False ,weight_decay=0.008680462582543217 ,use_ema=True ,ema_momentum=0.43219151520354615 ,rho=0.5603625326609483 ,epsilon=0.00030731026375946754 ,centered=False ,beta_1=0.22579913912999805 ,beta_2=0.2815816582318337 ,learning_rate_power=-0.5588769182753154 ,initial_accumulator_value=0.6167926951737681 ,l1_regularization_strength=0.891168410945824 ,l2_regularization_strength=0.30895693182533357 ,l2_shrinkage_regularization_strength=0.31691064982985184 ,beta=0.7047504294876052].
2023-08-01 15:20:11,611 INFO: Error reported to Coordinator: 
Traceback (most recent call last):
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/training/coordinator.py", line 293, in stop_on_exception
    yield
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/distribute/mirrored_run.py", line 277, in _call_for_each_replica
    merge_result = threads[0].merge_fn(distribution, *merge_args,
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py", line 689, in wrapper
    return converted_call(f, args, kwargs, options=options)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py", line 377, in converted_call
    return _call_unconverted(f, args, kwargs, options)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py", line 458, in _call_unconverted
    return f(*args, **kwargs)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1189, in distributed_apply_weight_decay
    distribution.extended.update(
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py", line 2637, in update
    return self._update(var, fn, args, kwargs, group)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/distribute/mirrored_strategy.py", line 792, in _update
    assert isinstance(var, values.DistributedVariable)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/abc.py", line 117, in __instancecheck__
    def __instancecheck__(cls, instance):
KeyboardInterrupt
2023-08-01 15:22:29,156 INFO: Starting experiment with the arguments logged below.
2023-08-01 15:22:29,157 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 15:22:29,157 INFO: Setting the random number generator seed for this experiment.
2023-08-01 15:22:29,157 INFO: Loading dataset for the experiment.
2023-08-01 15:22:29,678 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 15:22:29,678 INFO: Setting up DEAP toolbox.
2023-08-01 15:22:29,678 INFO: Registering individual initialization method.
2023-08-01 15:22:29,678 INFO: Registering population initialization method.
2023-08-01 15:22:29,678 INFO: Registering the selection method.
2023-08-01 15:22:29,678 INFO: Registering the evaluation method.
2023-08-01 15:22:29,678 INFO: Registering the genetic operators.
2023-08-01 15:22:29,678 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 15:22:29,679 INFO: Initializing the initial population.
2023-08-01 15:22:29,684 INFO: Running the evolutionary algorithm.
2023-08-01 15:22:29,685 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 15:22:29,685 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 15:22:29,685 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.6577926934419697, momentum=0.5154041765208547 ,nesterov=True ,amsgrad=False ,weight_decay=0.007524484644991123 ,use_ema=False ,ema_momentum=0.2693986519497267 ,rho=0.8607276185743826 ,epsilon=0.0003605195697989427 ,centered=True ,beta_1=0.129515366733295 ,beta_2=0.7258067184172337 ,learning_rate_power=-0.28564129118177406 ,initial_accumulator_value=0.16854586166486674 ,l1_regularization_strength=0.519595858553775 ,l2_regularization_strength=0.6851214161579285 ,l2_shrinkage_regularization_strength=0.5042110492265188 ,beta=0.4643603465768712].
2023-08-01 15:22:29,685 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.6577926934419697, momentum=0.5154041765208547 ,nesterov=True ,amsgrad=False ,weight_decay=0.007524484644991123 ,use_ema=False ,ema_momentum=0.2693986519497267 ,rho=0.8607276185743826 ,epsilon=0.0003605195697989427 ,centered=True ,beta_1=0.129515366733295 ,beta_2=0.7258067184172337 ,learning_rate_power=-0.28564129118177406 ,initial_accumulator_value=0.16854586166486674 ,l1_regularization_strength=0.519595858553775 ,l2_regularization_strength=0.6851214161579285 ,l2_shrinkage_regularization_strength=0.5042110492265188 ,beta=0.4643603465768712].
2023-08-01 15:22:29,752 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:22:29,755 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:22:29,778 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:22:29,781 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:22:29,804 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:22:29,806 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:22:29,826 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:22:29,828 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:22:29,848 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:22:29,850 INFO: Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:22:59,403 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:22:59,403 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:23:23,130 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6202305842528373, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0008796622664003392 ,use_ema=False ,ema_momentum=0.5456122960091104 ,rho=0.24483135369438114 ,epsilon=0.0004929981166868456 ,centered=False ,beta_1=0.7764632902775418 ,beta_2=0.4129266187301144 ,learning_rate_power=-0.32137805792011387 ,initial_accumulator_value=0.3302337136833552 ,l1_regularization_strength=0.7493359229911463 ,l2_regularization_strength=0.7799833121600964 ,l2_shrinkage_regularization_strength=0.889928227180419 ,beta=0.8301499811578963].
2023-08-01 15:23:23,130 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6202305842528373, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0008796622664003392 ,use_ema=False ,ema_momentum=0.5456122960091104 ,rho=0.24483135369438114 ,epsilon=0.0004929981166868456 ,centered=False ,beta_1=0.7764632902775418 ,beta_2=0.4129266187301144 ,learning_rate_power=-0.32137805792011387 ,initial_accumulator_value=0.3302337136833552 ,l1_regularization_strength=0.7493359229911463 ,l2_regularization_strength=0.7799833121600964 ,l2_shrinkage_regularization_strength=0.889928227180419 ,beta=0.8301499811578963].
2023-08-01 15:23:46,292 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.45115669079154375, momentum=0.351510550551992 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9973640775135687 ,rho=0.3185080756535048 ,epsilon=0.0004492943044109562 ,centered=False ,beta_1=0.13151516977601985 ,beta_2=0.08883661150942512 ,learning_rate_power=-0.7095537661800123 ,initial_accumulator_value=0.7814498580277127 ,l1_regularization_strength=0.9425699585638754 ,l2_regularization_strength=0.2628939292723358 ,l2_shrinkage_regularization_strength=0.3917629291744118 ,beta=0.15014827026450772].
2023-08-01 15:23:46,292 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.45115669079154375, momentum=0.351510550551992 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9973640775135687 ,rho=0.3185080756535048 ,epsilon=0.0004492943044109562 ,centered=False ,beta_1=0.13151516977601985 ,beta_2=0.08883661150942512 ,learning_rate_power=-0.7095537661800123 ,initial_accumulator_value=0.7814498580277127 ,l1_regularization_strength=0.9425699585638754 ,l2_regularization_strength=0.2628939292723358 ,l2_shrinkage_regularization_strength=0.3917629291744118 ,beta=0.15014827026450772].
2023-08-01 15:24:16,138 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.43571487588135815, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.005391348852477871 ,use_ema=False ,ema_momentum=0.2274900862095135 ,rho=0.39070150845602314 ,epsilon=0.0006948849825070811 ,centered=True ,beta_1=0.3483010524122434 ,beta_2=0.8431876993224326 ,learning_rate_power=-0.9065232156686297 ,initial_accumulator_value=0.8053862782388946 ,l1_regularization_strength=0.6157613165420236 ,l2_regularization_strength=0.8842782978823087 ,l2_shrinkage_regularization_strength=0.5487165604905465 ,beta=0.7313472384404719].
2023-08-01 15:24:16,139 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.43571487588135815, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.005391348852477871 ,use_ema=False ,ema_momentum=0.2274900862095135 ,rho=0.39070150845602314 ,epsilon=0.0006948849825070811 ,centered=True ,beta_1=0.3483010524122434 ,beta_2=0.8431876993224326 ,learning_rate_power=-0.9065232156686297 ,initial_accumulator_value=0.8053862782388946 ,l1_regularization_strength=0.6157613165420236 ,l2_regularization_strength=0.8842782978823087 ,l2_shrinkage_regularization_strength=0.5487165604905465 ,beta=0.7313472384404719].
2023-08-01 15:24:40,277 INFO: Applying selection operators for generation 1.
2023-08-01 15:24:40,278 INFO: Applying genetic operators for generation 1.
2023-08-01 15:24:40,279 INFO: Evaluating fitness for for generation 1.
2023-08-01 15:24:40,279 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 15:24:40,279 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.6577926934419697, momentum=0.5154041765208547 ,nesterov=True ,amsgrad=False ,weight_decay=0.007524484644991123 ,use_ema=False ,ema_momentum=0.2693986519497267 ,rho=0.8607276185743826 ,epsilon=0.0003605195697989427 ,centered=True ,beta_1=0.129515366733295 ,beta_2=0.7258067184172337 ,learning_rate_power=-0.28564129118177406 ,initial_accumulator_value=0.16854586166486674 ,l1_regularization_strength=0.519595858553775 ,l2_regularization_strength=0.6851214161579285 ,l2_shrinkage_regularization_strength=0.5042110492265188 ,beta=0.4643603465768712].
2023-08-01 15:24:40,279 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.6577926934419697, momentum=0.5154041765208547 ,nesterov=True ,amsgrad=False ,weight_decay=0.007524484644991123 ,use_ema=False ,ema_momentum=0.2693986519497267 ,rho=0.8607276185743826 ,epsilon=0.0003605195697989427 ,centered=True ,beta_1=0.129515366733295 ,beta_2=0.7258067184172337 ,learning_rate_power=-0.28564129118177406 ,initial_accumulator_value=0.16854586166486674 ,l1_regularization_strength=0.519595858553775 ,l2_regularization_strength=0.6851214161579285 ,l2_shrinkage_regularization_strength=0.5042110492265188 ,beta=0.4643603465768712].
2023-08-01 15:25:04,717 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.6577926934419697, momentum=0.5154041765208547 ,nesterov=True ,amsgrad=False ,weight_decay=0.007524484644991123 ,use_ema=False ,ema_momentum=0.2693986519497267 ,rho=0.8607276185743826 ,epsilon=0.0003605195697989427 ,centered=True ,beta_1=0.129515366733295 ,beta_2=0.7258067184172337 ,learning_rate_power=-0.28564129118177406 ,initial_accumulator_value=0.16854586166486674 ,l1_regularization_strength=0.519595858553775 ,l2_regularization_strength=0.6851214161579285 ,l2_shrinkage_regularization_strength=0.5042110492265188 ,beta=0.4643603465768712].
2023-08-01 15:25:04,717 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.6577926934419697, momentum=0.5154041765208547 ,nesterov=True ,amsgrad=False ,weight_decay=0.007524484644991123 ,use_ema=False ,ema_momentum=0.2693986519497267 ,rho=0.8607276185743826 ,epsilon=0.0003605195697989427 ,centered=True ,beta_1=0.129515366733295 ,beta_2=0.7258067184172337 ,learning_rate_power=-0.28564129118177406 ,initial_accumulator_value=0.16854586166486674 ,l1_regularization_strength=0.519595858553775 ,l2_regularization_strength=0.6851214161579285 ,l2_shrinkage_regularization_strength=0.5042110492265188 ,beta=0.4643603465768712].
2023-08-01 15:25:28,263 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.7100237420207484, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.4154084525265467 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2539620814665541 ,beta_2=0.33553523099095384 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.06036623790322149 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.8938831498600147 ,beta=0.07134976840457175].
2023-08-01 15:25:28,264 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.7100237420207484, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.4154084525265467 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2539620814665541 ,beta_2=0.33553523099095384 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.06036623790322149 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.8938831498600147 ,beta=0.07134976840457175].
2023-08-01 15:25:53,967 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:25:53,967 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:26:16,838 INFO: Applying selection operators for generation 2.
2023-08-01 15:26:16,838 INFO: Applying genetic operators for generation 2.
2023-08-01 15:26:16,838 INFO: Evaluating fitness for for generation 2.
2023-08-01 15:26:16,838 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 15:26:16,839 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:26:16,839 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:26:39,590 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:26:39,591 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:27:02,079 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:27:02,080 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:27:24,783 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:27:24,784 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:27:47,180 INFO: Applying selection operators for generation 3.
2023-08-01 15:27:47,181 INFO: Applying genetic operators for generation 3.
2023-08-01 15:27:47,182 INFO: Evaluating fitness for for generation 3.
2023-08-01 15:27:47,182 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 15:27:47,182 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:27:47,182 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:28:10,082 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:28:10,083 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:28:32,902 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:28:32,902 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:28:55,417 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:28:55,417 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:29:18,201 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.7282408211123946, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.7967812323020043 ,rho=0.5956521384193347 ,epsilon=0.0005155882613310563 ,centered=True ,beta_1=0.2579749572405039 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.04729506536860928 ,beta=0.7008246621028179].
2023-08-01 15:29:18,201 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.7282408211123946, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.7967812323020043 ,rho=0.5956521384193347 ,epsilon=0.0005155882613310563 ,centered=True ,beta_1=0.2579749572405039 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.04729506536860928 ,beta=0.7008246621028179].
2023-08-01 15:29:42,694 INFO: Applying selection operators for generation 4.
2023-08-01 15:29:42,694 INFO: Applying genetic operators for generation 4.
2023-08-01 15:29:42,695 INFO: Evaluating fitness for for generation 4.
2023-08-01 15:29:42,695 INFO: Will evaluate fitness for 3 individuals.
2023-08-01 15:29:42,695 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:29:42,695 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:30:05,381 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:30:05,381 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:30:28,233 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.7928568997630736, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.46750995043426413 ,epsilon=0.00035011640997923966 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.14813198536857386 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.6416844757075734 ,beta=0.7008246621028179].
2023-08-01 15:30:28,233 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.7928568997630736, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.46750995043426413 ,epsilon=0.00035011640997923966 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.14813198536857386 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.6416844757075734 ,beta=0.7008246621028179].
2023-08-01 15:30:52,216 INFO: Applying selection operators for generation 5.
2023-08-01 15:30:52,216 INFO: Applying genetic operators for generation 5.
2023-08-01 15:30:52,217 INFO: Evaluating fitness for for generation 5.
2023-08-01 15:30:52,217 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 15:30:52,217 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:30:52,218 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:31:14,992 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.3549044912590388, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.6415529501987858 ,rho=0.5255473633027499 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.8109354724233283 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.8086167982025761 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.8507372240681459 ,beta=0.7008246621028179].
2023-08-01 15:31:14,993 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.3549044912590388, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.6415529501987858 ,rho=0.5255473633027499 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.8109354724233283 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.8086167982025761 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.8507372240681459 ,beta=0.7008246621028179].
2023-08-01 15:31:39,247 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:31:39,247 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:32:01,873 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:32:01,873 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.10949822663024611, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8955461472533908 ,rho=0.48241425282216743 ,epsilon=0.0009648742006836546 ,centered=False ,beta_1=0.2641155567557327 ,beta_2=0.646204132166553 ,learning_rate_power=-0.9348512536299523 ,initial_accumulator_value=0.08336458037211714 ,l1_regularization_strength=0.34631578818707986 ,l2_regularization_strength=0.23627719715951812 ,l2_shrinkage_regularization_strength=0.897566417314721 ,beta=0.7008246621028179].
2023-08-01 15:32:24,619 INFO: Saving the results to the folder specified in the arguments.
2023-08-01 15:32:24,622 INFO: Experiment finished.
2023-08-01 15:39:04,757 INFO: Starting experiment with the arguments logged below.
2023-08-01 15:39:04,758 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 15:39:04,758 INFO: Setting the random number generator seed for this experiment.
2023-08-01 15:39:04,758 INFO: Loading dataset for the experiment.
2023-08-01 15:39:05,283 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 15:39:05,283 INFO: Setting up DEAP toolbox.
2023-08-01 15:39:05,283 INFO: Registering individual initialization method.
2023-08-01 15:39:05,283 INFO: Registering population initialization method.
2023-08-01 15:39:05,283 INFO: Registering the selection method.
2023-08-01 15:39:05,283 INFO: Registering the evaluation method.
2023-08-01 15:39:05,283 INFO: Registering the genetic operators.
2023-08-01 15:39:05,283 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 15:39:05,283 INFO: Initializing the initial population.
2023-08-01 15:39:05,289 INFO: Running the evolutionary algorithm.
2023-08-01 15:39:05,289 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 15:39:05,289 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 15:39:05,290 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.14573140426756026, momentum=0.3349518794216967 ,nesterov=True ,amsgrad=False ,weight_decay=0.008261333167112462 ,use_ema=False ,ema_momentum=0.13644859900695416 ,rho=0.13860226678035137 ,epsilon=0.0009783937356038874 ,centered=True ,beta_1=0.2686385662733858 ,beta_2=0.3796283273950767 ,learning_rate_power=-0.14109115430969044 ,initial_accumulator_value=0.5735052495686196 ,l1_regularization_strength=0.566850227646308 ,l2_regularization_strength=0.06475263415275312 ,l2_shrinkage_regularization_strength=0.23418845023314827 ,beta=0.6906878179933996].
2023-08-01 15:39:05,290 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.14573140426756026, momentum=0.3349518794216967 ,nesterov=True ,amsgrad=False ,weight_decay=0.008261333167112462 ,use_ema=False ,ema_momentum=0.13644859900695416 ,rho=0.13860226678035137 ,epsilon=0.0009783937356038874 ,centered=True ,beta_1=0.2686385662733858 ,beta_2=0.3796283273950767 ,learning_rate_power=-0.14109115430969044 ,initial_accumulator_value=0.5735052495686196 ,l1_regularization_strength=0.566850227646308 ,l2_regularization_strength=0.06475263415275312 ,l2_shrinkage_regularization_strength=0.23418845023314827 ,beta=0.6906878179933996].
2023-08-01 15:39:34,328 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.8438305244345418, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.3739958142648584 ,rho=0.37518531558826307 ,epsilon=0.0005448746021920731 ,centered=False ,beta_1=0.7003967903167609 ,beta_2=0.8416587370395208 ,learning_rate_power=-0.977017534839448 ,initial_accumulator_value=0.5817607247766156 ,l1_regularization_strength=0.10609793483560181 ,l2_regularization_strength=0.415553258298041 ,l2_shrinkage_regularization_strength=0.10928908861012754 ,beta=0.3606163400796726].
2023-08-01 15:39:34,328 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.8438305244345418, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.3739958142648584 ,rho=0.37518531558826307 ,epsilon=0.0005448746021920731 ,centered=False ,beta_1=0.7003967903167609 ,beta_2=0.8416587370395208 ,learning_rate_power=-0.977017534839448 ,initial_accumulator_value=0.5817607247766156 ,l1_regularization_strength=0.10609793483560181 ,l2_regularization_strength=0.415553258298041 ,l2_shrinkage_regularization_strength=0.10928908861012754 ,beta=0.3606163400796726].
2023-08-01 15:39:57,467 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.10767085905943719, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.004783046675467677 ,use_ema=False ,ema_momentum=0.9410029680430394 ,rho=0.12942622236235568 ,epsilon=0.0005595841970274723 ,centered=False ,beta_1=0.46747739786762665 ,beta_2=0.4575910065462617 ,learning_rate_power=-0.040460266384405585 ,initial_accumulator_value=0.9362883852508103 ,l1_regularization_strength=0.6122952460117115 ,l2_regularization_strength=0.263310477071511 ,l2_shrinkage_regularization_strength=0.9142861187172402 ,beta=0.8815176822857375].
2023-08-01 15:39:57,468 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.10767085905943719, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.004783046675467677 ,use_ema=False ,ema_momentum=0.9410029680430394 ,rho=0.12942622236235568 ,epsilon=0.0005595841970274723 ,centered=False ,beta_1=0.46747739786762665 ,beta_2=0.4575910065462617 ,learning_rate_power=-0.040460266384405585 ,initial_accumulator_value=0.9362883852508103 ,l1_regularization_strength=0.6122952460117115 ,l2_regularization_strength=0.263310477071511 ,l2_shrinkage_regularization_strength=0.9142861187172402 ,beta=0.8815176822857375].
2023-08-01 15:40:22,007 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.0884187566019965, momentum=0.563219213994087 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8714691725222887 ,rho=0.6497160417476903 ,epsilon=3.1477151813857896e-05 ,centered=False ,beta_1=0.9088466301915412 ,beta_2=0.301001655435998 ,learning_rate_power=-0.37078788331946866 ,initial_accumulator_value=0.24987453592204967 ,l1_regularization_strength=0.51662718648596 ,l2_regularization_strength=0.589542374791538 ,l2_shrinkage_regularization_strength=0.807856015676652 ,beta=0.6734877191623315].
2023-08-01 15:40:22,008 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.0884187566019965, momentum=0.563219213994087 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8714691725222887 ,rho=0.6497160417476903 ,epsilon=3.1477151813857896e-05 ,centered=False ,beta_1=0.9088466301915412 ,beta_2=0.301001655435998 ,learning_rate_power=-0.37078788331946866 ,initial_accumulator_value=0.24987453592204967 ,l1_regularization_strength=0.51662718648596 ,l2_regularization_strength=0.589542374791538 ,l2_shrinkage_regularization_strength=0.807856015676652 ,beta=0.6734877191623315].
2023-08-01 15:40:51,668 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.5157720350040594, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.001765130878385043 ,use_ema=False ,ema_momentum=0.2029481308850032 ,rho=0.1311673468312543 ,epsilon=0.0005495740049307729 ,centered=True ,beta_1=0.5260599405438435 ,beta_2=0.21079307999125563 ,learning_rate_power=-0.6609699737403246 ,initial_accumulator_value=0.4109009370540041 ,l1_regularization_strength=0.30462473231142895 ,l2_regularization_strength=0.6342552041560381 ,l2_shrinkage_regularization_strength=0.9933953582177127 ,beta=0.4579792894436092].
2023-08-01 15:40:51,668 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.5157720350040594, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.001765130878385043 ,use_ema=False ,ema_momentum=0.2029481308850032 ,rho=0.1311673468312543 ,epsilon=0.0005495740049307729 ,centered=True ,beta_1=0.5260599405438435 ,beta_2=0.21079307999125563 ,learning_rate_power=-0.6609699737403246 ,initial_accumulator_value=0.4109009370540041 ,l1_regularization_strength=0.30462473231142895 ,l2_regularization_strength=0.6342552041560381 ,l2_shrinkage_regularization_strength=0.9933953582177127 ,beta=0.4579792894436092].
2023-08-01 15:41:15,794 INFO: Applying selection operators for generation 1.
2023-08-01 15:41:15,794 INFO: Applying genetic operators for generation 1.
2023-08-01 15:41:15,796 INFO: Evaluating fitness for for generation 1.
2023-08-01 15:41:15,796 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 15:41:15,796 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.14573140426756026, momentum=0.3349518794216967 ,nesterov=True ,amsgrad=False ,weight_decay=0.008261333167112462 ,use_ema=False ,ema_momentum=0.9410029680430394 ,rho=0.12942622236235568 ,epsilon=0.0005595841970274723 ,centered=False ,beta_1=0.46747739786762665 ,beta_2=0.4575910065462617 ,learning_rate_power=-0.040460266384405585 ,initial_accumulator_value=0.9362883852508103 ,l1_regularization_strength=0.6122952460117115 ,l2_regularization_strength=0.263310477071511 ,l2_shrinkage_regularization_strength=0.23418845023314827 ,beta=0.6906878179933996].
2023-08-01 15:41:15,796 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.14573140426756026, momentum=0.3349518794216967 ,nesterov=True ,amsgrad=False ,weight_decay=0.008261333167112462 ,use_ema=False ,ema_momentum=0.9410029680430394 ,rho=0.12942622236235568 ,epsilon=0.0005595841970274723 ,centered=False ,beta_1=0.46747739786762665 ,beta_2=0.4575910065462617 ,learning_rate_power=-0.040460266384405585 ,initial_accumulator_value=0.9362883852508103 ,l1_regularization_strength=0.6122952460117115 ,l2_regularization_strength=0.263310477071511 ,l2_shrinkage_regularization_strength=0.23418845023314827 ,beta=0.6906878179933996].
2023-08-01 15:42:32,755 INFO: Starting experiment with the arguments logged below.
2023-08-01 15:42:32,756 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 15:42:32,756 INFO: Setting the random number generator seed for this experiment.
2023-08-01 15:42:32,756 INFO: Loading dataset for the experiment.
2023-08-01 15:42:33,301 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 15:42:33,301 INFO: Setting up DEAP toolbox.
2023-08-01 15:42:33,301 INFO: Registering individual initialization method.
2023-08-01 15:42:33,301 INFO: Registering population initialization method.
2023-08-01 15:42:33,301 INFO: Registering the selection method.
2023-08-01 15:42:33,301 INFO: Registering the evaluation method.
2023-08-01 15:42:33,301 INFO: Registering the genetic operators.
2023-08-01 15:42:33,301 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 15:42:33,302 INFO: Initializing the initial population.
2023-08-01 15:42:33,307 INFO: Running the evolutionary algorithm.
2023-08-01 15:42:33,307 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 15:42:33,307 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 15:42:33,308 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9275547704911834, momentum=0.6405045881323773 ,nesterov=True ,amsgrad=False ,weight_decay=0.0011144240702868342 ,use_ema=False ,ema_momentum=0.280097443708883 ,rho=0.9474650706233074 ,epsilon=0.0005110801525445737 ,centered=True ,beta_1=0.19035734379947666 ,beta_2=0.028449412750387926 ,learning_rate_power=-0.33114940280760063 ,initial_accumulator_value=0.8142258076199119 ,l1_regularization_strength=0.31282881813309305 ,l2_regularization_strength=0.20322098501089836 ,l2_shrinkage_regularization_strength=0.9118203823504153 ,beta=0.8081633056307975].
2023-08-01 15:42:33,308 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9275547704911834, momentum=0.6405045881323773 ,nesterov=True ,amsgrad=False ,weight_decay=0.0011144240702868342 ,use_ema=False ,ema_momentum=0.280097443708883 ,rho=0.9474650706233074 ,epsilon=0.0005110801525445737 ,centered=True ,beta_1=0.19035734379947666 ,beta_2=0.028449412750387926 ,learning_rate_power=-0.33114940280760063 ,initial_accumulator_value=0.8142258076199119 ,l1_regularization_strength=0.31282881813309305 ,l2_regularization_strength=0.20322098501089836 ,l2_shrinkage_regularization_strength=0.9118203823504153 ,beta=0.8081633056307975].
2023-08-01 15:42:34,146 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,150 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,168 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,172 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,190 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,194 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,210 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,214 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,231 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,235 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,426 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,427 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,428 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,430 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,431 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,432 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,433 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,435 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,436 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:34,437 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:42:41,990 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-08-01 15:42:43,263 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.07398111677931596, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.35250009994841647 ,rho=0.5347019348275002 ,epsilon=0.00027238689380759565 ,centered=False ,beta_1=0.6237643775504731 ,beta_2=0.060558266870108235 ,learning_rate_power=-0.7891944269861181 ,initial_accumulator_value=0.16856456985384105 ,l1_regularization_strength=0.650437527669131 ,l2_regularization_strength=0.5483746431629289 ,l2_shrinkage_regularization_strength=0.45057864572919326 ,beta=0.2781798994635123].
2023-08-01 15:42:43,263 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.07398111677931596, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.35250009994841647 ,rho=0.5347019348275002 ,epsilon=0.00027238689380759565 ,centered=False ,beta_1=0.6237643775504731 ,beta_2=0.060558266870108235 ,learning_rate_power=-0.7891944269861181 ,initial_accumulator_value=0.16856456985384105 ,l1_regularization_strength=0.650437527669131 ,l2_regularization_strength=0.5483746431629289 ,l2_shrinkage_regularization_strength=0.45057864572919326 ,beta=0.2781798994635123].
2023-08-01 15:42:46,885 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-08-01 15:42:47,969 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.4962437143192878, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0030201105881156195 ,use_ema=False ,ema_momentum=0.4391746214700494 ,rho=0.8255050658527558 ,epsilon=9.581439599237931e-05 ,centered=False ,beta_1=0.564358699986243 ,beta_2=0.9040453869867507 ,learning_rate_power=-0.18678762549888062 ,initial_accumulator_value=0.9152022909540233 ,l1_regularization_strength=0.5115366797776258 ,l2_regularization_strength=0.680227976596376 ,l2_shrinkage_regularization_strength=0.16208111600669495 ,beta=0.8734296516211827].
2023-08-01 15:42:47,969 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.4962437143192878, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0030201105881156195 ,use_ema=False ,ema_momentum=0.4391746214700494 ,rho=0.8255050658527558 ,epsilon=9.581439599237931e-05 ,centered=False ,beta_1=0.564358699986243 ,beta_2=0.9040453869867507 ,learning_rate_power=-0.18678762549888062 ,initial_accumulator_value=0.9152022909540233 ,l1_regularization_strength=0.5115366797776258 ,l2_regularization_strength=0.680227976596376 ,l2_shrinkage_regularization_strength=0.16208111600669495 ,beta=0.8734296516211827].
2023-08-01 15:42:51,471 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-08-01 15:42:52,449 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.9837186516849983, momentum=0.10719232728670403 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.139101485418328 ,rho=0.8556908988954749 ,epsilon=0.0007201217724140995 ,centered=False ,beta_1=0.4575366089611034 ,beta_2=0.5813990353826702 ,learning_rate_power=-0.06002662433083161 ,initial_accumulator_value=0.32649500783848684 ,l1_regularization_strength=0.5985479180051366 ,l2_regularization_strength=0.8220395373883688 ,l2_shrinkage_regularization_strength=0.4217085368485326 ,beta=0.9148672592188969].
2023-08-01 15:42:52,449 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.9837186516849983, momentum=0.10719232728670403 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.139101485418328 ,rho=0.8556908988954749 ,epsilon=0.0007201217724140995 ,centered=False ,beta_1=0.4575366089611034 ,beta_2=0.5813990353826702 ,learning_rate_power=-0.06002662433083161 ,initial_accumulator_value=0.32649500783848684 ,l1_regularization_strength=0.5985479180051366 ,l2_regularization_strength=0.8220395373883688 ,l2_shrinkage_regularization_strength=0.4217085368485326 ,beta=0.9148672592188969].
2023-08-01 15:42:59,423 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-08-01 15:43:00,520 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8736761880216405, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0021065095365048396 ,use_ema=False ,ema_momentum=0.28448173327582804 ,rho=0.6917373645430972 ,epsilon=0.0007761087908476618 ,centered=True ,beta_1=0.23840470771439615 ,beta_2=0.94126609422325 ,learning_rate_power=-0.4946669174526699 ,initial_accumulator_value=0.8436340556511774 ,l1_regularization_strength=0.5172843764432739 ,l2_regularization_strength=0.41742295525289486 ,l2_shrinkage_regularization_strength=0.9227962765115799 ,beta=0.9993318227696442].
2023-08-01 15:43:00,520 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8736761880216405, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0021065095365048396 ,use_ema=False ,ema_momentum=0.28448173327582804 ,rho=0.6917373645430972 ,epsilon=0.0007761087908476618 ,centered=True ,beta_1=0.23840470771439615 ,beta_2=0.94126609422325 ,learning_rate_power=-0.4946669174526699 ,initial_accumulator_value=0.8436340556511774 ,l1_regularization_strength=0.5172843764432739 ,l2_regularization_strength=0.41742295525289486 ,l2_shrinkage_regularization_strength=0.9227962765115799 ,beta=0.9993318227696442].
2023-08-01 15:43:04,195 WARNING: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.
2023-08-01 15:43:05,188 INFO: Applying selection operators for generation 1.
2023-08-01 15:43:05,188 INFO: Applying genetic operators for generation 1.
2023-08-01 15:43:05,189 INFO: Evaluating fitness for for generation 1.
2023-08-01 15:43:05,189 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 15:43:05,189 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9275547704911834, momentum=0.6405045881323773 ,nesterov=True ,amsgrad=False ,weight_decay=0.0011144240702868342 ,use_ema=False ,ema_momentum=0.4391746214700494 ,rho=0.8255050658527558 ,epsilon=9.581439599237931e-05 ,centered=False ,beta_1=0.564358699986243 ,beta_2=0.9040453869867507 ,learning_rate_power=-0.18678762549888062 ,initial_accumulator_value=0.9152022909540233 ,l1_regularization_strength=0.5115366797776258 ,l2_regularization_strength=0.680227976596376 ,l2_shrinkage_regularization_strength=0.9118203823504153 ,beta=0.8081633056307975].
2023-08-01 15:43:05,189 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9275547704911834, momentum=0.6405045881323773 ,nesterov=True ,amsgrad=False ,weight_decay=0.0011144240702868342 ,use_ema=False ,ema_momentum=0.4391746214700494 ,rho=0.8255050658527558 ,epsilon=9.581439599237931e-05 ,centered=False ,beta_1=0.564358699986243 ,beta_2=0.9040453869867507 ,learning_rate_power=-0.18678762549888062 ,initial_accumulator_value=0.9152022909540233 ,l1_regularization_strength=0.5115366797776258 ,l2_regularization_strength=0.680227976596376 ,l2_shrinkage_regularization_strength=0.9118203823504153 ,beta=0.8081633056307975].
2023-08-01 15:43:06,219 INFO: Error reported to Coordinator: 
Traceback (most recent call last):
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/training/coordinator.py", line 293, in stop_on_exception
    yield
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/distribute/mirrored_run.py", line 277, in _call_for_each_replica
    merge_result = threads[0].merge_fn(distribution, *merge_args,
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py", line 689, in wrapper
    return converted_call(f, args, kwargs, options=options)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py", line 377, in converted_call
    return _call_unconverted(f, args, kwargs, options)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py", line 458, in _call_unconverted
    return f(*args, **kwargs)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1189, in distributed_apply_weight_decay
    distribution.extended.update(
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py", line 2637, in update
    return self._update(var, fn, args, kwargs, group)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/distribute/parameter_server_strategy.py", line 551, in _update
    result = fn(var, *self._select_single_value(args),
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py", line 689, in wrapper
    return converted_call(f, args, kwargs, options=options)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py", line 331, in converted_call
    return _call_unconverted(f, args, kwargs, options, False)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py", line 458, in _call_unconverted
    return f(*args, **kwargs)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1185, in weight_decay_fn
    wd = tf.cast(self.weight_decay, variable.dtype)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py", line 150, in error_handler
    return fn(*args, **kwargs)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py", line 1176, in op_dispatch_handler
    return dispatch_target(*args, **kwargs)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py", line 1009, in cast
    x = ops.convert_to_tensor(x, name="x")
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py", line 183, in wrapped
    return func(*args, **kwargs)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 1642, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py", line 48, in _default_conversion_function
    return constant_op.constant(value, dtype, name=name)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py", line 268, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py", line 290, in _constant_impl
    const_tensor = g._create_op_internal(  # pylint: disable=protected-access
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py", line 707, in _create_op_internal
    return super()._create_op_internal(  # pylint: disable=protected-access
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 3814, in _create_op_internal
    ret = Operation(
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 2112, in __init__
    c_op = _create_c_op(g, node_def, inputs, control_input_ops, op_def=op_def)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py", line 150, in error_handler
    return fn(*args, **kwargs)
  File "/home/mbornman1/.conda/envs/evoopt-cnn-env/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 1943, in _create_c_op
    op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
KeyboardInterrupt
2023-08-01 15:43:34,995 INFO: Starting experiment with the arguments logged below.
2023-08-01 15:43:34,995 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=512, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 15:43:34,995 INFO: Setting the random number generator seed for this experiment.
2023-08-01 15:43:34,996 INFO: Loading dataset for the experiment.
2023-08-01 15:43:35,527 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 15:43:35,527 INFO: Setting up DEAP toolbox.
2023-08-01 15:43:35,527 INFO: Registering individual initialization method.
2023-08-01 15:43:35,527 INFO: Registering population initialization method.
2023-08-01 15:43:35,528 INFO: Registering the selection method.
2023-08-01 15:43:35,528 INFO: Registering the evaluation method.
2023-08-01 15:43:35,528 INFO: Registering the genetic operators.
2023-08-01 15:43:35,528 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 15:43:35,528 INFO: Initializing the initial population.
2023-08-01 15:43:35,534 INFO: Running the evolutionary algorithm.
2023-08-01 15:43:35,534 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 15:43:35,534 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 15:43:35,535 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9018441353748218, momentum=0.07322426840939 ,nesterov=True ,amsgrad=False ,weight_decay=0.009681959740410737 ,use_ema=False ,ema_momentum=0.8938259877697723 ,rho=0.05668042032748544 ,epsilon=0.0009084399247851081 ,centered=True ,beta_1=0.5870008746072889 ,beta_2=0.7896164219963125 ,learning_rate_power=-0.9917830516063133 ,initial_accumulator_value=0.9422957528949757 ,l1_regularization_strength=0.3794484353974932 ,l2_regularization_strength=0.24800618032694943 ,l2_shrinkage_regularization_strength=0.02528165795214543 ,beta=0.33816826417981516].
2023-08-01 15:43:35,535 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9018441353748218, momentum=0.07322426840939 ,nesterov=True ,amsgrad=False ,weight_decay=0.009681959740410737 ,use_ema=False ,ema_momentum=0.8938259877697723 ,rho=0.05668042032748544 ,epsilon=0.0009084399247851081 ,centered=True ,beta_1=0.5870008746072889 ,beta_2=0.7896164219963125 ,learning_rate_power=-0.9917830516063133 ,initial_accumulator_value=0.9422957528949757 ,l1_regularization_strength=0.3794484353974932 ,l2_regularization_strength=0.24800618032694943 ,l2_shrinkage_regularization_strength=0.02528165795214543 ,beta=0.33816826417981516].
2023-08-01 15:43:36,386 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,390 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,407 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,410 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,427 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,430 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,446 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,449 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,464 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,468 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,656 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,658 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,659 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,660 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,661 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,663 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,664 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,665 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,666 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:43:36,668 INFO: Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-08-01 15:44:06,178 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.061847681366933194, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8028013157206932 ,rho=0.9034655650422599 ,epsilon=0.00022500474253990858 ,centered=False ,beta_1=0.03363749359340662 ,beta_2=0.606713221071937 ,learning_rate_power=-0.7760586207076022 ,initial_accumulator_value=0.7785291248703021 ,l1_regularization_strength=0.3151159465165515 ,l2_regularization_strength=0.27393239223455546 ,l2_shrinkage_regularization_strength=0.25969568393607734 ,beta=0.6310777509922212].
2023-08-01 15:44:06,178 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.061847681366933194, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8028013157206932 ,rho=0.9034655650422599 ,epsilon=0.00022500474253990858 ,centered=False ,beta_1=0.03363749359340662 ,beta_2=0.606713221071937 ,learning_rate_power=-0.7760586207076022 ,initial_accumulator_value=0.7785291248703021 ,l1_regularization_strength=0.3151159465165515 ,l2_regularization_strength=0.27393239223455546 ,l2_shrinkage_regularization_strength=0.25969568393607734 ,beta=0.6310777509922212].
2023-08-01 15:44:39,885 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.21740196871291573, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.009925818461767844 ,use_ema=False ,ema_momentum=0.9249711645470639 ,rho=0.6994459627197451 ,epsilon=0.0009581736938536832 ,centered=False ,beta_1=0.24795010112162152 ,beta_2=0.4664422043361647 ,learning_rate_power=-0.7644514470704212 ,initial_accumulator_value=0.006408542258003469 ,l1_regularization_strength=0.29586773311944936 ,l2_regularization_strength=0.1364989430548751 ,l2_shrinkage_regularization_strength=0.759817679005663 ,beta=0.9618437067756215].
2023-08-01 15:44:39,885 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.21740196871291573, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.009925818461767844 ,use_ema=False ,ema_momentum=0.9249711645470639 ,rho=0.6994459627197451 ,epsilon=0.0009581736938536832 ,centered=False ,beta_1=0.24795010112162152 ,beta_2=0.4664422043361647 ,learning_rate_power=-0.7644514470704212 ,initial_accumulator_value=0.006408542258003469 ,l1_regularization_strength=0.29586773311944936 ,l2_regularization_strength=0.1364989430548751 ,l2_shrinkage_regularization_strength=0.759817679005663 ,beta=0.9618437067756215].
2023-08-01 16:03:32,100 INFO: Starting experiment with the arguments logged below.
2023-08-01 16:03:32,101 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=512, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 16:03:32,101 INFO: Setting the random number generator seed for this experiment.
2023-08-01 16:03:32,101 INFO: Loading dataset for the experiment.
2023-08-01 16:03:32,650 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 16:03:32,651 INFO: Setting up DEAP toolbox.
2023-08-01 16:03:32,651 INFO: Registering individual initialization method.
2023-08-01 16:03:32,651 INFO: Registering population initialization method.
2023-08-01 16:03:32,651 INFO: Registering the selection method.
2023-08-01 16:03:32,651 INFO: Registering the evaluation method.
2023-08-01 16:03:32,651 INFO: Registering the genetic operators.
2023-08-01 16:03:32,651 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 16:03:32,651 INFO: Initializing the initial population.
2023-08-01 16:03:32,657 INFO: Running the evolutionary algorithm.
2023-08-01 16:03:32,657 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 16:03:32,657 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 16:03:32,658 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.46230788953594737, momentum=0.10846784684641664 ,nesterov=True ,amsgrad=False ,weight_decay=0.003385569840982405 ,use_ema=False ,ema_momentum=0.4196824510769682 ,rho=0.7850099352784756 ,epsilon=0.0008586148351559904 ,centered=True ,beta_1=0.23783916569101227 ,beta_2=0.9321940808903548 ,learning_rate_power=-0.9307601554792458 ,initial_accumulator_value=0.9982806432010095 ,l1_regularization_strength=0.784682348055602 ,l2_regularization_strength=0.9259488926279386 ,l2_shrinkage_regularization_strength=0.7449066153438033 ,beta=0.6975835878088796].
2023-08-01 16:03:32,658 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.46230788953594737, momentum=0.10846784684641664 ,nesterov=True ,amsgrad=False ,weight_decay=0.003385569840982405 ,use_ema=False ,ema_momentum=0.4196824510769682 ,rho=0.7850099352784756 ,epsilon=0.0008586148351559904 ,centered=True ,beta_1=0.23783916569101227 ,beta_2=0.9321940808903548 ,learning_rate_power=-0.9307601554792458 ,initial_accumulator_value=0.9982806432010095 ,l1_regularization_strength=0.784682348055602 ,l2_regularization_strength=0.9259488926279386 ,l2_shrinkage_regularization_strength=0.7449066153438033 ,beta=0.6975835878088796].
2023-08-01 16:04:04,053 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.6483479409808841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.29167878009044446 ,rho=0.8327159089967358 ,epsilon=0.000747793774541488 ,centered=False ,beta_1=0.46724026490451287 ,beta_2=0.4308621236732153 ,learning_rate_power=-0.7838732774678918 ,initial_accumulator_value=0.8693450870613361 ,l1_regularization_strength=0.6370585280798446 ,l2_regularization_strength=0.8502825827487317 ,l2_shrinkage_regularization_strength=0.46418667336038666 ,beta=0.7175640525684889].
2023-08-01 16:04:04,054 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.6483479409808841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.29167878009044446 ,rho=0.8327159089967358 ,epsilon=0.000747793774541488 ,centered=False ,beta_1=0.46724026490451287 ,beta_2=0.4308621236732153 ,learning_rate_power=-0.7838732774678918 ,initial_accumulator_value=0.8693450870613361 ,l1_regularization_strength=0.6370585280798446 ,l2_regularization_strength=0.8502825827487317 ,l2_shrinkage_regularization_strength=0.46418667336038666 ,beta=0.7175640525684889].
2023-08-01 16:04:29,789 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.8483428938643853, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.004663292765058146 ,use_ema=False ,ema_momentum=0.6996633030876863 ,rho=0.31989051533526836 ,epsilon=0.0002630128858546867 ,centered=False ,beta_1=0.8349977821675553 ,beta_2=0.20760324046682244 ,learning_rate_power=-0.7044474786977665 ,initial_accumulator_value=0.703594277975383 ,l1_regularization_strength=0.019848102973820225 ,l2_regularization_strength=0.46710796278233024 ,l2_shrinkage_regularization_strength=0.9695614115367746 ,beta=0.5532328942669265].
2023-08-01 16:04:29,789 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.8483428938643853, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.004663292765058146 ,use_ema=False ,ema_momentum=0.6996633030876863 ,rho=0.31989051533526836 ,epsilon=0.0002630128858546867 ,centered=False ,beta_1=0.8349977821675553 ,beta_2=0.20760324046682244 ,learning_rate_power=-0.7044474786977665 ,initial_accumulator_value=0.703594277975383 ,l1_regularization_strength=0.019848102973820225 ,l2_regularization_strength=0.46710796278233024 ,l2_shrinkage_regularization_strength=0.9695614115367746 ,beta=0.5532328942669265].
2023-08-01 16:04:55,858 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.06663830282774585, momentum=0.5228076378980411 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.5591736689593463 ,rho=0.14222102105041123 ,epsilon=0.0002299461936086103 ,centered=False ,beta_1=0.5207776340982893 ,beta_2=0.05999325541701117 ,learning_rate_power=-0.7914465218427685 ,initial_accumulator_value=0.7586755150676402 ,l1_regularization_strength=0.8880180388443841 ,l2_regularization_strength=0.5874102061797936 ,l2_shrinkage_regularization_strength=0.776878133427641 ,beta=0.4503221547333284].
2023-08-01 16:04:55,858 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.06663830282774585, momentum=0.5228076378980411 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.5591736689593463 ,rho=0.14222102105041123 ,epsilon=0.0002299461936086103 ,centered=False ,beta_1=0.5207776340982893 ,beta_2=0.05999325541701117 ,learning_rate_power=-0.7914465218427685 ,initial_accumulator_value=0.7586755150676402 ,l1_regularization_strength=0.8880180388443841 ,l2_regularization_strength=0.5874102061797936 ,l2_shrinkage_regularization_strength=0.776878133427641 ,beta=0.4503221547333284].
2023-08-01 16:05:29,536 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.22009574500359286, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008060136241753576 ,use_ema=False ,ema_momentum=0.9422504812192517 ,rho=0.4820202242762899 ,epsilon=0.0007200740485119589 ,centered=True ,beta_1=0.30963750323454986 ,beta_2=0.831350269460823 ,learning_rate_power=-0.6665683431704928 ,initial_accumulator_value=0.12143198647851172 ,l1_regularization_strength=0.7809237105764948 ,l2_regularization_strength=0.40468931702041333 ,l2_shrinkage_regularization_strength=0.7127626627225098 ,beta=0.27084944173591163].
2023-08-01 16:05:29,536 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.22009574500359286, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008060136241753576 ,use_ema=False ,ema_momentum=0.9422504812192517 ,rho=0.4820202242762899 ,epsilon=0.0007200740485119589 ,centered=True ,beta_1=0.30963750323454986 ,beta_2=0.831350269460823 ,learning_rate_power=-0.6665683431704928 ,initial_accumulator_value=0.12143198647851172 ,l1_regularization_strength=0.7809237105764948 ,l2_regularization_strength=0.40468931702041333 ,l2_shrinkage_regularization_strength=0.7127626627225098 ,beta=0.27084944173591163].
2023-08-01 16:05:56,680 INFO: Applying selection operators for generation 1.
2023-08-01 16:05:56,681 INFO: Applying genetic operators for generation 1.
2023-08-01 16:05:56,682 INFO: Evaluating fitness for for generation 1.
2023-08-01 16:05:56,682 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 16:05:56,682 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.46230788953594737, momentum=0.10846784684641664 ,nesterov=True ,amsgrad=False ,weight_decay=0.003385569840982405 ,use_ema=False ,ema_momentum=0.4196824510769682 ,rho=0.7850099352784756 ,epsilon=0.0008586148351559904 ,centered=True ,beta_1=0.23783916569101227 ,beta_2=0.9321940808903548 ,learning_rate_power=-0.9307601554792458 ,initial_accumulator_value=0.9982806432010095 ,l1_regularization_strength=0.784682348055602 ,l2_regularization_strength=0.9259488926279386 ,l2_shrinkage_regularization_strength=0.7449066153438033 ,beta=0.6975835878088796].
2023-08-01 16:05:56,682 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.46230788953594737, momentum=0.10846784684641664 ,nesterov=True ,amsgrad=False ,weight_decay=0.003385569840982405 ,use_ema=False ,ema_momentum=0.4196824510769682 ,rho=0.7850099352784756 ,epsilon=0.0008586148351559904 ,centered=True ,beta_1=0.23783916569101227 ,beta_2=0.9321940808903548 ,learning_rate_power=-0.9307601554792458 ,initial_accumulator_value=0.9982806432010095 ,l1_regularization_strength=0.784682348055602 ,l2_regularization_strength=0.9259488926279386 ,l2_shrinkage_regularization_strength=0.7449066153438033 ,beta=0.6975835878088796].
2023-08-01 16:06:22,989 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.46230788953594737, momentum=0.10846784684641664 ,nesterov=True ,amsgrad=False ,weight_decay=0.003385569840982405 ,use_ema=False ,ema_momentum=0.4196824510769682 ,rho=0.7850099352784756 ,epsilon=0.0008586148351559904 ,centered=True ,beta_1=0.23783916569101227 ,beta_2=0.9321940808903548 ,learning_rate_power=-0.9307601554792458 ,initial_accumulator_value=0.9982806432010095 ,l1_regularization_strength=0.784682348055602 ,l2_regularization_strength=0.9259488926279386 ,l2_shrinkage_regularization_strength=0.7449066153438033 ,beta=0.6975835878088796].
2023-08-01 16:06:22,989 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.46230788953594737, momentum=0.10846784684641664 ,nesterov=True ,amsgrad=False ,weight_decay=0.003385569840982405 ,use_ema=False ,ema_momentum=0.4196824510769682 ,rho=0.7850099352784756 ,epsilon=0.0008586148351559904 ,centered=True ,beta_1=0.23783916569101227 ,beta_2=0.9321940808903548 ,learning_rate_power=-0.9307601554792458 ,initial_accumulator_value=0.9982806432010095 ,l1_regularization_strength=0.784682348055602 ,l2_regularization_strength=0.9259488926279386 ,l2_shrinkage_regularization_strength=0.7449066153438033 ,beta=0.6975835878088796].
2023-08-01 16:06:48,957 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.16678093764878543, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.3690911715686551 ,rho=0.7850099352784756 ,epsilon=0.0008586148351559904 ,centered=False ,beta_1=0.2692338112411222 ,beta_2=0.047837180287771175 ,learning_rate_power=-0.6665683431704928 ,initial_accumulator_value=0.12143198647851172 ,l1_regularization_strength=0.6734172374800189 ,l2_regularization_strength=0.9259488926279386 ,l2_shrinkage_regularization_strength=0.6777974368660252 ,beta=0.6609059032392368].
2023-08-01 16:06:48,957 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.16678093764878543, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.3690911715686551 ,rho=0.7850099352784756 ,epsilon=0.0008586148351559904 ,centered=False ,beta_1=0.2692338112411222 ,beta_2=0.047837180287771175 ,learning_rate_power=-0.6665683431704928 ,initial_accumulator_value=0.12143198647851172 ,l1_regularization_strength=0.6734172374800189 ,l2_regularization_strength=0.9259488926279386 ,l2_shrinkage_regularization_strength=0.6777974368660252 ,beta=0.6609059032392368].
2023-08-01 16:07:16,921 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.22009574500359286, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008060136241753576 ,use_ema=False ,ema_momentum=0.9422504812192517 ,rho=0.4820202242762899 ,epsilon=0.0007200740485119589 ,centered=True ,beta_1=0.23783916569101227 ,beta_2=0.9321940808903548 ,learning_rate_power=-0.9307601554792458 ,initial_accumulator_value=0.9982806432010095 ,l1_regularization_strength=0.784682348055602 ,l2_regularization_strength=0.40468931702041333 ,l2_shrinkage_regularization_strength=0.7127626627225098 ,beta=0.27084944173591163].
2023-08-01 16:07:16,921 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.22009574500359286, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008060136241753576 ,use_ema=False ,ema_momentum=0.9422504812192517 ,rho=0.4820202242762899 ,epsilon=0.0007200740485119589 ,centered=True ,beta_1=0.23783916569101227 ,beta_2=0.9321940808903548 ,learning_rate_power=-0.9307601554792458 ,initial_accumulator_value=0.9982806432010095 ,l1_regularization_strength=0.784682348055602 ,l2_regularization_strength=0.40468931702041333 ,l2_shrinkage_regularization_strength=0.7127626627225098 ,beta=0.27084944173591163].
2023-08-01 16:07:42,663 INFO: Applying selection operators for generation 2.
2023-08-01 16:07:42,663 INFO: Applying genetic operators for generation 2.
2023-08-01 16:07:42,663 INFO: Evaluating fitness for for generation 2.
2023-08-01 16:07:42,663 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 16:07:42,664 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.46230788953594737, momentum=0.10846784684641664 ,nesterov=True ,amsgrad=False ,weight_decay=0.003385569840982405 ,use_ema=False ,ema_momentum=0.4196824510769682 ,rho=0.7850099352784756 ,epsilon=0.0008586148351559904 ,centered=True ,beta_1=0.23783916569101227 ,beta_2=0.9321940808903548 ,learning_rate_power=-0.9307601554792458 ,initial_accumulator_value=0.9982806432010095 ,l1_regularization_strength=0.784682348055602 ,l2_regularization_strength=0.9259488926279386 ,l2_shrinkage_regularization_strength=0.7449066153438033 ,beta=0.6975835878088796].
2023-08-01 16:07:42,664 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.46230788953594737, momentum=0.10846784684641664 ,nesterov=True ,amsgrad=False ,weight_decay=0.003385569840982405 ,use_ema=False ,ema_momentum=0.4196824510769682 ,rho=0.7850099352784756 ,epsilon=0.0008586148351559904 ,centered=True ,beta_1=0.23783916569101227 ,beta_2=0.9321940808903548 ,learning_rate_power=-0.9307601554792458 ,initial_accumulator_value=0.9982806432010095 ,l1_regularization_strength=0.784682348055602 ,l2_regularization_strength=0.9259488926279386 ,l2_shrinkage_regularization_strength=0.7449066153438033 ,beta=0.6975835878088796].
2023-08-01 16:08:13,627 INFO: Starting experiment with the arguments logged below.
2023-08-01 16:08:13,628 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 16:08:13,628 INFO: Setting the random number generator seed for this experiment.
2023-08-01 16:08:13,628 INFO: Loading dataset for the experiment.
2023-08-01 16:08:14,167 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 16:08:14,167 INFO: Setting up DEAP toolbox.
2023-08-01 16:08:14,167 INFO: Registering individual initialization method.
2023-08-01 16:08:14,167 INFO: Registering population initialization method.
2023-08-01 16:08:14,168 INFO: Registering the selection method.
2023-08-01 16:08:14,168 INFO: Registering the evaluation method.
2023-08-01 16:08:14,168 INFO: Registering the genetic operators.
2023-08-01 16:08:14,168 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 16:08:14,168 INFO: Initializing the initial population.
2023-08-01 16:08:14,173 INFO: Running the evolutionary algorithm.
2023-08-01 16:08:14,173 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 16:08:14,174 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 16:08:14,174 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8346093553719924, momentum=0.9766824745706176 ,nesterov=True ,amsgrad=False ,weight_decay=0.000272190270945128 ,use_ema=False ,ema_momentum=0.42678270111246697 ,rho=0.15719155894127557 ,epsilon=0.00010360150594524902 ,centered=True ,beta_1=0.6440275630528282 ,beta_2=0.05574999756774379 ,learning_rate_power=-0.280477859489239 ,initial_accumulator_value=0.20072413256650734 ,l1_regularization_strength=0.5358517972609876 ,l2_regularization_strength=0.6945680995391363 ,l2_shrinkage_regularization_strength=0.8199199453543096 ,beta=0.3908402063828741].
2023-08-01 16:08:14,174 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8346093553719924, momentum=0.9766824745706176 ,nesterov=True ,amsgrad=False ,weight_decay=0.000272190270945128 ,use_ema=False ,ema_momentum=0.42678270111246697 ,rho=0.15719155894127557 ,epsilon=0.00010360150594524902 ,centered=True ,beta_1=0.6440275630528282 ,beta_2=0.05574999756774379 ,learning_rate_power=-0.280477859489239 ,initial_accumulator_value=0.20072413256650734 ,l1_regularization_strength=0.5358517972609876 ,l2_regularization_strength=0.6945680995391363 ,l2_shrinkage_regularization_strength=0.8199199453543096 ,beta=0.3908402063828741].
2023-08-01 16:08:42,008 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.022699626811289897, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.6330582503910567 ,rho=0.35115967941690085 ,epsilon=0.0004483080256883157 ,centered=False ,beta_1=0.5752309817274177 ,beta_2=0.6162308300114595 ,learning_rate_power=-0.9388825039743984 ,initial_accumulator_value=0.7591653511730362 ,l1_regularization_strength=0.31288060409337914 ,l2_regularization_strength=0.5747526815842109 ,l2_shrinkage_regularization_strength=0.2810095941450286 ,beta=0.750397994679453].
2023-08-01 16:08:42,008 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.022699626811289897, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.6330582503910567 ,rho=0.35115967941690085 ,epsilon=0.0004483080256883157 ,centered=False ,beta_1=0.5752309817274177 ,beta_2=0.6162308300114595 ,learning_rate_power=-0.9388825039743984 ,initial_accumulator_value=0.7591653511730362 ,l1_regularization_strength=0.31288060409337914 ,l2_regularization_strength=0.5747526815842109 ,l2_shrinkage_regularization_strength=0.2810095941450286 ,beta=0.750397994679453].
2023-08-01 16:09:04,984 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.5030411117179348, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.004062412442668151 ,use_ema=False ,ema_momentum=0.5466267142756707 ,rho=0.6131146533686878 ,epsilon=0.0009836630713532905 ,centered=False ,beta_1=0.1891408321854694 ,beta_2=0.8660190529049903 ,learning_rate_power=-0.6064386038757136 ,initial_accumulator_value=0.531387763212764 ,l1_regularization_strength=0.6214413341056988 ,l2_regularization_strength=0.844531624958103 ,l2_shrinkage_regularization_strength=0.3352993735138998 ,beta=0.13362370811362234].
2023-08-01 16:09:04,984 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.5030411117179348, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.004062412442668151 ,use_ema=False ,ema_momentum=0.5466267142756707 ,rho=0.6131146533686878 ,epsilon=0.0009836630713532905 ,centered=False ,beta_1=0.1891408321854694 ,beta_2=0.8660190529049903 ,learning_rate_power=-0.6064386038757136 ,initial_accumulator_value=0.531387763212764 ,l1_regularization_strength=0.6214413341056988 ,l2_regularization_strength=0.844531624958103 ,l2_shrinkage_regularization_strength=0.3352993735138998 ,beta=0.13362370811362234].
2023-08-01 16:09:27,487 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.7198028505168863, momentum=0.39192460795172024 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.5986770389154247 ,rho=0.3108042225829366 ,epsilon=0.0003323737539666626 ,centered=False ,beta_1=0.8238316884266733 ,beta_2=0.7040912217715816 ,learning_rate_power=-0.4009383589781743 ,initial_accumulator_value=0.24151826441705948 ,l1_regularization_strength=0.03415154437600576 ,l2_regularization_strength=0.40796808108784577 ,l2_shrinkage_regularization_strength=0.003825401911070947 ,beta=0.19145177807367286].
2023-08-01 16:09:27,488 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.7198028505168863, momentum=0.39192460795172024 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.5986770389154247 ,rho=0.3108042225829366 ,epsilon=0.0003323737539666626 ,centered=False ,beta_1=0.8238316884266733 ,beta_2=0.7040912217715816 ,learning_rate_power=-0.4009383589781743 ,initial_accumulator_value=0.24151826441705948 ,l1_regularization_strength=0.03415154437600576 ,l2_regularization_strength=0.40796808108784577 ,l2_shrinkage_regularization_strength=0.003825401911070947 ,beta=0.19145177807367286].
2023-08-01 16:09:56,846 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.32762186288973383, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.003169568808085107 ,use_ema=False ,ema_momentum=0.7565198295864517 ,rho=0.8520193175987826 ,epsilon=0.00039198665630535114 ,centered=True ,beta_1=0.5318332598096822 ,beta_2=0.6377365065599567 ,learning_rate_power=-0.2122674400352701 ,initial_accumulator_value=0.6514268363697112 ,l1_regularization_strength=0.24149578880265954 ,l2_regularization_strength=0.8736232118961346 ,l2_shrinkage_regularization_strength=0.1176338671870083 ,beta=0.1394551510769565].
2023-08-01 16:09:56,847 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.32762186288973383, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.003169568808085107 ,use_ema=False ,ema_momentum=0.7565198295864517 ,rho=0.8520193175987826 ,epsilon=0.00039198665630535114 ,centered=True ,beta_1=0.5318332598096822 ,beta_2=0.6377365065599567 ,learning_rate_power=-0.2122674400352701 ,initial_accumulator_value=0.6514268363697112 ,l1_regularization_strength=0.24149578880265954 ,l2_regularization_strength=0.8736232118961346 ,l2_shrinkage_regularization_strength=0.1176338671870083 ,beta=0.1394551510769565].
2023-08-01 16:10:20,668 INFO: Applying selection operators for generation 1.
2023-08-01 16:10:20,669 INFO: Applying genetic operators for generation 1.
2023-08-01 16:10:20,670 INFO: Evaluating fitness for for generation 1.
2023-08-01 16:10:20,670 INFO: Will evaluate fitness for 4 individuals.
2023-08-01 16:10:20,670 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8346093553719924, momentum=0.9766824745706176 ,nesterov=True ,amsgrad=False ,weight_decay=0.000272190270945128 ,use_ema=False ,ema_momentum=0.5466267142756707 ,rho=0.6131146533686878 ,epsilon=0.0009836630713532905 ,centered=False ,beta_1=0.1891408321854694 ,beta_2=0.8660190529049903 ,learning_rate_power=-0.6064386038757136 ,initial_accumulator_value=0.531387763212764 ,l1_regularization_strength=0.6214413341056988 ,l2_regularization_strength=0.844531624958103 ,l2_shrinkage_regularization_strength=0.8199199453543096 ,beta=0.3908402063828741].
2023-08-01 16:10:20,670 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8346093553719924, momentum=0.9766824745706176 ,nesterov=True ,amsgrad=False ,weight_decay=0.000272190270945128 ,use_ema=False ,ema_momentum=0.5466267142756707 ,rho=0.6131146533686878 ,epsilon=0.0009836630713532905 ,centered=False ,beta_1=0.1891408321854694 ,beta_2=0.8660190529049903 ,learning_rate_power=-0.6064386038757136 ,initial_accumulator_value=0.531387763212764 ,l1_regularization_strength=0.6214413341056988 ,l2_regularization_strength=0.844531624958103 ,l2_shrinkage_regularization_strength=0.8199199453543096 ,beta=0.3908402063828741].
2023-08-01 16:10:40,253 INFO: Starting experiment with the arguments logged below.
2023-08-01 16:10:40,253 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=512, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-01 16:10:40,253 INFO: Setting the random number generator seed for this experiment.
2023-08-01 16:10:40,254 INFO: Loading dataset for the experiment.
2023-08-01 16:10:40,783 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-01 16:10:40,783 INFO: Setting up DEAP toolbox.
2023-08-01 16:10:40,783 INFO: Registering individual initialization method.
2023-08-01 16:10:40,783 INFO: Registering population initialization method.
2023-08-01 16:10:40,783 INFO: Registering the selection method.
2023-08-01 16:10:40,783 INFO: Registering the evaluation method.
2023-08-01 16:10:40,784 INFO: Registering the genetic operators.
2023-08-01 16:10:40,784 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-01 16:10:40,784 INFO: Initializing the initial population.
2023-08-01 16:10:40,790 INFO: Running the evolutionary algorithm.
2023-08-01 16:10:40,790 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-01 16:10:40,790 INFO: Will evaluate fitness for 5 individuals.
2023-08-01 16:10:40,791 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8371366654525736, momentum=0.3724581859721393 ,nesterov=True ,amsgrad=False ,weight_decay=0.0014831676332535263 ,use_ema=False ,ema_momentum=0.20617773568969577 ,rho=0.9247935464280791 ,epsilon=0.00028250829786371557 ,centered=True ,beta_1=0.5307855718773847 ,beta_2=0.9325440239318754 ,learning_rate_power=-0.4346184598798689 ,initial_accumulator_value=0.4635714112959992 ,l1_regularization_strength=0.22232446735674627 ,l2_regularization_strength=0.11940905890384657 ,l2_shrinkage_regularization_strength=0.3831238121917169 ,beta=0.9579186730360906].
2023-08-01 16:10:40,791 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8371366654525736, momentum=0.3724581859721393 ,nesterov=True ,amsgrad=False ,weight_decay=0.0014831676332535263 ,use_ema=False ,ema_momentum=0.20617773568969577 ,rho=0.9247935464280791 ,epsilon=0.00028250829786371557 ,centered=True ,beta_1=0.5307855718773847 ,beta_2=0.9325440239318754 ,learning_rate_power=-0.4346184598798689 ,initial_accumulator_value=0.4635714112959992 ,l1_regularization_strength=0.22232446735674627 ,l2_regularization_strength=0.11940905890384657 ,l2_shrinkage_regularization_strength=0.3831238121917169 ,beta=0.9579186730360906].
2023-08-01 16:11:10,773 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.9176090644389436, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.13152279792810895 ,rho=0.048924057549340816 ,epsilon=0.00014810686010922883 ,centered=False ,beta_1=0.76071913666758 ,beta_2=0.8105109736040552 ,learning_rate_power=-0.6483026385238488 ,initial_accumulator_value=0.913224193982347 ,l1_regularization_strength=0.15545341893279385 ,l2_regularization_strength=0.6583357338681752 ,l2_shrinkage_regularization_strength=0.2910202693135917 ,beta=0.11697587866701153].
2023-08-01 16:11:10,773 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.9176090644389436, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.13152279792810895 ,rho=0.048924057549340816 ,epsilon=0.00014810686010922883 ,centered=False ,beta_1=0.76071913666758 ,beta_2=0.8105109736040552 ,learning_rate_power=-0.6483026385238488 ,initial_accumulator_value=0.913224193982347 ,l1_regularization_strength=0.15545341893279385 ,l2_regularization_strength=0.6583357338681752 ,l2_shrinkage_regularization_strength=0.2910202693135917 ,beta=0.11697587866701153].
2023-08-01 16:11:37,012 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.30771518623403205, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.006167488786491486 ,use_ema=False ,ema_momentum=0.9265393796135968 ,rho=0.2434912287767531 ,epsilon=0.0007122859718118365 ,centered=False ,beta_1=0.1050411995118894 ,beta_2=0.9228798621708162 ,learning_rate_power=-0.6101961143235721 ,initial_accumulator_value=0.20191401936138542 ,l1_regularization_strength=0.2911138963709916 ,l2_regularization_strength=0.719259440527022 ,l2_shrinkage_regularization_strength=0.8191692072239555 ,beta=0.3127436582173635].
2023-08-01 16:11:37,012 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.30771518623403205, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.006167488786491486 ,use_ema=False ,ema_momentum=0.9265393796135968 ,rho=0.2434912287767531 ,epsilon=0.0007122859718118365 ,centered=False ,beta_1=0.1050411995118894 ,beta_2=0.9228798621708162 ,learning_rate_power=-0.6101961143235721 ,initial_accumulator_value=0.20191401936138542 ,l1_regularization_strength=0.2911138963709916 ,l2_regularization_strength=0.719259440527022 ,l2_shrinkage_regularization_strength=0.8191692072239555 ,beta=0.3127436582173635].
2023-08-01 16:12:03,187 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.5573965993254028, momentum=0.01831580118735998 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.6502866214719049 ,rho=0.6685864306078895 ,epsilon=0.0007961633346276561 ,centered=False ,beta_1=0.5224026943376976 ,beta_2=0.42049338618348353 ,learning_rate_power=-0.6657458399255298 ,initial_accumulator_value=0.7557502628588958 ,l1_regularization_strength=0.34466069185799 ,l2_regularization_strength=0.3003111572088193 ,l2_shrinkage_regularization_strength=0.9790200697315067 ,beta=0.31539157416497643].
2023-08-01 16:12:03,187 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.5573965993254028, momentum=0.01831580118735998 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.6502866214719049 ,rho=0.6685864306078895 ,epsilon=0.0007961633346276561 ,centered=False ,beta_1=0.5224026943376976 ,beta_2=0.42049338618348353 ,learning_rate_power=-0.6657458399255298 ,initial_accumulator_value=0.7557502628588958 ,l1_regularization_strength=0.34466069185799 ,l2_regularization_strength=0.3003111572088193 ,l2_shrinkage_regularization_strength=0.9790200697315067 ,beta=0.31539157416497643].
2023-08-03 12:58:11,961 INFO: Starting experiment with the arguments logged below.
2023-08-03 12:58:11,977 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/test_experiment', cpu_count=2, seed=1, dataset='fashion_mnist', pop_size=5, ngen=5, model='alexnet', epochs=10, batch_size=1024, tournsize=3, cxpb=0.8, mutpb=0.2, gene_mut_prob=0.5)
2023-08-03 12:58:11,977 INFO: Setting the random number generator seed for this experiment.
2023-08-03 12:58:11,977 INFO: Initializing the multiprocessing pool.
2023-08-03 12:58:12,083 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-03 12:58:12,083 INFO: Setting up DEAP toolbox.
2023-08-03 12:58:12,083 INFO: Registering individual initialization method.
2023-08-03 12:58:12,083 INFO: Registering population initialization method.
2023-08-03 12:58:12,083 INFO: Registering the selection method.
2023-08-03 12:58:12,083 INFO: Registering the evaluation method.
2023-08-03 12:58:12,084 INFO: Registering the genetic operators.
2023-08-03 12:58:12,084 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-03 12:58:12,084 INFO: Initializing the initial population.
2023-08-03 12:58:12,089 INFO: Running the evolutionary algorithm.
2023-08-03 12:58:12,089 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-03 12:58:12,089 INFO: Will evaluate fitness for 5 individuals.
2023-08-03 12:58:24,868 DEBUG: Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-08-03 12:58:24,868 DEBUG: Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-08-03 12:58:27,227 DEBUG: Creating converter from 7 to 5
2023-08-03 12:58:27,227 DEBUG: Creating converter from 7 to 5
2023-08-03 12:58:27,227 DEBUG: Creating converter from 5 to 7
2023-08-03 12:58:27,227 DEBUG: Creating converter from 5 to 7
2023-08-03 12:58:27,227 DEBUG: Creating converter from 7 to 5
2023-08-03 12:58:27,227 DEBUG: Creating converter from 7 to 5
2023-08-03 12:58:27,227 DEBUG: Creating converter from 5 to 7
2023-08-03 12:58:27,227 DEBUG: Creating converter from 5 to 7
2023-08-03 12:58:43,134 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.7892605205420121, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.13690578079034232 ,rho=0.20401263547364834 ,epsilon=0.0009877841327735196 ,centered=False ,beta_1=0.007869387555294538 ,beta_2=0.5158946393778648 ,learning_rate_power=-0.09623103804652422 ,initial_accumulator_value=0.16201619075268447 ,l1_regularization_strength=0.5246319324392514 ,l2_regularization_strength=0.4949715738069945 ,l2_shrinkage_regularization_strength=0.23273294725473315 ,beta=0.014702544314142085].
2023-08-03 12:58:43,134 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.7892605205420121, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.13690578079034232 ,rho=0.20401263547364834 ,epsilon=0.0009877841327735196 ,centered=False ,beta_1=0.007869387555294538 ,beta_2=0.5158946393778648 ,learning_rate_power=-0.09623103804652422 ,initial_accumulator_value=0.16201619075268447 ,l1_regularization_strength=0.5246319324392514 ,l2_regularization_strength=0.4949715738069945 ,l2_shrinkage_regularization_strength=0.23273294725473315 ,beta=0.014702544314142085].
2023-08-03 12:58:43,146 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.42463613086333263, momentum=0.3836635812589053 ,nesterov=True ,amsgrad=False ,weight_decay=0.008751595203647008 ,use_ema=False ,ema_momentum=0.07203190809680449 ,rho=0.20824697665826875 ,epsilon=0.0006901229036231165 ,centered=True ,beta_1=0.5635839210508413 ,beta_2=0.8393075998739784 ,learning_rate_power=-0.24915695866786713 ,initial_accumulator_value=0.5805855832365422 ,l1_regularization_strength=0.20412322737541788 ,l2_regularization_strength=0.8527020177086823 ,l2_shrinkage_regularization_strength=0.7192813043684066 ,beta=0.33789690926324334].
2023-08-03 12:58:43,146 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.42463613086333263, momentum=0.3836635812589053 ,nesterov=True ,amsgrad=False ,weight_decay=0.008751595203647008 ,use_ema=False ,ema_momentum=0.07203190809680449 ,rho=0.20824697665826875 ,epsilon=0.0006901229036231165 ,centered=True ,beta_1=0.5635839210508413 ,beta_2=0.8393075998739784 ,learning_rate_power=-0.24915695866786713 ,initial_accumulator_value=0.5805855832365422 ,l1_regularization_strength=0.20412322737541788 ,l2_regularization_strength=0.8527020177086823 ,l2_shrinkage_regularization_strength=0.7192813043684066 ,beta=0.33789690926324334].
2023-08-03 12:59:10,543 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.5145647954494365, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.008526158994291932 ,use_ema=False ,ema_momentum=0.5913206937889133 ,rho=0.6687205802254935 ,epsilon=0.0003284796055127387 ,centered=False ,beta_1=0.2697056195888847 ,beta_2=0.2167316147619871 ,learning_rate_power=-0.05799168644480135 ,initial_accumulator_value=0.6000114228814452 ,l1_regularization_strength=0.4096067776878848 ,l2_regularization_strength=0.04891075094628927 ,l2_shrinkage_regularization_strength=0.6853270530246022 ,beta=0.39324331289752934].
2023-08-03 12:59:10,543 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.5145647954494365, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.008526158994291932 ,use_ema=False ,ema_momentum=0.5913206937889133 ,rho=0.6687205802254935 ,epsilon=0.0003284796055127387 ,centered=False ,beta_1=0.2697056195888847 ,beta_2=0.2167316147619871 ,learning_rate_power=-0.05799168644480135 ,initial_accumulator_value=0.6000114228814452 ,l1_regularization_strength=0.4096067776878848 ,l2_regularization_strength=0.04891075094628927 ,l2_shrinkage_regularization_strength=0.6853270530246022 ,beta=0.39324331289752934].
2023-08-03 12:59:10,876 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.21530538693173484, momentum=0.20221582834342489 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.684655591850109 ,rho=0.6035502104674876 ,epsilon=0.00037397091065957775 ,centered=False ,beta_1=0.9438772230550765 ,beta_2=0.2818951183208891 ,learning_rate_power=-0.47471981677579356 ,initial_accumulator_value=0.061408336787153406 ,l1_regularization_strength=0.6871827919786242 ,l2_regularization_strength=0.6588711953214506 ,l2_shrinkage_regularization_strength=0.2840907495383014 ,beta=0.8302892530605006].
2023-08-03 12:59:10,876 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.21530538693173484, momentum=0.20221582834342489 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.684655591850109 ,rho=0.6035502104674876 ,epsilon=0.00037397091065957775 ,centered=False ,beta_1=0.9438772230550765 ,beta_2=0.2818951183208891 ,learning_rate_power=-0.47471981677579356 ,initial_accumulator_value=0.061408336787153406 ,l1_regularization_strength=0.6871827919786242 ,l2_regularization_strength=0.6588711953214506 ,l2_shrinkage_regularization_strength=0.2840907495383014 ,beta=0.8302892530605006].
2023-08-03 12:59:29,752 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 12:59:29,752 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 12:59:49,475 INFO: Applying selection operators for generation 1.
2023-08-03 12:59:49,475 INFO: Applying genetic operators for generation 1.
2023-08-03 12:59:49,476 INFO: Evaluating fitness for for generation 1.
2023-08-03 12:59:49,477 INFO: Will evaluate fitness for 4 individuals.
2023-08-03 12:59:49,956 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 12:59:49,956 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 12:59:49,960 INFO: Building the Keras optimizer to use for [Base=Adam, learning_rate=0.5133952349678159, momentum=0.2717411294335871 ,nesterov=False ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.8907835571997968 ,rho=0.780540826289044 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.6935981853463751 ,beta_2=0.2976444006816852 ,learning_rate_power=-0.09092849826944582 ,initial_accumulator_value=0.6364469920517164 ,l1_regularization_strength=0.9301449997551554 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.020878566508993668].
2023-08-03 12:59:49,960 INFO: Building Adam optimizer to use for [Base=Adam, learning_rate=0.5133952349678159, momentum=0.2717411294335871 ,nesterov=False ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.8907835571997968 ,rho=0.780540826289044 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.6935981853463751 ,beta_2=0.2976444006816852 ,learning_rate_power=-0.09092849826944582 ,initial_accumulator_value=0.6364469920517164 ,l1_regularization_strength=0.9301449997551554 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.020878566508993668].
2023-08-03 13:00:10,470 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.5145647954494365, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.008526158994291932 ,use_ema=False ,ema_momentum=0.07203190809680449 ,rho=0.6687205802254935 ,epsilon=0.0003284796055127387 ,centered=False ,beta_1=0.2697056195888847 ,beta_2=0.2167316147619871 ,learning_rate_power=-0.05799168644480135 ,initial_accumulator_value=0.6000114228814452 ,l1_regularization_strength=0.4096067776878848 ,l2_regularization_strength=0.04891075094628927 ,l2_shrinkage_regularization_strength=0.6853270530246022 ,beta=0.39324331289752934].
2023-08-03 13:00:10,470 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.5145647954494365, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.008526158994291932 ,use_ema=False ,ema_momentum=0.07203190809680449 ,rho=0.6687205802254935 ,epsilon=0.0003284796055127387 ,centered=False ,beta_1=0.2697056195888847 ,beta_2=0.2167316147619871 ,learning_rate_power=-0.05799168644480135 ,initial_accumulator_value=0.6000114228814452 ,l1_regularization_strength=0.4096067776878848 ,l2_regularization_strength=0.04891075094628927 ,l2_shrinkage_regularization_strength=0.6853270530246022 ,beta=0.39324331289752934].
2023-08-03 13:00:11,815 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.42463613086333263, momentum=0.3836635812589053 ,nesterov=True ,amsgrad=False ,weight_decay=0.008751595203647008 ,use_ema=False ,ema_momentum=0.5913206937889133 ,rho=0.20824697665826875 ,epsilon=0.0006901229036231165 ,centered=True ,beta_1=0.5635839210508413 ,beta_2=0.8393075998739784 ,learning_rate_power=-0.24915695866786713 ,initial_accumulator_value=0.5805855832365422 ,l1_regularization_strength=0.20412322737541788 ,l2_regularization_strength=0.8527020177086823 ,l2_shrinkage_regularization_strength=0.7192813043684066 ,beta=0.33789690926324334].
2023-08-03 13:00:11,815 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.42463613086333263, momentum=0.3836635812589053 ,nesterov=True ,amsgrad=False ,weight_decay=0.008751595203647008 ,use_ema=False ,ema_momentum=0.5913206937889133 ,rho=0.20824697665826875 ,epsilon=0.0006901229036231165 ,centered=True ,beta_1=0.5635839210508413 ,beta_2=0.8393075998739784 ,learning_rate_power=-0.24915695866786713 ,initial_accumulator_value=0.5805855832365422 ,l1_regularization_strength=0.20412322737541788 ,l2_regularization_strength=0.8527020177086823 ,l2_shrinkage_regularization_strength=0.7192813043684066 ,beta=0.33789690926324334].
2023-08-03 13:00:31,546 INFO: Applying selection operators for generation 2.
2023-08-03 13:00:31,547 INFO: Applying genetic operators for generation 2.
2023-08-03 13:00:31,547 INFO: Evaluating fitness for for generation 2.
2023-08-03 13:00:31,547 INFO: Will evaluate fitness for 4 individuals.
2023-08-03 13:00:32,022 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.42463613086333263, momentum=0.3836635812589053 ,nesterov=True ,amsgrad=False ,weight_decay=0.008751595203647008 ,use_ema=False ,ema_momentum=0.5913206937889133 ,rho=0.20824697665826875 ,epsilon=0.0006901229036231165 ,centered=True ,beta_1=0.5635839210508413 ,beta_2=0.8393075998739784 ,learning_rate_power=-0.24915695866786713 ,initial_accumulator_value=0.5805855832365422 ,l1_regularization_strength=0.20412322737541788 ,l2_regularization_strength=0.8527020177086823 ,l2_shrinkage_regularization_strength=0.7192813043684066 ,beta=0.33789690926324334].
2023-08-03 13:00:32,022 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.42463613086333263, momentum=0.3836635812589053 ,nesterov=True ,amsgrad=False ,weight_decay=0.008751595203647008 ,use_ema=False ,ema_momentum=0.5913206937889133 ,rho=0.20824697665826875 ,epsilon=0.0006901229036231165 ,centered=True ,beta_1=0.5635839210508413 ,beta_2=0.8393075998739784 ,learning_rate_power=-0.24915695866786713 ,initial_accumulator_value=0.5805855832365422 ,l1_regularization_strength=0.20412322737541788 ,l2_regularization_strength=0.8527020177086823 ,l2_shrinkage_regularization_strength=0.7192813043684066 ,beta=0.33789690926324334].
2023-08-03 13:00:32,026 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.42463613086333263, momentum=0.3836635812589053 ,nesterov=True ,amsgrad=False ,weight_decay=0.008751595203647008 ,use_ema=False ,ema_momentum=0.5913206937889133 ,rho=0.20824697665826875 ,epsilon=0.0006901229036231165 ,centered=True ,beta_1=0.5635839210508413 ,beta_2=0.8393075998739784 ,learning_rate_power=-0.24915695866786713 ,initial_accumulator_value=0.5805855832365422 ,l1_regularization_strength=0.20412322737541788 ,l2_regularization_strength=0.8527020177086823 ,l2_shrinkage_regularization_strength=0.7192813043684066 ,beta=0.33789690926324334].
2023-08-03 13:00:32,027 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.42463613086333263, momentum=0.3836635812589053 ,nesterov=True ,amsgrad=False ,weight_decay=0.008751595203647008 ,use_ema=False ,ema_momentum=0.5913206937889133 ,rho=0.20824697665826875 ,epsilon=0.0006901229036231165 ,centered=True ,beta_1=0.5635839210508413 ,beta_2=0.8393075998739784 ,learning_rate_power=-0.24915695866786713 ,initial_accumulator_value=0.5805855832365422 ,l1_regularization_strength=0.20412322737541788 ,l2_regularization_strength=0.8527020177086823 ,l2_shrinkage_regularization_strength=0.7192813043684066 ,beta=0.33789690926324334].
2023-08-03 13:00:51,117 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.42463613086333263, momentum=0.3836635812589053 ,nesterov=True ,amsgrad=False ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.5635839210508413 ,beta_2=0.8393075998739784 ,learning_rate_power=-0.24915695866786713 ,initial_accumulator_value=0.5805855832365422 ,l1_regularization_strength=0.20412322737541788 ,l2_regularization_strength=0.8527020177086823 ,l2_shrinkage_regularization_strength=0.7192813043684066 ,beta=0.33789690926324334].
2023-08-03 13:00:51,117 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.42463613086333263, momentum=0.3836635812589053 ,nesterov=True ,amsgrad=False ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.5635839210508413 ,beta_2=0.8393075998739784 ,learning_rate_power=-0.24915695866786713 ,initial_accumulator_value=0.5805855832365422 ,l1_regularization_strength=0.20412322737541788 ,l2_regularization_strength=0.8527020177086823 ,l2_shrinkage_regularization_strength=0.7192813043684066 ,beta=0.33789690926324334].
2023-08-03 13:00:51,178 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008751595203647008 ,use_ema=False ,ema_momentum=0.5913206937889133 ,rho=0.20824697665826875 ,epsilon=0.0006901229036231165 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:00:51,178 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008751595203647008 ,use_ema=False ,ema_momentum=0.5913206937889133 ,rho=0.20824697665826875 ,epsilon=0.0006901229036231165 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:01:10,075 INFO: Applying selection operators for generation 3.
2023-08-03 13:01:10,076 INFO: Applying genetic operators for generation 3.
2023-08-03 13:01:10,076 INFO: Evaluating fitness for for generation 3.
2023-08-03 13:01:10,076 INFO: Will evaluate fitness for 4 individuals.
2023-08-03 13:01:10,564 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.5635839210508413 ,beta_2=0.8393075998739784 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:01:10,564 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.5635839210508413 ,beta_2=0.8393075998739784 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:01:10,568 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.42463613086333263, momentum=0.3836635812589053 ,nesterov=True ,amsgrad=False ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.24915695866786713 ,initial_accumulator_value=0.5805855832365422 ,l1_regularization_strength=0.20412322737541788 ,l2_regularization_strength=0.8527020177086823 ,l2_shrinkage_regularization_strength=0.7192813043684066 ,beta=0.33789690926324334].
2023-08-03 13:01:10,569 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.42463613086333263, momentum=0.3836635812589053 ,nesterov=True ,amsgrad=False ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.24915695866786713 ,initial_accumulator_value=0.5805855832365422 ,l1_regularization_strength=0.20412322737541788 ,l2_regularization_strength=0.8527020177086823 ,l2_shrinkage_regularization_strength=0.7192813043684066 ,beta=0.33789690926324334].
2023-08-03 13:01:29,652 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:01:29,652 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:01:29,822 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:01:29,822 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:01:48,590 INFO: Applying selection operators for generation 4.
2023-08-03 13:01:48,590 INFO: Applying genetic operators for generation 4.
2023-08-03 13:01:48,591 INFO: Evaluating fitness for for generation 4.
2023-08-03 13:01:48,591 INFO: Will evaluate fitness for 4 individuals.
2023-08-03 13:01:49,077 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:01:49,078 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:01:49,079 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:01:49,079 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:02:08,189 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9250586762190163, momentum=0.6528903854202834 ,nesterov=False ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.0004507272583528479 ,centered=True ,beta_1=0.4789548276475476 ,beta_2=0.4137196621847019 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.8595116920962365 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.4211986625590606 ,beta=0.8320967011834174].
2023-08-03 13:02:08,190 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9250586762190163, momentum=0.6528903854202834 ,nesterov=False ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.0004507272583528479 ,centered=True ,beta_1=0.4789548276475476 ,beta_2=0.4137196621847019 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.8595116920962365 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.4211986625590606 ,beta=0.8320967011834174].
2023-08-03 13:02:08,347 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008751595203647008 ,use_ema=False ,ema_momentum=0.5913206937889133 ,rho=0.20824697665826875 ,epsilon=0.0006901229036231165 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:02:08,348 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008751595203647008 ,use_ema=False ,ema_momentum=0.5913206937889133 ,rho=0.20824697665826875 ,epsilon=0.0006901229036231165 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:02:28,436 INFO: Applying selection operators for generation 5.
2023-08-03 13:02:28,437 INFO: Applying genetic operators for generation 5.
2023-08-03 13:02:28,437 INFO: Evaluating fitness for for generation 5.
2023-08-03 13:02:28,437 INFO: Will evaluate fitness for 2 individuals.
2023-08-03 13:02:28,925 INFO: Building the Keras optimizer to use for [Base=Adadelta, learning_rate=0.43457137559268577, momentum=0.2917523919934102 ,nesterov=True ,amsgrad=True ,weight_decay=0.009627589085928243 ,use_ema=True ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=False ,beta_1=0.41446186513558103 ,beta_2=0.055564420757508115 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.24206700695319305 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:02:28,925 INFO: Building Adadelta optimizer to use for [Base=Adadelta, learning_rate=0.43457137559268577, momentum=0.2917523919934102 ,nesterov=True ,amsgrad=True ,weight_decay=0.009627589085928243 ,use_ema=True ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=False ,beta_1=0.41446186513558103 ,beta_2=0.055564420757508115 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.24206700695319305 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:02:28,925 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
2023-08-03 13:02:28,925 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.829749949976381, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008961686849280606 ,use_ema=False ,ema_momentum=0.9100314742694164 ,rho=0.5404273307972887 ,epsilon=0.00011125230260560478 ,centered=True ,beta_1=0.23773838012457993 ,beta_2=0.8697018371646825 ,learning_rate_power=-0.3182501345802571 ,initial_accumulator_value=0.010579367305145637 ,l1_regularization_strength=0.40785781223358897 ,l2_regularization_strength=0.1911484651323221 ,l2_shrinkage_regularization_strength=0.6306105680392141 ,beta=0.8320967011834174].
