2023-08-03 18:15:17,917 INFO: Starting experiment with the arguments logged below.
2023-08-03 18:15:17,917 INFO: Namespace(results_path='/mnt/lustre/users/mbornman1/evoopt-cnn/experiments/fashionmnist_pop20_gen40_alexnet_tou5_cxp50_mut20_gmp50_alexnet_epo10_bat1024_sed1', cpu_count=2, seed=1, dataset='fashion_mnist', pop_size=20, ngen=40, model='alexnet', epochs=10, batch_size=1024, tournsize=5, cxpb=0.5, mutpb=0.2, gene_mut_prob=0.5)
2023-08-03 18:15:17,917 INFO: Setting the random number generator seed for this experiment.
2023-08-03 18:15:17,917 INFO: Initializing the multiprocessing pool.
2023-08-03 18:15:17,927 INFO: Running the evolutionary algorithm with the given hyper-parameters. This may take a while. Statistics for every generation will be printed below.
2023-08-03 18:15:17,927 INFO: Setting up DEAP toolbox.
2023-08-03 18:15:17,927 INFO: Registering individual initialization method.
2023-08-03 18:15:17,927 INFO: Registering population initialization method.
2023-08-03 18:15:17,927 INFO: Registering the selection method.
2023-08-03 18:15:17,927 INFO: Registering the evaluation method.
2023-08-03 18:15:17,927 INFO: Registering the genetic operators.
2023-08-03 18:15:17,927 INFO: Setting up the hall of fame and stats we want to keep track of.
2023-08-03 18:15:17,927 INFO: Initializing the initial population.
2023-08-03 18:15:17,950 INFO: Running the evolutionary algorithm.
2023-08-03 18:15:17,950 INFO: Evaluating fitness for the initial generation of individuals.
2023-08-03 18:15:17,950 INFO: Will evaluate fitness for 20 individuals.
2023-08-03 18:15:19,572 DEBUG: Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-08-03 18:15:19,572 DEBUG: Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-08-03 18:15:19,843 DEBUG: Creating converter from 7 to 5
2023-08-03 18:15:19,843 DEBUG: Creating converter from 7 to 5
2023-08-03 18:15:19,843 DEBUG: Creating converter from 5 to 7
2023-08-03 18:15:19,843 DEBUG: Creating converter from 5 to 7
2023-08-03 18:15:19,843 DEBUG: Creating converter from 7 to 5
2023-08-03 18:15:19,843 DEBUG: Creating converter from 7 to 5
2023-08-03 18:15:19,843 DEBUG: Creating converter from 5 to 7
2023-08-03 18:15:19,843 DEBUG: Creating converter from 5 to 7
2023-08-03 18:15:25,232 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.2227988821180935, momentum=0.7944230206920849 ,nesterov=True ,amsgrad=False ,weight_decay=0.00800500614084864 ,use_ema=False ,ema_momentum=0.9998020806514435 ,rho=0.7713249155735865 ,epsilon=0.000968002341529183 ,centered=True ,beta_1=0.7785767471828772 ,beta_2=0.9528098391114165 ,learning_rate_power=-0.9161094096958404 ,initial_accumulator_value=0.3890359377770447 ,l1_regularization_strength=0.9914302166182112 ,l2_regularization_strength=0.4638993657485858 ,l2_shrinkage_regularization_strength=0.8967539409456841 ,beta=0.23863920104393765].
2023-08-03 18:15:25,233 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.2227988821180935, momentum=0.7944230206920849 ,nesterov=True ,amsgrad=False ,weight_decay=0.00800500614084864 ,use_ema=False ,ema_momentum=0.9998020806514435 ,rho=0.7713249155735865 ,epsilon=0.000968002341529183 ,centered=True ,beta_1=0.7785767471828772 ,beta_2=0.9528098391114165 ,learning_rate_power=-0.9161094096958404 ,initial_accumulator_value=0.3890359377770447 ,l1_regularization_strength=0.9914302166182112 ,l2_regularization_strength=0.4638993657485858 ,l2_shrinkage_regularization_strength=0.8967539409456841 ,beta=0.23863920104393765].
2023-08-03 18:15:25,252 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.05142581958423986, momentum=0.3100866376305802 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.17421936258302984 ,rho=0.7036358220426868 ,epsilon=0.00016019339674992237 ,centered=False ,beta_1=0.26630496822680916 ,beta_2=0.2276666144617986 ,learning_rate_power=-0.724752906962613 ,initial_accumulator_value=0.2835979968572403 ,l1_regularization_strength=0.9701319404505007 ,l2_regularization_strength=0.31083125351145324 ,l2_shrinkage_regularization_strength=0.9657827501061889 ,beta=0.5857677316058388].
2023-08-03 18:15:25,252 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.05142581958423986, momentum=0.3100866376305802 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.17421936258302984 ,rho=0.7036358220426868 ,epsilon=0.00016019339674992237 ,centered=False ,beta_1=0.26630496822680916 ,beta_2=0.2276666144617986 ,learning_rate_power=-0.724752906962613 ,initial_accumulator_value=0.2835979968572403 ,l1_regularization_strength=0.9701319404505007 ,l2_regularization_strength=0.31083125351145324 ,l2_shrinkage_regularization_strength=0.9657827501061889 ,beta=0.5857677316058388].
2023-08-03 18:15:52,729 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.5776051224382196, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.03484805538007385 ,rho=0.32722809123752616 ,epsilon=0.0004159059595584733 ,centered=False ,beta_1=0.7370737896760065 ,beta_2=0.7849413944629755 ,learning_rate_power=-0.3153617034805989 ,initial_accumulator_value=0.36131126429109195 ,l1_regularization_strength=0.1303734228036476 ,l2_regularization_strength=0.804477105530082 ,l2_shrinkage_regularization_strength=0.022316953613097823 ,beta=0.44393579506417014].
2023-08-03 18:15:52,729 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.5776051224382196, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.03484805538007385 ,rho=0.32722809123752616 ,epsilon=0.0004159059595584733 ,centered=False ,beta_1=0.7370737896760065 ,beta_2=0.7849413944629755 ,learning_rate_power=-0.3153617034805989 ,initial_accumulator_value=0.36131126429109195 ,l1_regularization_strength=0.1303734228036476 ,l2_regularization_strength=0.804477105530082 ,l2_shrinkage_regularization_strength=0.022316953613097823 ,beta=0.44393579506417014].
2023-08-03 18:15:57,886 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.5515628696416867, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.007732753249794531 ,use_ema=False ,ema_momentum=0.510838876341507 ,rho=0.48911941121154845 ,epsilon=0.000886667025574642 ,centered=True ,beta_1=0.6417567939173262 ,beta_2=0.7321661119320917 ,learning_rate_power=-0.9838855316124239 ,initial_accumulator_value=0.7896081891765332 ,l1_regularization_strength=0.1856195377756148 ,l2_regularization_strength=0.3451688344003556 ,l2_shrinkage_regularization_strength=0.33758358391826637 ,beta=0.11297377703609324].
2023-08-03 18:15:57,886 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.5515628696416867, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.007732753249794531 ,use_ema=False ,ema_momentum=0.510838876341507 ,rho=0.48911941121154845 ,epsilon=0.000886667025574642 ,centered=True ,beta_1=0.6417567939173262 ,beta_2=0.7321661119320917 ,learning_rate_power=-0.9838855316124239 ,initial_accumulator_value=0.7896081891765332 ,l1_regularization_strength=0.1856195377756148 ,l2_regularization_strength=0.3451688344003556 ,l2_shrinkage_regularization_strength=0.33758358391826637 ,beta=0.11297377703609324].
2023-08-03 18:16:14,675 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.12356268261410286, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.007503878942873098 ,use_ema=False ,ema_momentum=0.7192532655380136 ,rho=0.684470516470616 ,epsilon=9.558965777447963e-05 ,centered=False ,beta_1=0.978306518771984 ,beta_2=0.06274460088744205 ,learning_rate_power=-0.031374417407634914 ,initial_accumulator_value=0.7592814897528769 ,l1_regularization_strength=0.22271498016931823 ,l2_regularization_strength=0.07988854821628466 ,l2_shrinkage_regularization_strength=0.15464134085024828 ,beta=0.40336921419201677].
2023-08-03 18:16:14,676 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.12356268261410286, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.007503878942873098 ,use_ema=False ,ema_momentum=0.7192532655380136 ,rho=0.684470516470616 ,epsilon=9.558965777447963e-05 ,centered=False ,beta_1=0.978306518771984 ,beta_2=0.06274460088744205 ,learning_rate_power=-0.031374417407634914 ,initial_accumulator_value=0.7592814897528769 ,l1_regularization_strength=0.22271498016931823 ,l2_regularization_strength=0.07988854821628466 ,l2_shrinkage_regularization_strength=0.15464134085024828 ,beta=0.40336921419201677].
2023-08-03 18:16:19,910 INFO: Building the Keras optimizer to use for [Base=Ftrl, learning_rate=0.7921332417464847, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.006047651829242997 ,use_ema=False ,ema_momentum=0.9782636305793425 ,rho=0.4596620738274213 ,epsilon=0.0009295386094739998 ,centered=False ,beta_1=0.7162236489844565 ,beta_2=0.3617057347449446 ,learning_rate_power=-0.14988597129310777 ,initial_accumulator_value=0.6376381558402975 ,l1_regularization_strength=0.19293513762526282 ,l2_regularization_strength=0.11739149741791177 ,l2_shrinkage_regularization_strength=0.059316566645080004 ,beta=0.6687741788074077].
2023-08-03 18:16:19,910 INFO: Building Ftrl optimizer to use for [Base=Ftrl, learning_rate=0.7921332417464847, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.006047651829242997 ,use_ema=False ,ema_momentum=0.9782636305793425 ,rho=0.4596620738274213 ,epsilon=0.0009295386094739998 ,centered=False ,beta_1=0.7162236489844565 ,beta_2=0.3617057347449446 ,learning_rate_power=-0.14988597129310777 ,initial_accumulator_value=0.6376381558402975 ,l1_regularization_strength=0.19293513762526282 ,l2_regularization_strength=0.11739149741791177 ,l2_shrinkage_regularization_strength=0.059316566645080004 ,beta=0.6687741788074077].
2023-08-03 18:16:36,022 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.6017027238458981, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.00829297312395499 ,use_ema=True ,ema_momentum=0.4971154211728147 ,rho=0.061949952348120396 ,epsilon=0.00021628150667799256 ,centered=True ,beta_1=0.6230601556244172 ,beta_2=0.8009691409763187 ,learning_rate_power=-0.32215215030178146 ,initial_accumulator_value=0.8758628851765163 ,l1_regularization_strength=0.16862849855951922 ,l2_regularization_strength=0.36175608292549 ,l2_shrinkage_regularization_strength=0.7789925249544253 ,beta=0.21790147699127638].
2023-08-03 18:16:36,022 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.6017027238458981, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.00829297312395499 ,use_ema=True ,ema_momentum=0.4971154211728147 ,rho=0.061949952348120396 ,epsilon=0.00021628150667799256 ,centered=True ,beta_1=0.6230601556244172 ,beta_2=0.8009691409763187 ,learning_rate_power=-0.32215215030178146 ,initial_accumulator_value=0.8758628851765163 ,l1_regularization_strength=0.16862849855951922 ,l2_regularization_strength=0.36175608292549 ,l2_shrinkage_regularization_strength=0.7789925249544253 ,beta=0.21790147699127638].
2023-08-03 18:16:43,072 INFO: Building the Keras optimizer to use for [Base=Adam, learning_rate=0.8924076469701606, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=0.004237968189195551 ,use_ema=False ,ema_momentum=0.6740556962742845 ,rho=0.513014160846997 ,epsilon=0.00020698325799343555 ,centered=False ,beta_1=0.7714007283109098 ,beta_2=0.8952452233581082 ,learning_rate_power=-0.4900780241557934 ,initial_accumulator_value=0.679685137636444 ,l1_regularization_strength=0.5377173487873951 ,l2_regularization_strength=0.07634255898439135 ,l2_shrinkage_regularization_strength=0.9272692876729844 ,beta=0.26333341140756095].
2023-08-03 18:16:43,072 INFO: Building Adam optimizer to use for [Base=Adam, learning_rate=0.8924076469701606, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=0.004237968189195551 ,use_ema=False ,ema_momentum=0.6740556962742845 ,rho=0.513014160846997 ,epsilon=0.00020698325799343555 ,centered=False ,beta_1=0.7714007283109098 ,beta_2=0.8952452233581082 ,learning_rate_power=-0.4900780241557934 ,initial_accumulator_value=0.679685137636444 ,l1_regularization_strength=0.5377173487873951 ,l2_regularization_strength=0.07634255898439135 ,l2_shrinkage_regularization_strength=0.9272692876729844 ,beta=0.26333341140756095].
2023-08-03 18:17:01,061 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.35147008459438, momentum=0.8082391065150721 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07456245847826926 ,rho=0.2407200029805806 ,epsilon=0.0008617565706164213 ,centered=False ,beta_1=0.023870861736984827 ,beta_2=0.3592414768582569 ,learning_rate_power=-0.3668283214619419 ,initial_accumulator_value=0.37779740596501965 ,l1_regularization_strength=0.1904163442139284 ,l2_regularization_strength=0.41458732152346167 ,l2_shrinkage_regularization_strength=0.01412568548841564 ,beta=0.5675719765756659].
2023-08-03 18:17:01,062 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.35147008459438, momentum=0.8082391065150721 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07456245847826926 ,rho=0.2407200029805806 ,epsilon=0.0008617565706164213 ,centered=False ,beta_1=0.023870861736984827 ,beta_2=0.3592414768582569 ,learning_rate_power=-0.3668283214619419 ,initial_accumulator_value=0.37779740596501965 ,l1_regularization_strength=0.1904163442139284 ,l2_regularization_strength=0.41458732152346167 ,l2_shrinkage_regularization_strength=0.01412568548841564 ,beta=0.5675719765756659].
2023-08-03 18:17:06,690 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:17:06,691 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:17:23,656 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.48934858944707504, momentum=0.10524043694866703 ,nesterov=True ,amsgrad=True ,weight_decay=0.0012638308415041477 ,use_ema=False ,ema_momentum=0.6347166606598859 ,rho=0.3452519137587078 ,epsilon=0.00047025607385954126 ,centered=False ,beta_1=0.8122735052634658 ,beta_2=0.9452900747721875 ,learning_rate_power=-0.2988187569747004 ,initial_accumulator_value=0.703607947088775 ,l1_regularization_strength=0.5911057952599814 ,l2_regularization_strength=0.7772978265860645 ,l2_shrinkage_regularization_strength=0.26210555308268946 ,beta=0.2788403262334167].
2023-08-03 18:17:23,657 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.48934858944707504, momentum=0.10524043694866703 ,nesterov=True ,amsgrad=True ,weight_decay=0.0012638308415041477 ,use_ema=False ,ema_momentum=0.6347166606598859 ,rho=0.3452519137587078 ,epsilon=0.00047025607385954126 ,centered=False ,beta_1=0.8122735052634658 ,beta_2=0.9452900747721875 ,learning_rate_power=-0.2988187569747004 ,initial_accumulator_value=0.703607947088775 ,l1_regularization_strength=0.5911057952599814 ,l2_regularization_strength=0.7772978265860645 ,l2_shrinkage_regularization_strength=0.26210555308268946 ,beta=0.2788403262334167].
2023-08-03 18:17:28,811 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.7888420420760004, momentum=0.12955259712111433 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9552260991997987 ,rho=0.18523696287819091 ,epsilon=1.8598535738683658e-05 ,centered=False ,beta_1=0.4816432422856587 ,beta_2=0.9318398239076996 ,learning_rate_power=-0.3219247023571594 ,initial_accumulator_value=0.8623592914627588 ,l1_regularization_strength=0.7898005556181097 ,l2_regularization_strength=0.6669192132686438 ,l2_shrinkage_regularization_strength=0.7535467933219827 ,beta=0.30545139087571616].
2023-08-03 18:17:28,811 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.7888420420760004, momentum=0.12955259712111433 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9552260991997987 ,rho=0.18523696287819091 ,epsilon=1.8598535738683658e-05 ,centered=False ,beta_1=0.4816432422856587 ,beta_2=0.9318398239076996 ,learning_rate_power=-0.3219247023571594 ,initial_accumulator_value=0.8623592914627588 ,l1_regularization_strength=0.7898005556181097 ,l2_regularization_strength=0.6669192132686438 ,l2_shrinkage_regularization_strength=0.7535467933219827 ,beta=0.30545139087571616].
2023-08-03 18:17:45,398 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.2901580521578797, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.006705835345062227 ,use_ema=False ,ema_momentum=0.07411764011560218 ,rho=0.8018749958577461 ,epsilon=0.0004228857648596191 ,centered=False ,beta_1=0.8825068575840208 ,beta_2=0.7084826955550771 ,learning_rate_power=-0.38031508274213566 ,initial_accumulator_value=0.5814343585737816 ,l1_regularization_strength=0.7855066721826579 ,l2_regularization_strength=0.2743497279707985 ,l2_shrinkage_regularization_strength=0.25241960857598356 ,beta=0.17143894819846384].
2023-08-03 18:17:45,399 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.2901580521578797, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.006705835345062227 ,use_ema=False ,ema_momentum=0.07411764011560218 ,rho=0.8018749958577461 ,epsilon=0.0004228857648596191 ,centered=False ,beta_1=0.8825068575840208 ,beta_2=0.7084826955550771 ,learning_rate_power=-0.38031508274213566 ,initial_accumulator_value=0.5814343585737816 ,l1_regularization_strength=0.7855066721826579 ,l2_regularization_strength=0.2743497279707985 ,l2_shrinkage_regularization_strength=0.25241960857598356 ,beta=0.17143894819846384].
2023-08-03 18:17:49,668 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.2371410076101781, momentum=0.957203893392492 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.2209495656371302 ,rho=0.25326824501451095 ,epsilon=0.0005814094042632438 ,centered=True ,beta_1=0.5604294906754307 ,beta_2=0.6837699457616392 ,learning_rate_power=-0.562425260553125 ,initial_accumulator_value=0.7951155871571373 ,l1_regularization_strength=0.9805548053357139 ,l2_regularization_strength=0.30396532111943264 ,l2_shrinkage_regularization_strength=0.9257363204633576 ,beta=0.4415976557778486].
2023-08-03 18:17:49,668 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.2371410076101781, momentum=0.957203893392492 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.2209495656371302 ,rho=0.25326824501451095 ,epsilon=0.0005814094042632438 ,centered=True ,beta_1=0.5604294906754307 ,beta_2=0.6837699457616392 ,learning_rate_power=-0.562425260553125 ,initial_accumulator_value=0.7951155871571373 ,l1_regularization_strength=0.9805548053357139 ,l2_regularization_strength=0.30396532111943264 ,l2_shrinkage_regularization_strength=0.9257363204633576 ,beta=0.4415976557778486].
2023-08-03 18:18:07,904 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.5267426048524844, momentum=0.41434559519955716 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.13492945390739464 ,rho=0.040924477273808546 ,epsilon=0.00028386649587700636 ,centered=True ,beta_1=0.43989181725949456 ,beta_2=0.36245135850785815 ,learning_rate_power=-0.6319946976792447 ,initial_accumulator_value=0.5148563316187021 ,l1_regularization_strength=0.7696069313215129 ,l2_regularization_strength=0.5653295404221189 ,l2_shrinkage_regularization_strength=0.6605871989577535 ,beta=0.7519961121785561].
2023-08-03 18:18:07,904 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.5267426048524844, momentum=0.41434559519955716 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.13492945390739464 ,rho=0.040924477273808546 ,epsilon=0.00028386649587700636 ,centered=True ,beta_1=0.43989181725949456 ,beta_2=0.36245135850785815 ,learning_rate_power=-0.6319946976792447 ,initial_accumulator_value=0.5148563316187021 ,l1_regularization_strength=0.7696069313215129 ,l2_regularization_strength=0.5653295404221189 ,l2_shrinkage_regularization_strength=0.6605871989577535 ,beta=0.7519961121785561].
2023-08-03 18:18:13,776 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.1851437290476261, momentum=0.56096118149896 ,nesterov=True ,amsgrad=False ,weight_decay=0.0025908469789552315 ,use_ema=False ,ema_momentum=0.039914142887790494 ,rho=0.37306708708617564 ,epsilon=0.0008111819532427529 ,centered=True ,beta_1=0.25696083370477685 ,beta_2=0.7367346679511643 ,learning_rate_power=-0.7395539402298122 ,initial_accumulator_value=0.49101791829097785 ,l1_regularization_strength=0.1004596663319276 ,l2_regularization_strength=0.4642740430462413 ,l2_shrinkage_regularization_strength=0.9002839165815879 ,beta=0.1582482425098961].
2023-08-03 18:18:13,776 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.1851437290476261, momentum=0.56096118149896 ,nesterov=True ,amsgrad=False ,weight_decay=0.0025908469789552315 ,use_ema=False ,ema_momentum=0.039914142887790494 ,rho=0.37306708708617564 ,epsilon=0.0008111819532427529 ,centered=True ,beta_1=0.25696083370477685 ,beta_2=0.7367346679511643 ,learning_rate_power=-0.7395539402298122 ,initial_accumulator_value=0.49101791829097785 ,l1_regularization_strength=0.1004596663319276 ,l2_regularization_strength=0.4642740430462413 ,l2_shrinkage_regularization_strength=0.9002839165815879 ,beta=0.1582482425098961].
2023-08-03 18:18:28,891 INFO: Building the Keras optimizer to use for [Base=Ftrl, learning_rate=0.2242926167362893, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.5263163253322755 ,rho=0.6992995324702402 ,epsilon=6.986090117892124e-06 ,centered=True ,beta_1=0.9174368021192266 ,beta_2=0.5225613652746275 ,learning_rate_power=-0.8335571948819962 ,initial_accumulator_value=0.2992934270630567 ,l1_regularization_strength=0.8066085972544804 ,l2_regularization_strength=0.7948122964538487 ,l2_shrinkage_regularization_strength=0.9127623301282001 ,beta=0.2787612699831451].
2023-08-03 18:18:28,891 INFO: Building Ftrl optimizer to use for [Base=Ftrl, learning_rate=0.2242926167362893, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.5263163253322755 ,rho=0.6992995324702402 ,epsilon=6.986090117892124e-06 ,centered=True ,beta_1=0.9174368021192266 ,beta_2=0.5225613652746275 ,learning_rate_power=-0.8335571948819962 ,initial_accumulator_value=0.2992934270630567 ,l1_regularization_strength=0.8066085972544804 ,l2_regularization_strength=0.7948122964538487 ,l2_shrinkage_regularization_strength=0.9127623301282001 ,beta=0.2787612699831451].
2023-08-03 18:18:37,272 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.7423737146024841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.0034196148193335525 ,use_ema=True ,ema_momentum=0.5943968427448371 ,rho=0.29850283576292225 ,epsilon=0.0005811729811732214 ,centered=True ,beta_1=0.8485335425052123 ,beta_2=0.67386213838017 ,learning_rate_power=-0.3448445471174162 ,initial_accumulator_value=0.6613004751068721 ,l1_regularization_strength=0.8680669940212825 ,l2_regularization_strength=0.213370768561856 ,l2_shrinkage_regularization_strength=0.5837928579287582 ,beta=0.6759270554824357].
2023-08-03 18:18:37,272 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.7423737146024841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.0034196148193335525 ,use_ema=True ,ema_momentum=0.5943968427448371 ,rho=0.29850283576292225 ,epsilon=0.0005811729811732214 ,centered=True ,beta_1=0.8485335425052123 ,beta_2=0.67386213838017 ,learning_rate_power=-0.3448445471174162 ,initial_accumulator_value=0.6613004751068721 ,l1_regularization_strength=0.8680669940212825 ,l2_regularization_strength=0.213370768561856 ,l2_shrinkage_regularization_strength=0.5837928579287582 ,beta=0.6759270554824357].
2023-08-03 18:18:51,589 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.978445072200473, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.5226734147704694 ,rho=0.7846747690074231 ,epsilon=0.0006944390096971888 ,centered=False ,beta_1=0.7755487922937851 ,beta_2=0.4308417973705718 ,learning_rate_power=-0.20487016101165023 ,initial_accumulator_value=0.3986692065356452 ,l1_regularization_strength=0.9835521908428725 ,l2_regularization_strength=0.7271319845738262 ,l2_shrinkage_regularization_strength=0.5906620312767182 ,beta=0.9734853375261809].
2023-08-03 18:18:51,590 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.978445072200473, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.5226734147704694 ,rho=0.7846747690074231 ,epsilon=0.0006944390096971888 ,centered=False ,beta_1=0.7755487922937851 ,beta_2=0.4308417973705718 ,learning_rate_power=-0.20487016101165023 ,initial_accumulator_value=0.3986692065356452 ,l1_regularization_strength=0.9835521908428725 ,l2_regularization_strength=0.7271319845738262 ,l2_shrinkage_regularization_strength=0.5906620312767182 ,beta=0.9734853375261809].
2023-08-03 18:19:13,126 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:19:13,127 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:19:34,702 INFO: Applying selection operators for generation 1.
2023-08-03 18:19:34,702 INFO: Applying genetic operators for generation 1.
2023-08-03 18:19:34,703 INFO: Evaluating fitness for for generation 1.
2023-08-03 18:19:34,703 INFO: Will evaluate fitness for 14 individuals.
2023-08-03 18:19:35,230 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.7888420420760004, momentum=0.12955259712111433 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9552260991997987 ,rho=0.18523696287819091 ,epsilon=1.8598535738683658e-05 ,centered=False ,beta_1=0.8122735052634658 ,beta_2=0.9452900747721875 ,learning_rate_power=-0.2988187569747004 ,initial_accumulator_value=0.703607947088775 ,l1_regularization_strength=0.5911057952599814 ,l2_regularization_strength=0.6669192132686438 ,l2_shrinkage_regularization_strength=0.7535467933219827 ,beta=0.30545139087571616].
2023-08-03 18:19:35,230 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.7888420420760004, momentum=0.12955259712111433 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9552260991997987 ,rho=0.18523696287819091 ,epsilon=1.8598535738683658e-05 ,centered=False ,beta_1=0.8122735052634658 ,beta_2=0.9452900747721875 ,learning_rate_power=-0.2988187569747004 ,initial_accumulator_value=0.703607947088775 ,l1_regularization_strength=0.5911057952599814 ,l2_regularization_strength=0.6669192132686438 ,l2_shrinkage_regularization_strength=0.7535467933219827 ,beta=0.30545139087571616].
2023-08-03 18:19:35,386 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.9161094096958404 ,initial_accumulator_value=0.3890359377770447 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:19:35,386 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.9161094096958404 ,initial_accumulator_value=0.3890359377770447 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:19:55,823 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.2227988821180935, momentum=0.7944230206920849 ,nesterov=True ,amsgrad=False ,weight_decay=0.00800500614084864 ,use_ema=False ,ema_momentum=0.9998020806514435 ,rho=0.7713249155735865 ,epsilon=0.000968002341529183 ,centered=True ,beta_1=0.7785767471828772 ,beta_2=0.9528098391114165 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.9914302166182112 ,l2_regularization_strength=0.4638993657485858 ,l2_shrinkage_regularization_strength=0.8967539409456841 ,beta=0.23863920104393765].
2023-08-03 18:19:55,823 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.2227988821180935, momentum=0.7944230206920849 ,nesterov=True ,amsgrad=False ,weight_decay=0.00800500614084864 ,use_ema=False ,ema_momentum=0.9998020806514435 ,rho=0.7713249155735865 ,epsilon=0.000968002341529183 ,centered=True ,beta_1=0.7785767471828772 ,beta_2=0.9528098391114165 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.9914302166182112 ,l2_regularization_strength=0.4638993657485858 ,l2_shrinkage_regularization_strength=0.8967539409456841 ,beta=0.23863920104393765].
2023-08-03 18:19:55,865 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.48934858944707504, momentum=0.10524043694866703 ,nesterov=True ,amsgrad=True ,weight_decay=0.0012638308415041477 ,use_ema=False ,ema_momentum=0.6347166606598859 ,rho=0.3452519137587078 ,epsilon=0.00047025607385954126 ,centered=False ,beta_1=0.4816432422856587 ,beta_2=0.9318398239076996 ,learning_rate_power=-0.3219247023571594 ,initial_accumulator_value=0.8623592914627588 ,l1_regularization_strength=0.7898005556181097 ,l2_regularization_strength=0.7772978265860645 ,l2_shrinkage_regularization_strength=0.26210555308268946 ,beta=0.2788403262334167].
2023-08-03 18:19:55,865 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.48934858944707504, momentum=0.10524043694866703 ,nesterov=True ,amsgrad=True ,weight_decay=0.0012638308415041477 ,use_ema=False ,ema_momentum=0.6347166606598859 ,rho=0.3452519137587078 ,epsilon=0.00047025607385954126 ,centered=False ,beta_1=0.4816432422856587 ,beta_2=0.9318398239076996 ,learning_rate_power=-0.3219247023571594 ,initial_accumulator_value=0.8623592914627588 ,l1_regularization_strength=0.7898005556181097 ,l2_regularization_strength=0.7772978265860645 ,l2_shrinkage_regularization_strength=0.26210555308268946 ,beta=0.2788403262334167].
2023-08-03 18:20:15,471 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:20:15,471 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:20:18,210 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:20:18,210 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:20:37,903 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:20:37,903 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:20:38,796 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:20:38,797 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:20:57,905 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.2988187569747004 ,initial_accumulator_value=0.703607947088775 ,l1_regularization_strength=0.5911057952599814 ,l2_regularization_strength=0.7772978265860645 ,l2_shrinkage_regularization_strength=0.26210555308268946 ,beta=0.2788403262334167].
2023-08-03 18:20:57,906 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.2988187569747004 ,initial_accumulator_value=0.703607947088775 ,l1_regularization_strength=0.5911057952599814 ,l2_regularization_strength=0.7772978265860645 ,l2_shrinkage_regularization_strength=0.26210555308268946 ,beta=0.2788403262334167].
2023-08-03 18:20:59,096 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.2901580521578797, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.006705835345062227 ,use_ema=False ,ema_momentum=0.07411764011560218 ,rho=0.8018749958577461 ,epsilon=0.0004228857648596191 ,centered=False ,beta_1=0.8825068575840208 ,beta_2=0.7084826955550771 ,learning_rate_power=-0.38031508274213566 ,initial_accumulator_value=0.5814343585737816 ,l1_regularization_strength=0.7855066721826579 ,l2_regularization_strength=0.2743497279707985 ,l2_shrinkage_regularization_strength=0.25241960857598356 ,beta=0.17143894819846384].
2023-08-03 18:20:59,096 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.2901580521578797, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.006705835345062227 ,use_ema=False ,ema_momentum=0.07411764011560218 ,rho=0.8018749958577461 ,epsilon=0.0004228857648596191 ,centered=False ,beta_1=0.8825068575840208 ,beta_2=0.7084826955550771 ,learning_rate_power=-0.38031508274213566 ,initial_accumulator_value=0.5814343585737816 ,l1_regularization_strength=0.7855066721826579 ,l2_regularization_strength=0.2743497279707985 ,l2_shrinkage_regularization_strength=0.25241960857598356 ,beta=0.17143894819846384].
2023-08-03 18:21:18,439 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.48934858944707504, momentum=0.10524043694866703 ,nesterov=True ,amsgrad=True ,weight_decay=0.0012638308415041477 ,use_ema=False ,ema_momentum=0.6347166606598859 ,rho=0.3452519137587078 ,epsilon=0.00047025607385954126 ,centered=False ,beta_1=0.8122735052634658 ,beta_2=0.9452900747721875 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:21:18,440 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.48934858944707504, momentum=0.10524043694866703 ,nesterov=True ,amsgrad=True ,weight_decay=0.0012638308415041477 ,use_ema=False ,ema_momentum=0.6347166606598859 ,rho=0.3452519137587078 ,epsilon=0.00047025607385954126 ,centered=False ,beta_1=0.8122735052634658 ,beta_2=0.9452900747721875 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:21:21,520 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.2901580521578797, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.006705835345062227 ,use_ema=False ,ema_momentum=0.07411764011560218 ,rho=0.8018749958577461 ,epsilon=0.0004228857648596191 ,centered=False ,beta_1=0.8825068575840208 ,beta_2=0.7084826955550771 ,learning_rate_power=-0.38031508274213566 ,initial_accumulator_value=0.5814343585737816 ,l1_regularization_strength=0.7855066721826579 ,l2_regularization_strength=0.2743497279707985 ,l2_shrinkage_regularization_strength=0.25241960857598356 ,beta=0.17143894819846384].
2023-08-03 18:21:21,520 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.2901580521578797, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.006705835345062227 ,use_ema=False ,ema_momentum=0.07411764011560218 ,rho=0.8018749958577461 ,epsilon=0.0004228857648596191 ,centered=False ,beta_1=0.8825068575840208 ,beta_2=0.7084826955550771 ,learning_rate_power=-0.38031508274213566 ,initial_accumulator_value=0.5814343585737816 ,l1_regularization_strength=0.7855066721826579 ,l2_regularization_strength=0.2743497279707985 ,l2_shrinkage_regularization_strength=0.25241960857598356 ,beta=0.17143894819846384].
2023-08-03 18:21:37,978 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.007503878942873098 ,use_ema=False ,ema_momentum=0.7192532655380136 ,rho=0.684470516470616 ,epsilon=9.558965777447963e-05 ,centered=False ,beta_1=0.978306518771984 ,beta_2=0.06274460088744205 ,learning_rate_power=-0.031374417407634914 ,initial_accumulator_value=0.7592814897528769 ,l1_regularization_strength=0.22271498016931823 ,l2_regularization_strength=0.07988854821628466 ,l2_shrinkage_regularization_strength=0.15464134085024828 ,beta=0.40336921419201677].
2023-08-03 18:21:37,978 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.007503878942873098 ,use_ema=False ,ema_momentum=0.7192532655380136 ,rho=0.684470516470616 ,epsilon=9.558965777447963e-05 ,centered=False ,beta_1=0.978306518771984 ,beta_2=0.06274460088744205 ,learning_rate_power=-0.031374417407634914 ,initial_accumulator_value=0.7592814897528769 ,l1_regularization_strength=0.22271498016931823 ,l2_regularization_strength=0.07988854821628466 ,l2_shrinkage_regularization_strength=0.15464134085024828 ,beta=0.40336921419201677].
2023-08-03 18:21:57,801 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.12356268261410286, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:21:57,801 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.12356268261410286, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:22:17,774 INFO: Applying selection operators for generation 2.
2023-08-03 18:22:17,774 INFO: Applying genetic operators for generation 2.
2023-08-03 18:22:17,775 INFO: Evaluating fitness for for generation 2.
2023-08-03 18:22:17,776 INFO: Will evaluate fitness for 14 individuals.
2023-08-03 18:22:18,303 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:22:18,304 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:22:18,420 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:22:18,421 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:22:38,914 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:22:38,914 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:22:41,188 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:22:41,188 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:22:59,765 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:22:59,765 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:23:02,112 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:23:02,112 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:23:20,309 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:23:20,310 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:23:23,199 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:23:23,200 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:23:40,950 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=0.0025612789389810914 ,use_ema=True ,ema_momentum=0.7734116261271153 ,rho=0.6113110670992823 ,epsilon=0.00041725678201922104 ,centered=True ,beta_1=0.39651745979972963 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.4833192259206872 ,initial_accumulator_value=0.9276886328680699 ,l1_regularization_strength=0.7491087594954552 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.7206211105862749 ,beta=0.4595789340603377].
2023-08-03 18:23:40,950 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=0.0025612789389810914 ,use_ema=True ,ema_momentum=0.7734116261271153 ,rho=0.6113110670992823 ,epsilon=0.00041725678201922104 ,centered=True ,beta_1=0.39651745979972963 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.4833192259206872 ,initial_accumulator_value=0.9276886328680699 ,l1_regularization_strength=0.7491087594954552 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.7206211105862749 ,beta=0.4595789340603377].
2023-08-03 18:23:44,175 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:23:44,176 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:24:03,902 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.12138064410914318].
2023-08-03 18:24:03,902 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.12138064410914318].
2023-08-03 18:24:05,581 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:24:05,581 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:24:24,200 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.9161094096958404 ,initial_accumulator_value=0.3890359377770447 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:24:24,200 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.9161094096958404 ,initial_accumulator_value=0.3890359377770447 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:24:44,055 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.9161094096958404 ,initial_accumulator_value=0.3890359377770447 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:24:44,056 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.9161094096958404 ,initial_accumulator_value=0.3890359377770447 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:25:03,518 INFO: Applying selection operators for generation 3.
2023-08-03 18:25:03,518 INFO: Applying genetic operators for generation 3.
2023-08-03 18:25:03,519 INFO: Evaluating fitness for for generation 3.
2023-08-03 18:25:03,519 INFO: Will evaluate fitness for 10 individuals.
2023-08-03 18:25:04,026 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:25:04,026 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:25:04,046 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.12138064410914318].
2023-08-03 18:25:04,047 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.12138064410914318].
2023-08-03 18:25:24,610 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.613204762355715, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6112819406493571 ,rho=0.5679202370533989 ,epsilon=0.0003833071579846069 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.7966712201124944 ,learning_rate_power=-0.9161094096958404 ,initial_accumulator_value=0.3890359377770447 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.4083801195000879].
2023-08-03 18:25:24,611 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.613204762355715, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6112819406493571 ,rho=0.5679202370533989 ,epsilon=0.0003833071579846069 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.7966712201124944 ,learning_rate_power=-0.9161094096958404 ,initial_accumulator_value=0.3890359377770447 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.4083801195000879].
2023-08-03 18:25:24,974 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:25:24,974 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:25:45,987 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:25:45,987 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:25:46,788 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.2988187569747004 ,initial_accumulator_value=0.703607947088775 ,l1_regularization_strength=0.5911057952599814 ,l2_regularization_strength=0.7772978265860645 ,l2_shrinkage_regularization_strength=0.26210555308268946 ,beta=0.4083801195000879].
2023-08-03 18:25:46,788 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.2988187569747004 ,initial_accumulator_value=0.703607947088775 ,l1_regularization_strength=0.5911057952599814 ,l2_regularization_strength=0.7772978265860645 ,l2_shrinkage_regularization_strength=0.26210555308268946 ,beta=0.4083801195000879].
2023-08-03 18:26:06,774 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.8743774684762426, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.790142469593172 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=False ,beta_1=0.5941134114752747 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.9161094096958404 ,initial_accumulator_value=0.3890359377770447 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.4247864085335602].
2023-08-03 18:26:06,774 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.8743774684762426, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.790142469593172 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=False ,beta_1=0.5941134114752747 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.9161094096958404 ,initial_accumulator_value=0.3890359377770447 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.4247864085335602].
2023-08-03 18:26:07,460 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.2788403262334167].
2023-08-03 18:26:07,461 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.2788403262334167].
2023-08-03 18:26:27,975 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.9161094096958404 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:26:27,975 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.9161094096958404 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:26:47,891 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.3890359377770447 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:26:47,892 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.16002654708321062, momentum=0.7582560686142644 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.3890359377770447 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.02236917853392628 ,l2_shrinkage_regularization_strength=0.3534048424448123 ,beta=0.12138064410914318].
2023-08-03 18:27:08,236 INFO: Applying selection operators for generation 4.
2023-08-03 18:27:08,236 INFO: Applying genetic operators for generation 4.
2023-08-03 18:27:08,239 INFO: Evaluating fitness for for generation 4.
2023-08-03 18:27:08,239 INFO: Will evaluate fitness for 16 individuals.
2023-08-03 18:27:08,903 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:27:08,903 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:27:08,913 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:27:08,913 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:27:29,624 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:27:29,625 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:27:29,880 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.7118068100742689 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.648163503189234 ,rho=0.6113110670992823 ,epsilon=0.0005357518497100086 ,centered=True ,beta_1=0.37402797302399027 ,beta_2=0.09459467133454424 ,learning_rate_power=-0.8374708211407615 ,initial_accumulator_value=0.3828497085069774 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9006280839177415 ,beta=0.4083801195000879].
2023-08-03 18:27:29,880 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.7118068100742689 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.648163503189234 ,rho=0.6113110670992823 ,epsilon=0.0005357518497100086 ,centered=True ,beta_1=0.37402797302399027 ,beta_2=0.09459467133454424 ,learning_rate_power=-0.8374708211407615 ,initial_accumulator_value=0.3828497085069774 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9006280839177415 ,beta=0.4083801195000879].
2023-08-03 18:27:50,247 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.24524655337654755, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.12353682590620318 ,rho=0.13523970166714916 ,epsilon=0.0006490776325585537 ,centered=False ,beta_1=0.49975694977553 ,beta_2=0.570174030069475 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.15975296152724527 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7020846768347587 ,beta=0.4083801195000879].
2023-08-03 18:27:50,247 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.24524655337654755, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.12353682590620318 ,rho=0.13523970166714916 ,epsilon=0.0006490776325585537 ,centered=False ,beta_1=0.49975694977553 ,beta_2=0.570174030069475 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.15975296152724527 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7020846768347587 ,beta=0.4083801195000879].
2023-08-03 18:27:52,940 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.2788403262334167].
2023-08-03 18:27:52,941 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.2788403262334167].
2023-08-03 18:28:13,047 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:28:13,048 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0008253196823584551 ,centered=True ,beta_1=0.196639950835082 ,beta_2=0.4958428446214548 ,learning_rate_power=-0.6810755980034096 ,initial_accumulator_value=0.08877370224431558 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:28:14,166 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.003970051761191392 ,use_ema=True ,ema_momentum=0.5140428533189683 ,rho=0.4340955475041678 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.5033147417051047 ,l1_regularization_strength=0.398328069228139 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9597900288233946 ,beta=0.04048500359710783].
2023-08-03 18:28:14,166 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.003970051761191392 ,use_ema=True ,ema_momentum=0.5140428533189683 ,rho=0.4340955475041678 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.5033147417051047 ,l1_regularization_strength=0.398328069228139 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9597900288233946 ,beta=0.04048500359710783].
2023-08-03 18:28:33,661 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:28:33,662 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:28:35,708 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:28:35,708 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:28:54,241 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:28:54,241 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:28:56,701 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=False ,amsgrad=False ,weight_decay=0.007766253855779541 ,use_ema=True ,ema_momentum=0.5104548822495777 ,rho=0.5806057366384987 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.7328182697035238 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.3056359021511351 ,beta=0.4083801195000879].
2023-08-03 18:28:56,702 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=False ,amsgrad=False ,weight_decay=0.007766253855779541 ,use_ema=True ,ema_momentum=0.5104548822495777 ,rho=0.5806057366384987 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.7328182697035238 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.3056359021511351 ,beta=0.4083801195000879].
2023-08-03 18:29:14,756 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:29:14,757 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:29:18,200 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:29:18,200 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:29:35,708 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:29:35,709 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:29:39,545 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:29:39,545 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:29:59,852 INFO: Applying selection operators for generation 5.
2023-08-03 18:29:59,852 INFO: Applying genetic operators for generation 5.
2023-08-03 18:29:59,855 INFO: Evaluating fitness for for generation 5.
2023-08-03 18:29:59,855 INFO: Will evaluate fitness for 16 individuals.
2023-08-03 18:30:00,383 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.39403258761764215 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.5897933868123852 ,l1_regularization_strength=0.3942477425360693 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9545409759540653 ,beta=0.4083801195000879].
2023-08-03 18:30:00,384 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.39403258761764215 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.5897933868123852 ,l1_regularization_strength=0.3942477425360693 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9545409759540653 ,beta=0.4083801195000879].
2023-08-03 18:30:00,563 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:30:00,564 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:30:21,061 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:30:21,061 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:30:21,520 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.2788403262334167].
2023-08-03 18:30:21,520 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.2788403262334167].
2023-08-03 18:30:41,656 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:30:41,656 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:30:42,720 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.7068133394624884, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.005487735841284134 ,use_ema=True ,ema_momentum=0.5785360300522356 ,rho=0.650471903556834 ,epsilon=0.0005661007757633098 ,centered=True ,beta_1=0.3059225315547126 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.7490669888539164 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.1492235631631338 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4330594394820745 ,beta=0.4083801195000879].
2023-08-03 18:30:42,721 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.7068133394624884, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.005487735841284134 ,use_ema=True ,ema_momentum=0.5785360300522356 ,rho=0.650471903556834 ,epsilon=0.0005661007757633098 ,centered=True ,beta_1=0.3059225315547126 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.7490669888539164 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.1492235631631338 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4330594394820745 ,beta=0.4083801195000879].
2023-08-03 18:31:02,619 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:31:02,619 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:31:13,788 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.06245578113511219 ,rho=0.7031536052993892 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.7109701827775996 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.13348926377186654 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.6597567786288648].
2023-08-03 18:31:13,788 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.06245578113511219 ,rho=0.7031536052993892 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.7109701827775996 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.13348926377186654 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.6597567786288648].
2023-08-03 18:31:23,334 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:31:23,334 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:31:37,602 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:31:37,602 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:31:44,038 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:31:44,038 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:31:58,769 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:31:58,769 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:32:04,733 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:32:04,733 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:32:19,936 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:32:19,936 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:32:25,899 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.2788403262334167].
2023-08-03 18:32:25,899 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6409638232453914 ,rho=0.5679202370533989 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.17676979761386236 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.2788403262334167].
2023-08-03 18:32:41,360 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:32:41,360 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:33:03,656 INFO: Applying selection operators for generation 6.
2023-08-03 18:33:03,656 INFO: Applying genetic operators for generation 6.
2023-08-03 18:33:03,659 INFO: Evaluating fitness for for generation 6.
2023-08-03 18:33:03,659 INFO: Will evaluate fitness for 7 individuals.
2023-08-03 18:33:04,160 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:33:04,160 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:33:04,308 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:33:04,308 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:33:25,182 INFO: Building the Keras optimizer to use for [Base=Adadelta, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6875111741759613 ,rho=0.5610126242861723 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.6054854433149436 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.02113772420568405 ,beta=0.3142437073690333].
2023-08-03 18:33:25,182 INFO: Building Adadelta optimizer to use for [Base=Adadelta, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6875111741759613 ,rho=0.5610126242861723 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.6054854433149436 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.02113772420568405 ,beta=0.3142437073690333].
2023-08-03 18:33:25,987 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:33:25,988 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:33:47,377 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.5719114131754818 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.7893576862449349 ,beta_2=0.781861887631347 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.4524224851380232 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7484810603995387 ,beta=0.4083801195000879].
2023-08-03 18:33:47,377 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.5719114131754818 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.7893576862449349 ,beta_2=0.781861887631347 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.4524224851380232 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7484810603995387 ,beta=0.4083801195000879].
2023-08-03 18:33:49,098 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0032235314880367815 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.6577130836979568 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.8523376627092975 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.458605612615961 ,beta=0.3154390517428888].
2023-08-03 18:33:49,099 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0032235314880367815 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.6577130836979568 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.8523376627092975 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.458605612615961 ,beta=0.3154390517428888].
2023-08-03 18:34:08,778 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.0729444459850841, momentum=0.17802876368406528 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.13149914235930837 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.11951684591208322 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.0866071734313959 ,l1_regularization_strength=0.0026864805818362925 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.213091391015526 ,beta=0.7326962733659083].
2023-08-03 18:34:08,779 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.0729444459850841, momentum=0.17802876368406528 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.13149914235930837 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.11951684591208322 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.0866071734313959 ,l1_regularization_strength=0.0026864805818362925 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.213091391015526 ,beta=0.7326962733659083].
2023-08-03 18:34:31,951 INFO: Applying selection operators for generation 7.
2023-08-03 18:34:31,951 INFO: Applying genetic operators for generation 7.
2023-08-03 18:34:31,954 INFO: Evaluating fitness for for generation 7.
2023-08-03 18:34:31,954 INFO: Will evaluate fitness for 7 individuals.
2023-08-03 18:34:32,451 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.08073319059853523, momentum=0.20963575097806386 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.27680983717723695 ,rho=0.14121180299783354 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.6780601665210442 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.4672302653928738 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7566715654405857 ,beta=0.3154390517428888].
2023-08-03 18:34:32,451 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.08073319059853523, momentum=0.20963575097806386 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.27680983717723695 ,rho=0.14121180299783354 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.6780601665210442 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.4672302653928738 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7566715654405857 ,beta=0.3154390517428888].
2023-08-03 18:34:32,574 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:34:32,574 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:34:54,381 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:34:54,382 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:34:56,280 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.29404691921796366 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.20617052792807045 ,learning_rate_power=-0.5165425805124585 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.9262311237769955 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.05404166024424617 ,beta=0.3154390517428888].
2023-08-03 18:34:56,280 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.29404691921796366 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.20617052792807045 ,learning_rate_power=-0.5165425805124585 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.9262311237769955 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.05404166024424617 ,beta=0.3154390517428888].
2023-08-03 18:35:15,658 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.007459357482223343 ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=7.380245555743964e-05 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.9657544703396735 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.551581757041066 ,beta=0.3154390517428888].
2023-08-03 18:35:15,659 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.007459357482223343 ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=7.380245555743964e-05 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.9657544703396735 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.551581757041066 ,beta=0.3154390517428888].
2023-08-03 18:35:18,044 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.002824845748704543 ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.00018123970984939924 ,centered=False ,beta_1=0.9013273443475001 ,beta_2=0.6988320090801888 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.1590056167025219 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6817885697182521 ,beta=0.4083801195000879].
2023-08-03 18:35:18,044 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.002824845748704543 ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.00018123970984939924 ,centered=False ,beta_1=0.9013273443475001 ,beta_2=0.6988320090801888 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.1590056167025219 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6817885697182521 ,beta=0.4083801195000879].
2023-08-03 18:35:39,305 INFO: Building the Keras optimizer to use for [Base=Adadelta, learning_rate=0.27150673437129524, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.832191683429645 ,rho=0.29497168332767065 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.2412728273660446 ,beta_2=0.9133061156474409 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:35:39,305 INFO: Building Adadelta optimizer to use for [Base=Adadelta, learning_rate=0.27150673437129524, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.832191683429645 ,rho=0.29497168332767065 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.2412728273660446 ,beta_2=0.9133061156474409 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:36:03,121 INFO: Applying selection operators for generation 8.
2023-08-03 18:36:03,122 INFO: Applying genetic operators for generation 8.
2023-08-03 18:36:03,124 INFO: Evaluating fitness for for generation 8.
2023-08-03 18:36:03,124 INFO: Will evaluate fitness for 13 individuals.
2023-08-03 18:36:03,675 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:36:03,676 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:36:03,838 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:36:03,839 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:36:24,532 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.19964405594321244, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.9515462068614712 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.35880265570958536 ,beta=0.3154390517428888].
2023-08-03 18:36:24,532 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.19964405594321244, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.9515462068614712 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.35880265570958536 ,beta=0.3154390517428888].
2023-08-03 18:36:25,073 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:36:25,073 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.33312786662919125 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07128957391152235 ,rho=0.6113110670992823 ,epsilon=0.0003878737457886262 ,centered=True ,beta_1=0.9013273443475001 ,beta_2=0.7277406868028135 ,learning_rate_power=-0.22702807973494887 ,initial_accumulator_value=0.8431868448456038 ,l1_regularization_strength=0.3728493077091609 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4083801195000879].
2023-08-03 18:36:44,839 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.7664143529808994, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.0066269977666320356 ,use_ema=True ,ema_momentum=0.7995739502822151 ,rho=0.5090448390326792 ,epsilon=0.00089637061099747 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.5064029253170901 ,l1_regularization_strength=0.4764609664394812 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:36:44,839 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.7664143529808994, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.0066269977666320356 ,use_ema=True ,ema_momentum=0.7995739502822151 ,rho=0.5090448390326792 ,epsilon=0.00089637061099747 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.5064029253170901 ,l1_regularization_strength=0.4764609664394812 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:36:46,847 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.23388557614075456 ,rho=0.8001976339256608 ,epsilon=4.5550128862409475e-05 ,centered=False ,beta_1=0.37157103008888903 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.23562907349389972 ,initial_accumulator_value=0.3101390858228982 ,l1_regularization_strength=0.17666604325961033 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8187771719278716 ,beta=0.3154390517428888].
2023-08-03 18:36:46,847 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.23388557614075456 ,rho=0.8001976339256608 ,epsilon=4.5550128862409475e-05 ,centered=False ,beta_1=0.37157103008888903 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.23562907349389972 ,initial_accumulator_value=0.3101390858228982 ,l1_regularization_strength=0.17666604325961033 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8187771719278716 ,beta=0.3154390517428888].
2023-08-03 18:37:09,076 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:37:09,077 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:37:09,906 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:37:09,906 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:37:30,172 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:37:30,172 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:37:32,505 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:37:32,505 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:37:51,538 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.6019424536171223 ,epsilon=5.1274193857636524e-05 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.5093289978736447 ,l1_regularization_strength=0.828247146160052 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8398526641855784 ,beta=0.3154390517428888].
2023-08-03 18:37:51,538 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.6019424536171223 ,epsilon=5.1274193857636524e-05 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.5093289978736447 ,l1_regularization_strength=0.828247146160052 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8398526641855784 ,beta=0.3154390517428888].
2023-08-03 18:37:54,512 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:37:54,512 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:38:11,511 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:38:11,512 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:38:32,094 INFO: Applying selection operators for generation 9.
2023-08-03 18:38:32,094 INFO: Applying genetic operators for generation 9.
2023-08-03 18:38:32,097 INFO: Evaluating fitness for for generation 9.
2023-08-03 18:38:32,097 INFO: Will evaluate fitness for 14 individuals.
2023-08-03 18:38:32,620 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:38:32,621 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:38:32,739 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.7788408980034736 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.9397245184926923 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.5759887119669062 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.9952729633615304 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4626198372158997].
2023-08-03 18:38:32,739 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.7788408980034736 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.9397245184926923 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.5759887119669062 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.9952729633615304 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4626198372158997].
2023-08-03 18:38:53,547 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:38:53,548 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:38:54,724 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:38:54,724 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:39:14,235 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:39:14,236 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:39:16,901 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.12005714147025404, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.001845720997150034 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.713862981473942 ,beta_2=0.04098669546568612 ,learning_rate_power=-0.6459948261972865 ,initial_accumulator_value=0.02037306438000963 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.41843750825073467 ,beta=0.3154390517428888].
2023-08-03 18:39:16,901 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.12005714147025404, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.001845720997150034 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.713862981473942 ,beta_2=0.04098669546568612 ,learning_rate_power=-0.6459948261972865 ,initial_accumulator_value=0.02037306438000963 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.41843750825073467 ,beta=0.3154390517428888].
2023-08-03 18:39:35,226 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:39:35,226 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:39:42,794 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:39:42,794 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:39:56,556 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:39:56,556 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:40:05,039 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:40:05,039 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:40:17,481 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.7927537100907357, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.05747229208238591 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.865021330845398 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.22393817446223185 ,beta=0.7829658650425495].
2023-08-03 18:40:17,481 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.7927537100907357, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.05747229208238591 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.865021330845398 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.22393817446223185 ,beta=0.7829658650425495].
2023-08-03 18:40:27,963 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.48240913788996753, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0014320713393601832 ,use_ema=True ,ema_momentum=0.4767962092432323 ,rho=0.9706182587403125 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.31520888204955455 ,beta_2=0.2756259944428954 ,learning_rate_power=-0.03767776662939848 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.8168086267704988 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6977343929756674 ,beta=0.043051401628197206].
2023-08-03 18:40:27,963 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.48240913788996753, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0014320713393601832 ,use_ema=True ,ema_momentum=0.4767962092432323 ,rho=0.9706182587403125 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.31520888204955455 ,beta_2=0.2756259944428954 ,learning_rate_power=-0.03767776662939848 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.8168086267704988 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6977343929756674 ,beta=0.043051401628197206].
2023-08-03 18:40:37,845 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:40:37,845 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:40:58,479 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:40:58,480 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:41:18,833 INFO: Applying selection operators for generation 10.
2023-08-03 18:41:18,833 INFO: Applying genetic operators for generation 10.
2023-08-03 18:41:18,836 INFO: Evaluating fitness for for generation 10.
2023-08-03 18:41:18,836 INFO: Will evaluate fitness for 11 individuals.
2023-08-03 18:41:19,416 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:41:19,416 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:41:19,656 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:41:19,657 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:41:40,304 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.3682076579627448 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.8791277220712571 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5188026515644149 ,beta=0.3154390517428888].
2023-08-03 18:41:40,304 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.3682076579627448 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.8791277220712571 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5188026515644149 ,beta=0.3154390517428888].
2023-08-03 18:41:41,040 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:41:41,040 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:42:01,125 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.10848362899621722 ,rho=0.3478099135831231 ,epsilon=0.00031702176743931664 ,centered=True ,beta_1=0.29988180891256233 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.2827208122956636 ,initial_accumulator_value=0.9268740884014883 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:42:01,125 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.10848362899621722 ,rho=0.3478099135831231 ,epsilon=0.00031702176743931664 ,centered=True ,beta_1=0.29988180891256233 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.2827208122956636 ,initial_accumulator_value=0.9268740884014883 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:42:03,083 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0034145894135118133 ,use_ema=True ,ema_momentum=0.26465761893231277 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.46832866953042207 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.23454317966002514 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.252471540293178 ,beta=0.3154390517428888].
2023-08-03 18:42:03,083 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0034145894135118133 ,use_ema=True ,ema_momentum=0.26465761893231277 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.46832866953042207 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.23454317966002514 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.252471540293178 ,beta=0.3154390517428888].
2023-08-03 18:42:24,952 INFO: Building the Keras optimizer to use for [Base=Adadelta, learning_rate=0.9290409894910479, momentum=0.5325413386226433 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.9537056125515478 ,epsilon=0.0005251840671061293 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5356492103261729 ,beta=0.9468536765505197].
2023-08-03 18:42:24,952 INFO: Building Adadelta optimizer to use for [Base=Adadelta, learning_rate=0.9290409894910479, momentum=0.5325413386226433 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.9537056125515478 ,epsilon=0.0005251840671061293 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5356492103261729 ,beta=0.9468536765505197].
2023-08-03 18:42:25,289 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:42:25,290 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:42:46,703 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:42:46,703 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:42:49,678 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.1630307634298751, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.006059612222748713 ,use_ema=True ,ema_momentum=0.7061335812420385 ,rho=0.612511767071765 ,epsilon=0.0007744791624379434 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8650735914082547 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.9819505870610368 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4269028104817053 ,beta=0.3154390517428888].
2023-08-03 18:42:49,678 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.1630307634298751, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.006059612222748713 ,use_ema=True ,ema_momentum=0.7061335812420385 ,rho=0.612511767071765 ,epsilon=0.0007744791624379434 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8650735914082547 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.9819505870610368 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4269028104817053 ,beta=0.3154390517428888].
2023-08-03 18:43:07,858 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:43:07,858 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:43:28,894 INFO: Applying selection operators for generation 11.
2023-08-03 18:43:28,895 INFO: Applying genetic operators for generation 11.
2023-08-03 18:43:28,898 INFO: Evaluating fitness for for generation 11.
2023-08-03 18:43:28,898 INFO: Will evaluate fitness for 16 individuals.
2023-08-03 18:43:29,409 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.38917527665526375, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.003085482898437715 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.7568641553218132 ,epsilon=0.000262698468999361 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7935466120794554 ,learning_rate_power=-0.10818328967423707 ,initial_accumulator_value=0.7738558439033629 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.34468931418842996 ,beta=0.586218483128211].
2023-08-03 18:43:29,409 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.38917527665526375, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.003085482898437715 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.7568641553218132 ,epsilon=0.000262698468999361 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7935466120794554 ,learning_rate_power=-0.10818328967423707 ,initial_accumulator_value=0.7738558439033629 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.34468931418842996 ,beta=0.586218483128211].
2023-08-03 18:43:29,411 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.3294483909315342, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.22462380341333665 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.8269539559965297 ,l1_regularization_strength=0.2545282567661423 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:43:29,412 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.3294483909315342, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.22462380341333665 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.8269539559965297 ,l1_regularization_strength=0.2545282567661423 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:43:50,798 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:43:50,799 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:43:55,113 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:43:55,113 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:44:12,906 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:44:12,907 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:44:16,870 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:44:16,870 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:44:34,133 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:44:34,133 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:44:38,122 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:44:38,122 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:44:54,980 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:44:54,980 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:45:00,138 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.30333303156853886, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0005709743252353823 ,use_ema=True ,ema_momentum=0.20248669252295404 ,rho=0.9810930132900426 ,epsilon=7.389359415987885e-05 ,centered=True ,beta_1=0.8842604429884339 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.873643633855243 ,initial_accumulator_value=0.39913266727708774 ,l1_regularization_strength=0.7369984068849356 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:45:00,139 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.30333303156853886, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0005709743252353823 ,use_ema=True ,ema_momentum=0.20248669252295404 ,rho=0.9810930132900426 ,epsilon=7.389359415987885e-05 ,centered=True ,beta_1=0.8842604429884339 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.873643633855243 ,initial_accumulator_value=0.39913266727708774 ,l1_regularization_strength=0.7369984068849356 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:45:16,158 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.5110410116312863, momentum=0.9655768932898704 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8210900893926889 ,rho=0.32298739129182696 ,epsilon=0.0004920103758507692 ,centered=False ,beta_1=0.028376278748838635 ,beta_2=0.8847552816049824 ,learning_rate_power=-0.17532142558018504 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.44497231891605615].
2023-08-03 18:45:16,158 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.5110410116312863, momentum=0.9655768932898704 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.8210900893926889 ,rho=0.32298739129182696 ,epsilon=0.0004920103758507692 ,centered=False ,beta_1=0.028376278748838635 ,beta_2=0.8847552816049824 ,learning_rate_power=-0.17532142558018504 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.44497231891605615].
2023-08-03 18:45:24,266 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.0729444459850841, momentum=0.3088917453884994 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9940861156594908 ,rho=0.22550015743336604 ,epsilon=0.0003946619600732622 ,centered=True ,beta_1=0.899967943907892 ,beta_2=0.40093914086818316 ,learning_rate_power=-0.05441312174267987 ,initial_accumulator_value=0.13993472395228213 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9984230077478798 ,beta=0.07393969593309446].
2023-08-03 18:45:24,267 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.0729444459850841, momentum=0.3088917453884994 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9940861156594908 ,rho=0.22550015743336604 ,epsilon=0.0003946619600732622 ,centered=True ,beta_1=0.899967943907892 ,beta_2=0.40093914086818316 ,learning_rate_power=-0.05441312174267987 ,initial_accumulator_value=0.13993472395228213 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9984230077478798 ,beta=0.07393969593309446].
2023-08-03 18:45:39,560 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:45:39,560 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:45:49,153 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:45:49,154 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:46:00,649 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:46:00,649 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:46:10,941 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:46:10,941 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:46:31,771 INFO: Applying selection operators for generation 12.
2023-08-03 18:46:31,771 INFO: Applying genetic operators for generation 12.
2023-08-03 18:46:31,773 INFO: Evaluating fitness for for generation 12.
2023-08-03 18:46:31,773 INFO: Will evaluate fitness for 11 individuals.
2023-08-03 18:46:32,363 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:46:32,363 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:46:32,374 INFO: Building the Keras optimizer to use for [Base=Ftrl, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.00573537953623282 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.5859370297942469 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.40529172300229555 ,l1_regularization_strength=0.6255668661478473 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5175781394542012 ,beta=0.3154390517428888].
2023-08-03 18:46:32,374 INFO: Building Ftrl optimizer to use for [Base=Ftrl, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.00573537953623282 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.5859370297942469 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.40529172300229555 ,l1_regularization_strength=0.6255668661478473 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5175781394542012 ,beta=0.3154390517428888].
2023-08-03 18:46:53,309 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:46:53,309 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:46:57,643 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:46:57,643 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:47:14,739 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:47:14,739 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:47:18,720 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:47:18,721 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:47:35,681 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:47:35,681 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:47:40,501 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.20188959565875086, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.24870584262727247 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.21732139726226118 ,beta=0.026843707596786892].
2023-08-03 18:47:40,502 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.20188959565875086, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.24870584262727247 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.21732139726226118 ,beta=0.026843707596786892].
2023-08-03 18:47:56,818 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:47:56,819 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:48:01,763 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:48:01,763 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:48:17,744 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:48:17,744 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:48:38,682 INFO: Applying selection operators for generation 13.
2023-08-03 18:48:38,682 INFO: Applying genetic operators for generation 13.
2023-08-03 18:48:38,686 INFO: Evaluating fitness for for generation 13.
2023-08-03 18:48:38,686 INFO: Will evaluate fitness for 13 individuals.
2023-08-03 18:48:39,216 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:48:39,216 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:48:39,235 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.11303049860540071, momentum=0.9654993791075219 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.32645959784762324 ,epsilon=0.0002582299076807892 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.4935737335509961 ,learning_rate_power=-0.6052365947344913 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8951250646232636 ,beta=0.3154390517428888].
2023-08-03 18:48:39,236 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.11303049860540071, momentum=0.9654993791075219 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.32645959784762324 ,epsilon=0.0002582299076807892 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.4935737335509961 ,learning_rate_power=-0.6052365947344913 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8951250646232636 ,beta=0.3154390517428888].
2023-08-03 18:49:00,825 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:49:00,826 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:49:02,942 INFO: Building the Keras optimizer to use for [Base=Adadelta, learning_rate=0.34997101352110194, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.6387544589083307 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.5471264392105898 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.10598145866928277 ,beta=0.3154390517428888].
2023-08-03 18:49:02,943 INFO: Building Adadelta optimizer to use for [Base=Adadelta, learning_rate=0.34997101352110194, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.6387544589083307 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.5471264392105898 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.10598145866928277 ,beta=0.3154390517428888].
2023-08-03 18:49:22,652 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:49:22,652 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:49:27,231 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.4061176271744802, momentum=0.9755442019377386 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.10988251541653005 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.2016459594030694 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.28391529601901433 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8044243680528536 ,beta=0.3154390517428888].
2023-08-03 18:49:27,231 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.4061176271744802, momentum=0.9755442019377386 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.10988251541653005 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.2016459594030694 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.28391529601901433 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8044243680528536 ,beta=0.3154390517428888].
2023-08-03 18:49:43,961 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.6433238968203965, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.9063855942106045 ,epsilon=0.000641077079473607 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.5688054086456475 ,learning_rate_power=-0.05375694426750055 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5414923734243773 ,beta=0.05429771090991853].
2023-08-03 18:49:43,961 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.6433238968203965, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.9063855942106045 ,epsilon=0.000641077079473607 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.5688054086456475 ,learning_rate_power=-0.05375694426750055 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5414923734243773 ,beta=0.05429771090991853].
2023-08-03 18:49:50,984 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:49:50,984 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:50:08,452 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:50:08,453 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:50:12,347 INFO: Building the Keras optimizer to use for [Base=Ftrl, learning_rate=0.5109355307074744, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.04723904453688077 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.9037105596983688 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4056795566828707].
2023-08-03 18:50:12,347 INFO: Building Ftrl optimizer to use for [Base=Ftrl, learning_rate=0.5109355307074744, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.04723904453688077 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.9037105596983688 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.4056795566828707].
2023-08-03 18:50:29,739 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:50:29,740 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:50:35,985 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:50:35,986 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:50:51,825 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.20673142185861415 ,rho=0.46274312203008516 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.25321520236215433 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.32230372312446876 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7065543990960717 ,beta=0.3154390517428888].
2023-08-03 18:50:51,826 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.20673142185861415 ,rho=0.46274312203008516 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.25321520236215433 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.32230372312446876 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7065543990960717 ,beta=0.3154390517428888].
2023-08-03 18:51:12,221 INFO: Applying selection operators for generation 14.
2023-08-03 18:51:12,221 INFO: Applying genetic operators for generation 14.
2023-08-03 18:51:12,225 INFO: Evaluating fitness for for generation 14.
2023-08-03 18:51:12,225 INFO: Will evaluate fitness for 10 individuals.
2023-08-03 18:51:12,743 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:51:12,743 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:51:12,906 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:51:12,906 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:51:35,539 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=0.008970833666787812 ,use_ema=True ,ema_momentum=0.5076524555296265 ,rho=0.8001976339256608 ,epsilon=0.0008364234112610889 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.14714896261282084 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.3029820931246989 ,l1_regularization_strength=0.8495263653102483 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.981415480905544 ,beta=0.9101258032541282].
2023-08-03 18:51:35,539 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=0.008970833666787812 ,use_ema=True ,ema_momentum=0.5076524555296265 ,rho=0.8001976339256608 ,epsilon=0.0008364234112610889 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.14714896261282084 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.3029820931246989 ,l1_regularization_strength=0.8495263653102483 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.981415480905544 ,beta=0.9101258032541282].
2023-08-03 18:51:35,748 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:51:35,748 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:51:57,375 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.4582561171815275, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.23828814986659164 ,rho=0.8001976339256608 ,epsilon=0.0005729958616488194 ,centered=False ,beta_1=0.43787674085940076 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.476950478386981 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5490797808485298 ,beta=0.3154390517428888].
2023-08-03 18:51:57,375 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.4582561171815275, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.23828814986659164 ,rho=0.8001976339256608 ,epsilon=0.0005729958616488194 ,centered=False ,beta_1=0.43787674085940076 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.476950478386981 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5490797808485298 ,beta=0.3154390517428888].
2023-08-03 18:52:01,068 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.0729444459850841, momentum=0.7927158198019529 ,nesterov=True ,amsgrad=False ,weight_decay=0.005689509531085971 ,use_ema=True ,ema_momentum=0.44145685913191646 ,rho=0.8001976339256608 ,epsilon=0.00017167378658907692 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.12713417086185874 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.14476687295675306 ,beta=0.7590555918706094].
2023-08-03 18:52:01,068 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.0729444459850841, momentum=0.7927158198019529 ,nesterov=True ,amsgrad=False ,weight_decay=0.005689509531085971 ,use_ema=True ,ema_momentum=0.44145685913191646 ,rho=0.8001976339256608 ,epsilon=0.00017167378658907692 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.12713417086185874 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.14476687295675306 ,beta=0.7590555918706094].
2023-08-03 18:52:22,076 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.634432949022269, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.002229516903200717 ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0008520364942092451 ,centered=False ,beta_1=0.06791057651991494 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.2696278417038308 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4937297729124296 ,beta=0.3154390517428888].
2023-08-03 18:52:22,076 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.634432949022269, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.002229516903200717 ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0008520364942092451 ,centered=False ,beta_1=0.06791057651991494 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.2696278417038308 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4937297729124296 ,beta=0.3154390517428888].
2023-08-03 18:52:26,920 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.6038123795715337 ,rho=0.665237955208412 ,epsilon=0.0005199917296048845 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.4258871430829585 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.06580253721731943 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.22590147979624853 ,beta=0.032348568814762].
2023-08-03 18:52:26,920 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.6038123795715337 ,rho=0.665237955208412 ,epsilon=0.0005199917296048845 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.4258871430829585 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.06580253721731943 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.22590147979624853 ,beta=0.032348568814762].
2023-08-03 18:52:46,568 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:52:46,568 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:53:07,619 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:53:07,619 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:53:27,921 INFO: Applying selection operators for generation 15.
2023-08-03 18:53:27,922 INFO: Applying genetic operators for generation 15.
2023-08-03 18:53:27,925 INFO: Evaluating fitness for for generation 15.
2023-08-03 18:53:27,925 INFO: Will evaluate fitness for 15 individuals.
2023-08-03 18:53:28,424 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.8389541837779507, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.00878287284604489 ,use_ema=False ,ema_momentum=0.49772233696902535 ,rho=0.3271833791579384 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.27082460435733524 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.44551704108740875 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7550407928209644 ,beta=0.10421538355810711].
2023-08-03 18:53:28,424 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.8389541837779507, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.00878287284604489 ,use_ema=False ,ema_momentum=0.49772233696902535 ,rho=0.3271833791579384 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.27082460435733524 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.44551704108740875 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7550407928209644 ,beta=0.10421538355810711].
2023-08-03 18:53:28,591 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:53:28,591 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:53:48,318 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:53:48,318 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:53:51,061 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8954769711418693, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008470124253491586 ,use_ema=True ,ema_momentum=0.9950873094598292 ,rho=0.8206278346937652 ,epsilon=0.000724236497862739 ,centered=True ,beta_1=0.4504345060728281 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.940603631020825 ,initial_accumulator_value=0.8580860342474519 ,l1_regularization_strength=0.7080675904484589 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9397060717750801 ,beta=0.3154390517428888].
2023-08-03 18:53:51,061 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8954769711418693, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.008470124253491586 ,use_ema=True ,ema_momentum=0.9950873094598292 ,rho=0.8206278346937652 ,epsilon=0.000724236497862739 ,centered=True ,beta_1=0.4504345060728281 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.940603631020825 ,initial_accumulator_value=0.8580860342474519 ,l1_regularization_strength=0.7080675904484589 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9397060717750801 ,beta=0.3154390517428888].
2023-08-03 18:54:09,661 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:54:09,662 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:54:16,138 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:54:16,139 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:54:30,852 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:54:30,852 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:54:37,466 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0005898186631559312 ,centered=True ,beta_1=0.8416187771860723 ,beta_2=0.6440925596786767 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.7578207233195312 ,l1_regularization_strength=0.4566124911369449 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6197249932936211 ,beta=0.18521751946118248].
2023-08-03 18:54:37,467 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0005898186631559312 ,centered=True ,beta_1=0.8416187771860723 ,beta_2=0.6440925596786767 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.7578207233195312 ,l1_regularization_strength=0.4566124911369449 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6197249932936211 ,beta=0.18521751946118248].
2023-08-03 18:54:52,534 INFO: Building the Keras optimizer to use for [Base=Adam, learning_rate=0.9631099732083579, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.7118011822714636 ,epsilon=0.000589970770377178 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.8610704411624925 ,learning_rate_power=-0.1112235028363947 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.848449857907381 ,beta=0.00391111267460531].
2023-08-03 18:54:52,534 INFO: Building Adam optimizer to use for [Base=Adam, learning_rate=0.9631099732083579, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.7118011822714636 ,epsilon=0.000589970770377178 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.8610704411624925 ,learning_rate_power=-0.1112235028363947 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.848449857907381 ,beta=0.00391111267460531].
2023-08-03 18:55:02,899 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:55:02,899 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:55:18,842 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:55:18,842 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:55:25,668 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:55:25,668 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:55:39,850 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:55:39,850 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:55:47,099 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:55:47,099 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:56:00,644 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:56:00,644 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:56:21,616 INFO: Applying selection operators for generation 16.
2023-08-03 18:56:21,616 INFO: Applying genetic operators for generation 16.
2023-08-03 18:56:21,619 INFO: Evaluating fitness for for generation 16.
2023-08-03 18:56:21,619 INFO: Will evaluate fitness for 13 individuals.
2023-08-03 18:56:22,140 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:56:22,140 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:56:22,146 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:56:22,146 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:56:43,536 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:56:43,536 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:56:43,554 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.5533571510821075 ,epsilon=0.0004253138694269138 ,centered=False ,beta_1=0.09668960057225895 ,beta_2=0.2199439607020649 ,learning_rate_power=-0.5151283216870747 ,initial_accumulator_value=0.03881831518475887 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6600687837076098 ,beta=0.3154390517428888].
2023-08-03 18:56:43,554 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.5533571510821075 ,epsilon=0.0004253138694269138 ,centered=False ,beta_1=0.09668960057225895 ,beta_2=0.2199439607020649 ,learning_rate_power=-0.5151283216870747 ,initial_accumulator_value=0.03881831518475887 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6600687837076098 ,beta=0.3154390517428888].
2023-08-03 18:57:05,058 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:57:05,058 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:57:07,125 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:57:07,125 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:57:26,183 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.38628577438026956, momentum=0.22410734214114858 ,nesterov=True ,amsgrad=False ,weight_decay=0.004556213845841008 ,use_ema=True ,ema_momentum=0.24013726239271127 ,rho=0.8430916908117181 ,epsilon=0.00017759512609725724 ,centered=False ,beta_1=0.7478502371968911 ,beta_2=0.45196573556609443 ,learning_rate_power=-0.7681103718683394 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4203771454365657 ,beta=0.14173371103834265].
2023-08-03 18:57:26,184 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.38628577438026956, momentum=0.22410734214114858 ,nesterov=True ,amsgrad=False ,weight_decay=0.004556213845841008 ,use_ema=True ,ema_momentum=0.24013726239271127 ,rho=0.8430916908117181 ,epsilon=0.00017759512609725724 ,centered=False ,beta_1=0.7478502371968911 ,beta_2=0.45196573556609443 ,learning_rate_power=-0.7681103718683394 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4203771454365657 ,beta=0.14173371103834265].
2023-08-03 18:57:27,765 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:57:27,765 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:57:49,006 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.2355397776951781, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.0011732287160677159 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.00047950008357720093 ,centered=True ,beta_1=0.7449424325689381 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.09553337856715349 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9471184908768365 ,beta=0.3154390517428888].
2023-08-03 18:57:49,006 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.2355397776951781, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.0011732287160677159 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.00047950008357720093 ,centered=True ,beta_1=0.7449424325689381 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.09553337856715349 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9471184908768365 ,beta=0.3154390517428888].
2023-08-03 18:57:51,144 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:57:51,145 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:58:12,831 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.39327272050706796, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0009516720855492869 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.02014159507953106 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.6078918805043021 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.7309524741456708 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.563192099156129 ,beta=0.3154390517428888].
2023-08-03 18:58:12,831 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.39327272050706796, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0009516720855492869 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.02014159507953106 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.6078918805043021 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.7309524741456708 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.563192099156129 ,beta=0.3154390517428888].
2023-08-03 18:58:13,272 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:58:13,272 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:58:33,982 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:58:33,983 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:58:53,994 INFO: Applying selection operators for generation 17.
2023-08-03 18:58:53,995 INFO: Applying genetic operators for generation 17.
2023-08-03 18:58:53,997 INFO: Evaluating fitness for for generation 17.
2023-08-03 18:58:53,997 INFO: Will evaluate fitness for 8 individuals.
2023-08-03 18:58:54,517 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:58:54,517 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:58:54,520 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.11460063016470445, momentum=0.5128662235643701 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.44194412254017457 ,initial_accumulator_value=0.466778194750067 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.3044738618123557 ,beta=0.3154390517428888].
2023-08-03 18:58:54,520 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.11460063016470445, momentum=0.5128662235643701 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.44194412254017457 ,initial_accumulator_value=0.466778194750067 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.3044738618123557 ,beta=0.3154390517428888].
2023-08-03 18:59:16,013 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:59:16,013 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:59:28,361 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:59:28,361 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:59:37,951 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:59:37,952 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:59:49,039 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:59:49,039 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:59:59,718 INFO: Building the Keras optimizer to use for [Base=Adadelta, learning_rate=0.7156065019391488, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.2804578072658813 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.674862276609254 ,beta_2=0.1306050159506792 ,learning_rate_power=-0.0779189487201083 ,initial_accumulator_value=0.6411621211754084 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 18:59:59,718 INFO: Building Adadelta optimizer to use for [Base=Adadelta, learning_rate=0.7156065019391488, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.2804578072658813 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.674862276609254 ,beta_2=0.1306050159506792 ,learning_rate_power=-0.0779189487201083 ,initial_accumulator_value=0.6411621211754084 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:00:09,867 INFO: Building the Keras optimizer to use for [Base=Adam, learning_rate=0.16889263375470054, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.05543471833456903 ,rho=0.8001976339256608 ,epsilon=0.0007665695695270625 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.8557133106081526 ,learning_rate_power=-0.3459151236955742 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8133767765332951 ,beta=0.4991268647527092].
2023-08-03 19:00:09,867 INFO: Building Adam optimizer to use for [Base=Adam, learning_rate=0.16889263375470054, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.05543471833456903 ,rho=0.8001976339256608 ,epsilon=0.0007665695695270625 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.8557133106081526 ,learning_rate_power=-0.3459151236955742 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8133767765332951 ,beta=0.4991268647527092].
2023-08-03 19:00:34,604 INFO: Applying selection operators for generation 18.
2023-08-03 19:00:34,604 INFO: Applying genetic operators for generation 18.
2023-08-03 19:00:34,606 INFO: Evaluating fitness for for generation 18.
2023-08-03 19:00:34,606 INFO: Will evaluate fitness for 14 individuals.
2023-08-03 19:00:35,160 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:00:35,161 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:00:35,439 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:00:35,440 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:00:56,067 INFO: Building the Keras optimizer to use for [Base=Adam, learning_rate=0.5189947525178313, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0038752427560829333 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.9626742635062298 ,beta_2=0.20282258461820724 ,learning_rate_power=-0.8126722581341651 ,initial_accumulator_value=0.5346166118686845 ,l1_regularization_strength=0.4560705481487526 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6130884987026753 ,beta=0.438550501797466].
2023-08-03 19:00:56,067 INFO: Building Adam optimizer to use for [Base=Adam, learning_rate=0.5189947525178313, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0038752427560829333 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.9626742635062298 ,beta_2=0.20282258461820724 ,learning_rate_power=-0.8126722581341651 ,initial_accumulator_value=0.5346166118686845 ,l1_regularization_strength=0.4560705481487526 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6130884987026753 ,beta=0.438550501797466].
2023-08-03 19:00:57,168 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:00:57,168 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:01:18,483 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:01:18,484 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:01:22,603 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:01:22,603 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:01:41,354 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:01:41,354 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:01:43,640 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:01:43,640 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:02:03,307 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:02:03,307 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:02:04,938 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:02:04,938 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:02:24,079 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:02:24,080 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:02:25,814 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:02:25,814 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:02:44,872 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:02:44,873 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:03:07,058 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:03:07,058 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:03:28,373 INFO: Applying selection operators for generation 19.
2023-08-03 19:03:28,373 INFO: Applying genetic operators for generation 19.
2023-08-03 19:03:28,375 INFO: Evaluating fitness for for generation 19.
2023-08-03 19:03:28,375 INFO: Will evaluate fitness for 8 individuals.
2023-08-03 19:03:28,893 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:03:28,893 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:03:29,051 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:03:29,052 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:03:50,300 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:03:50,301 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:03:50,644 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:03:50,644 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:04:12,049 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:04:12,049 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:04:12,194 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:04:12,194 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:04:32,807 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.32258201139011367, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.8086190245699381 ,rho=0.3241975906572844 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.06524325218653781 ,initial_accumulator_value=0.08238267801665111 ,l1_regularization_strength=0.4424577056278355 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.47545766497015174 ,beta=0.3154390517428888].
2023-08-03 19:04:32,807 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.32258201139011367, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.8086190245699381 ,rho=0.3241975906572844 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.06524325218653781 ,initial_accumulator_value=0.08238267801665111 ,l1_regularization_strength=0.4424577056278355 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.47545766497015174 ,beta=0.3154390517428888].
2023-08-03 19:04:34,506 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.7186345319168113 ,nesterov=True ,amsgrad=False ,weight_decay=0.009870489509467641 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.15758162328882486 ,epsilon=0.0002569161861899012 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.6138562495130893 ,l1_regularization_strength=0.4607944255235147 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.20979810927612463 ,beta=0.7581215017451853].
2023-08-03 19:04:34,506 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.7186345319168113 ,nesterov=True ,amsgrad=False ,weight_decay=0.009870489509467641 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.15758162328882486 ,epsilon=0.0002569161861899012 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.6138562495130893 ,l1_regularization_strength=0.4607944255235147 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.20979810927612463 ,beta=0.7581215017451853].
2023-08-03 19:04:59,176 INFO: Applying selection operators for generation 20.
2023-08-03 19:04:59,176 INFO: Applying genetic operators for generation 20.
2023-08-03 19:04:59,179 INFO: Evaluating fitness for for generation 20.
2023-08-03 19:04:59,179 INFO: Will evaluate fitness for 14 individuals.
2023-08-03 19:04:59,702 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:04:59,702 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:04:59,750 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:04:59,750 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:05:20,478 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:05:20,478 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:05:21,252 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:05:21,252 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:05:41,574 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:05:41,574 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:05:42,046 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:05:42,047 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:06:02,444 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:06:02,444 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:06:03,564 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:06:03,564 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:06:23,302 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.7234318868876694, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.7304463822033743 ,rho=0.8001976339256608 ,epsilon=0.0005399080375227897 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.923916368577337 ,learning_rate_power=-0.8179605535431583 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.5874148593759063 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5631528786514939 ,beta=0.749486593420985].
2023-08-03 19:06:23,303 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.7234318868876694, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.7304463822033743 ,rho=0.8001976339256608 ,epsilon=0.0005399080375227897 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.923916368577337 ,learning_rate_power=-0.8179605535431583 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.5874148593759063 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5631528786514939 ,beta=0.749486593420985].
2023-08-03 19:06:24,631 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:06:24,632 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:06:45,646 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:06:45,646 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:06:47,045 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.5163368294071652, momentum=0.13267960646535903 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.4475884396997425 ,rho=0.811989677549098 ,epsilon=0.0005097639416736351 ,centered=False ,beta_1=0.1589119638476406 ,beta_2=0.20937971841560443 ,learning_rate_power=-0.44344947728680917 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.901197569719493 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.1494778779871504 ,beta=0.3154390517428888].
2023-08-03 19:06:47,045 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.5163368294071652, momentum=0.13267960646535903 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.4475884396997425 ,rho=0.811989677549098 ,epsilon=0.0005097639416736351 ,centered=False ,beta_1=0.1589119638476406 ,beta_2=0.20937971841560443 ,learning_rate_power=-0.44344947728680917 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.901197569719493 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.1494778779871504 ,beta=0.3154390517428888].
2023-08-03 19:07:07,117 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.5500360189279405, momentum=0.4791650250926669 ,nesterov=True ,amsgrad=True ,weight_decay=0.00912749581307512 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8467096733317598 ,initial_accumulator_value=0.4084765736091648 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.16677757572333185 ,beta=0.33708015064157615].
2023-08-03 19:07:07,118 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.5500360189279405, momentum=0.4791650250926669 ,nesterov=True ,amsgrad=True ,weight_decay=0.00912749581307512 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8467096733317598 ,initial_accumulator_value=0.4084765736091648 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.16677757572333185 ,beta=0.33708015064157615].
2023-08-03 19:07:42,359 INFO: Building the Keras optimizer to use for [Base=Adadelta, learning_rate=0.13745026075664546, momentum=0.10790814448354602 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.5946248511787119 ,rho=0.7835197350266823 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3170632954394256 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.15036236148231386 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.07537916777971121 ,beta=0.3154390517428888].
2023-08-03 19:07:42,360 INFO: Building Adadelta optimizer to use for [Base=Adadelta, learning_rate=0.13745026075664546, momentum=0.10790814448354602 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.5946248511787119 ,rho=0.7835197350266823 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3170632954394256 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.15036236148231386 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.07537916777971121 ,beta=0.3154390517428888].
2023-08-03 19:08:06,112 INFO: Applying selection operators for generation 21.
2023-08-03 19:08:06,112 INFO: Applying genetic operators for generation 21.
2023-08-03 19:08:06,114 INFO: Evaluating fitness for for generation 21.
2023-08-03 19:08:06,114 INFO: Will evaluate fitness for 8 individuals.
2023-08-03 19:08:06,625 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:08:06,625 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:08:06,793 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:08:06,794 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:08:27,727 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.2872239377707455 ,nesterov=True ,amsgrad=True ,weight_decay=0.005101309770256032 ,use_ema=True ,ema_momentum=0.08652565430477055 ,rho=0.8001976339256608 ,epsilon=0.0005485686427920074 ,centered=True ,beta_1=0.21190081008961492 ,beta_2=0.06295996806402526 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.020304412533480698 ,beta=0.3154390517428888].
2023-08-03 19:08:27,727 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.2872239377707455 ,nesterov=True ,amsgrad=True ,weight_decay=0.005101309770256032 ,use_ema=True ,ema_momentum=0.08652565430477055 ,rho=0.8001976339256608 ,epsilon=0.0005485686427920074 ,centered=True ,beta_1=0.21190081008961492 ,beta_2=0.06295996806402526 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.020304412533480698 ,beta=0.3154390517428888].
2023-08-03 19:08:28,035 INFO: Building the Keras optimizer to use for [Base=Adadelta, learning_rate=0.30225542492777235, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.003383616474146187 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.3936678097042441 ,initial_accumulator_value=0.6204733618984793 ,l1_regularization_strength=0.694081178737591 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.19872699451582254 ,beta=0.30269680987849856].
2023-08-03 19:08:28,035 INFO: Building Adadelta optimizer to use for [Base=Adadelta, learning_rate=0.30225542492777235, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.003383616474146187 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.3936678097042441 ,initial_accumulator_value=0.6204733618984793 ,l1_regularization_strength=0.694081178737591 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.19872699451582254 ,beta=0.30269680987849856].
2023-08-03 19:08:52,491 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:08:52,491 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:08:55,162 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:08:55,162 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:09:13,085 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:09:13,085 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:09:16,395 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.39512863408419785 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.867622601949378 ,rho=0.8001976339256608 ,epsilon=0.0007044237626406687 ,centered=False ,beta_1=0.07365515788246113 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.2742384613159723 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.37243168691254047 ,beta=0.3154390517428888].
2023-08-03 19:09:16,396 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.39512863408419785 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.867622601949378 ,rho=0.8001976339256608 ,epsilon=0.0007044237626406687 ,centered=False ,beta_1=0.07365515788246113 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.2742384613159723 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.37243168691254047 ,beta=0.3154390517428888].
2023-08-03 19:09:39,963 INFO: Applying selection operators for generation 22.
2023-08-03 19:09:39,964 INFO: Applying genetic operators for generation 22.
2023-08-03 19:09:39,967 INFO: Evaluating fitness for for generation 22.
2023-08-03 19:09:39,967 INFO: Will evaluate fitness for 17 individuals.
2023-08-03 19:09:40,490 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:09:40,490 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:09:40,494 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:09:40,494 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:10:01,326 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:10:01,326 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:10:01,463 INFO: Building the Keras optimizer to use for [Base=Adadelta, learning_rate=0.31128489053785746, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.516120902282105 ,rho=0.16876569755695026 ,epsilon=0.0007950293164797947 ,centered=False ,beta_1=0.5585998592207969 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.443260506271852 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4979122685320765 ,beta=0.3154390517428888].
2023-08-03 19:10:01,464 INFO: Building Adadelta optimizer to use for [Base=Adadelta, learning_rate=0.31128489053785746, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.516120902282105 ,rho=0.16876569755695026 ,epsilon=0.0007950293164797947 ,centered=False ,beta_1=0.5585998592207969 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.443260506271852 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4979122685320765 ,beta=0.3154390517428888].
2023-08-03 19:10:22,073 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:10:22,073 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:10:26,435 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:10:26,435 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:10:43,015 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:10:43,015 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:10:48,001 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:10:48,001 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:11:04,770 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:11:04,770 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:11:09,603 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:11:09,604 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:11:25,603 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:11:25,603 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:11:30,507 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.0729444459850841, momentum=0.4744903670897863 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.20461174611780353 ,rho=0.4291651681673315 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.9414036725267171 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.25620767734762095 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.8275398929976268 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.3620935811926731 ,beta=0.8056808585537235].
2023-08-03 19:11:30,508 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.0729444459850841, momentum=0.4744903670897863 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.20461174611780353 ,rho=0.4291651681673315 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.9414036725267171 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.25620767734762095 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.8275398929976268 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.3620935811926731 ,beta=0.8056808585537235].
2023-08-03 19:11:46,717 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.284584015192335, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.00502621120544845 ,use_ema=False ,ema_momentum=0.2726134082069199 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.7353718811519353 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.5858423597450386 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.2009489913245902 ,beta=0.3154390517428888].
2023-08-03 19:11:46,717 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.284584015192335, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.00502621120544845 ,use_ema=False ,ema_momentum=0.2726134082069199 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.7353718811519353 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.5858423597450386 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.2009489913245902 ,beta=0.3154390517428888].
2023-08-03 19:11:56,008 INFO: Building the Keras optimizer to use for [Base=Adam, learning_rate=0.15053605549993632, momentum=0.9364764485005596 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0009737379618622226 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.6389295852312619 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.13962474709289563 ,beta=0.30910286998339975].
2023-08-03 19:11:56,009 INFO: Building Adam optimizer to use for [Base=Adam, learning_rate=0.15053605549993632, momentum=0.9364764485005596 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0009737379618622226 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.6389295852312619 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.13962474709289563 ,beta=0.30910286998339975].
2023-08-03 19:12:07,395 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:12:07,396 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:12:21,512 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.4948797277810164, momentum=0.7034286467623603 ,nesterov=False ,amsgrad=False ,weight_decay=0.0015128455885936567 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.00027214718812903636 ,centered=False ,beta_1=0.6806915145590715 ,beta_2=0.3418799999645016 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.11714203843986581 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.31578139697637453 ,beta=0.938934529394367].
2023-08-03 19:12:21,512 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.4948797277810164, momentum=0.7034286467623603 ,nesterov=False ,amsgrad=False ,weight_decay=0.0015128455885936567 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.00027214718812903636 ,centered=False ,beta_1=0.6806915145590715 ,beta_2=0.3418799999645016 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.11714203843986581 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.31578139697637453 ,beta=0.938934529394367].
2023-08-03 19:12:28,679 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:12:28,679 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:12:49,323 INFO: Applying selection operators for generation 23.
2023-08-03 19:12:49,323 INFO: Applying genetic operators for generation 23.
2023-08-03 19:12:49,326 INFO: Evaluating fitness for for generation 23.
2023-08-03 19:12:49,326 INFO: Will evaluate fitness for 16 individuals.
2023-08-03 19:12:50,049 INFO: Building the Keras optimizer to use for [Base=Adam, learning_rate=0.2770099224355195, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.7100442646059437 ,rho=0.378282804713299 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.46246614491053095 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9064819667682161 ,beta=0.0804710505386651].
2023-08-03 19:12:50,049 INFO: Building Adam optimizer to use for [Base=Adam, learning_rate=0.2770099224355195, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.7100442646059437 ,rho=0.378282804713299 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.46246614491053095 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9064819667682161 ,beta=0.0804710505386651].
2023-08-03 19:12:50,056 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:12:50,057 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:13:11,151 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9102195954217056, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.007154014073242913 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.5308981378225864 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7635419769625513 ,beta=0.3154390517428888].
2023-08-03 19:13:11,151 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9102195954217056, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.007154014073242913 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.5308981378225864 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7635419769625513 ,beta=0.3154390517428888].
2023-08-03 19:13:17,214 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:13:17,215 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:13:33,595 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:13:33,595 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:13:37,935 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:13:37,935 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:13:54,873 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:13:54,873 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:13:59,432 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:13:59,432 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:14:16,508 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:14:16,509 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:14:20,409 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:14:20,409 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:14:37,768 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.6183979509333748, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0059459102175133645 ,use_ema=True ,ema_momentum=0.8796388322882909 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.22993657888619323 ,beta_2=0.775743292022702 ,learning_rate_power=-0.537075626437052 ,initial_accumulator_value=0.5318532607720425 ,l1_regularization_strength=0.5865561667080025 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8188121610329571 ,beta=0.3154390517428888].
2023-08-03 19:14:37,769 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.6183979509333748, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0059459102175133645 ,use_ema=True ,ema_momentum=0.8796388322882909 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.22993657888619323 ,beta_2=0.775743292022702 ,learning_rate_power=-0.537075626437052 ,initial_accumulator_value=0.5318532607720425 ,l1_regularization_strength=0.5865561667080025 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8188121610329571 ,beta=0.3154390517428888].
2023-08-03 19:14:41,317 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:14:41,318 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:15:00,920 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:15:00,920 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:15:02,444 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:15:02,445 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:15:22,230 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:15:22,230 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:15:24,074 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:15:24,074 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:15:44,401 INFO: Applying selection operators for generation 24.
2023-08-03 19:15:44,402 INFO: Applying genetic operators for generation 24.
2023-08-03 19:15:44,405 INFO: Evaluating fitness for for generation 24.
2023-08-03 19:15:44,405 INFO: Will evaluate fitness for 12 individuals.
2023-08-03 19:15:44,927 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.0729444459850841, momentum=0.726657146172601 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.24301565150272242 ,rho=0.8001976339256608 ,epsilon=0.0004559615978725784 ,centered=False ,beta_1=0.6826705575518021 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.42320470079590045 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5458773231156798 ,beta=0.977179009659076].
2023-08-03 19:15:44,928 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.0729444459850841, momentum=0.726657146172601 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.24301565150272242 ,rho=0.8001976339256608 ,epsilon=0.0004559615978725784 ,centered=False ,beta_1=0.6826705575518021 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.42320470079590045 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5458773231156798 ,beta=0.977179009659076].
2023-08-03 19:15:44,928 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0036883900272681736, momentum=0.7659486395685446 ,nesterov=False ,amsgrad=False ,weight_decay=0.00839221955726046 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8466837113600583 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.007311048730649361 ,learning_rate_power=-0.931290678643833 ,initial_accumulator_value=0.9352040714943688 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.05450718484263284 ,beta=0.3154390517428888].
2023-08-03 19:15:44,928 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0036883900272681736, momentum=0.7659486395685446 ,nesterov=False ,amsgrad=False ,weight_decay=0.00839221955726046 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8466837113600583 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.007311048730649361 ,learning_rate_power=-0.931290678643833 ,initial_accumulator_value=0.9352040714943688 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.05450718484263284 ,beta=0.3154390517428888].
2023-08-03 19:16:06,611 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:16:06,612 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:16:09,815 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.433870242845771, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.008604608254828193 ,use_ema=True ,ema_momentum=0.7530251987185779 ,rho=0.8001976339256608 ,epsilon=0.0007358787480989976 ,centered=False ,beta_1=0.051540052314896334 ,beta_2=0.5729907224765495 ,learning_rate_power=-0.6897067303702511 ,initial_accumulator_value=0.9481272712838184 ,l1_regularization_strength=0.37130235847846615 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4875816517299685 ,beta=0.3154390517428888].
2023-08-03 19:16:09,816 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.433870242845771, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.008604608254828193 ,use_ema=True ,ema_momentum=0.7530251987185779 ,rho=0.8001976339256608 ,epsilon=0.0007358787480989976 ,centered=False ,beta_1=0.051540052314896334 ,beta_2=0.5729907224765495 ,learning_rate_power=-0.6897067303702511 ,initial_accumulator_value=0.9481272712838184 ,l1_regularization_strength=0.37130235847846615 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4875816517299685 ,beta=0.3154390517428888].
2023-08-03 19:16:27,611 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:16:27,611 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:16:36,575 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:16:36,575 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:16:48,955 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:16:48,955 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:16:57,971 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:16:57,972 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:17:09,739 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:17:09,739 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:17:19,217 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:17:19,218 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:17:30,574 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.08159155041050514 ,rho=0.8001976339256608 ,epsilon=3.358161932544555e-05 ,centered=False ,beta_1=0.5757771628350592 ,beta_2=0.8382256900217798 ,learning_rate_power=-0.572414316071578 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.3727080385115309 ,beta=0.3154390517428888].
2023-08-03 19:17:30,574 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.08159155041050514 ,rho=0.8001976339256608 ,epsilon=3.358161932544555e-05 ,centered=False ,beta_1=0.5757771628350592 ,beta_2=0.8382256900217798 ,learning_rate_power=-0.572414316071578 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.3727080385115309 ,beta=0.3154390517428888].
2023-08-03 19:17:39,927 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:17:39,927 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:18:01,033 INFO: Applying selection operators for generation 25.
2023-08-03 19:18:01,033 INFO: Applying genetic operators for generation 25.
2023-08-03 19:18:01,035 INFO: Evaluating fitness for for generation 25.
2023-08-03 19:18:01,035 INFO: Will evaluate fitness for 13 individuals.
2023-08-03 19:18:01,612 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:18:01,613 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:18:01,816 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:18:01,817 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:18:22,583 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:18:22,583 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:18:23,240 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:18:23,241 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:18:43,869 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:18:43,869 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:18:43,981 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:18:43,982 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:19:04,690 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:19:04,690 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:19:04,724 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.7982391097124237, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9937123016115079 ,rho=0.3864667336208757 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.19889035029617863 ,beta_2=0.9975317463478541 ,learning_rate_power=-0.3774702376996456 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.2746676679811092 ,beta=0.3154390517428888].
2023-08-03 19:19:04,725 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.7982391097124237, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9937123016115079 ,rho=0.3864667336208757 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.19889035029617863 ,beta_2=0.9975317463478541 ,learning_rate_power=-0.3774702376996456 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.2746676679811092 ,beta=0.3154390517428888].
2023-08-03 19:19:25,264 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.9395507822504998 ,epsilon=0.0003652914033894122 ,centered=False ,beta_1=0.41746661408031116 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.2781705857486213 ,initial_accumulator_value=0.6075318341876615 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.045984795149189295 ,beta=0.7237089748760089].
2023-08-03 19:19:25,264 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.9395507822504998 ,epsilon=0.0003652914033894122 ,centered=False ,beta_1=0.41746661408031116 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.2781705857486213 ,initial_accumulator_value=0.6075318341876615 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.045984795149189295 ,beta=0.7237089748760089].
2023-08-03 19:19:27,121 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:19:27,121 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:19:49,135 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:19:49,135 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:19:50,050 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:19:50,051 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:20:10,764 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:20:10,764 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:20:31,261 INFO: Applying selection operators for generation 26.
2023-08-03 19:20:31,261 INFO: Applying genetic operators for generation 26.
2023-08-03 19:20:31,262 INFO: Evaluating fitness for for generation 26.
2023-08-03 19:20:31,262 INFO: Will evaluate fitness for 8 individuals.
2023-08-03 19:20:31,783 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:20:31,783 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:20:31,933 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:20:31,934 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:20:53,019 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:20:53,019 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:20:53,787 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:20:53,788 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:21:13,786 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:21:13,786 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:21:14,847 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:21:14,847 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:21:34,571 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.5303722934440175 ,epsilon=0.0008305955067634112 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.4198622580398558 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.8799410840610125 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9544189148184623 ,beta=0.3154390517428888].
2023-08-03 19:21:34,572 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.5303722934440175 ,epsilon=0.0008305955067634112 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.4198622580398558 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.8799410840610125 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9544189148184623 ,beta=0.3154390517428888].
2023-08-03 19:21:36,132 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:21:36,133 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:21:58,518 INFO: Applying selection operators for generation 27.
2023-08-03 19:21:58,518 INFO: Applying genetic operators for generation 27.
2023-08-03 19:21:58,521 INFO: Evaluating fitness for for generation 27.
2023-08-03 19:21:58,521 INFO: Will evaluate fitness for 13 individuals.
2023-08-03 19:21:59,065 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:21:59,066 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:21:59,070 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:21:59,070 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:22:20,007 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:22:20,007 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:22:20,191 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:22:20,191 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:22:40,726 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:22:40,726 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:22:41,961 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.007379023254942542 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0009523020087939576 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.21554752407388966 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.3646920715682356 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9815329333699417 ,beta=0.3154390517428888].
2023-08-03 19:22:41,961 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.007379023254942542 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0009523020087939576 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.21554752407388966 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.3646920715682356 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9815329333699417 ,beta=0.3154390517428888].
2023-08-03 19:23:01,336 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:23:01,337 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:23:07,570 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:23:07,571 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:23:22,763 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.1991399310517652, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.3469571036101907 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.21826786209132543 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.550909897813798 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9785997072528941 ,beta=0.9443167804802749].
2023-08-03 19:23:22,763 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.1991399310517652, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.3469571036101907 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.21826786209132543 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.550909897813798 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9785997072528941 ,beta=0.9443167804802749].
2023-08-03 19:23:28,710 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:23:28,710 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:23:42,721 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:23:42,722 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:23:49,700 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:23:49,700 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:24:03,453 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.3194964661316936, momentum=0.3438091561153156 ,nesterov=True ,amsgrad=False ,weight_decay=0.004109365728357984 ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.6797293177674021 ,beta_2=0.7012299585106047 ,learning_rate_power=-0.29540676921356435 ,initial_accumulator_value=0.41917732250464934 ,l1_regularization_strength=0.8612193629940723 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6740430459455037 ,beta=0.8577238081846871].
2023-08-03 19:24:03,454 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.3194964661316936, momentum=0.3438091561153156 ,nesterov=True ,amsgrad=False ,weight_decay=0.004109365728357984 ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.6797293177674021 ,beta_2=0.7012299585106047 ,learning_rate_power=-0.29540676921356435 ,initial_accumulator_value=0.41917732250464934 ,l1_regularization_strength=0.8612193629940723 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6740430459455037 ,beta=0.8577238081846871].
2023-08-03 19:24:23,368 INFO: Applying selection operators for generation 28.
2023-08-03 19:24:23,369 INFO: Applying genetic operators for generation 28.
2023-08-03 19:24:23,373 INFO: Evaluating fitness for for generation 28.
2023-08-03 19:24:23,373 INFO: Will evaluate fitness for 16 individuals.
2023-08-03 19:24:23,883 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:24:23,883 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:24:23,892 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.030933840678619595, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.5397673660250626 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.2222248650592913 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.559643838181225 ,beta=0.3154390517428888].
2023-08-03 19:24:23,892 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.030933840678619595, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.5397673660250626 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.2222248650592913 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.559643838181225 ,beta=0.3154390517428888].
2023-08-03 19:24:45,231 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:24:45,231 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:24:49,594 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:24:49,594 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:25:06,248 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.22157191261400166, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.5338389652760354 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.4110955002876914 ,beta_2=0.11040326255970578 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.023997104099967692 ,beta=0.3154390517428888].
2023-08-03 19:25:06,248 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.22157191261400166, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.5338389652760354 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.4110955002876914 ,beta_2=0.11040326255970578 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.023997104099967692 ,beta=0.3154390517428888].
2023-08-03 19:25:10,987 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:25:10,987 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:25:26,770 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.7043228086320248 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.3721365396090155 ,beta=0.3154390517428888].
2023-08-03 19:25:26,770 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.7043228086320248 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.3721365396090155 ,beta=0.3154390517428888].
2023-08-03 19:25:33,083 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:25:33,084 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:25:48,348 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:25:48,348 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:25:54,496 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.5016156396095129, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07161456545133693 ,rho=0.806043427466507 ,epsilon=0.0007109014942566255 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.7773318567043078 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4135702493371507 ,beta=0.3154390517428888].
2023-08-03 19:25:54,497 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.5016156396095129, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.07161456545133693 ,rho=0.806043427466507 ,epsilon=0.0007109014942566255 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.7773318567043078 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4135702493371507 ,beta=0.3154390517428888].
2023-08-03 19:26:10,492 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:26:10,492 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:26:19,682 INFO: Building the Keras optimizer to use for [Base=Adam, learning_rate=0.23098179559766274, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.17216708143435822 ,rho=0.4556398156946302 ,epsilon=0.0009042431040647069 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7924023683328795 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.21457418470856449 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.2345645654391938].
2023-08-03 19:26:19,683 INFO: Building Adam optimizer to use for [Base=Adam, learning_rate=0.23098179559766274, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.17216708143435822 ,rho=0.4556398156946302 ,epsilon=0.0009042431040647069 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7924023683328795 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.21457418470856449 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.2345645654391938].
2023-08-03 19:26:31,860 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:26:31,861 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:26:46,432 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.9500967342773922, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.6715405515808414 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.036043531623154434 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.4205995432323575 ,l1_regularization_strength=0.24087036090594904 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.30745681292731797 ,beta=0.4503975926859062].
2023-08-03 19:26:46,433 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.9500967342773922, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.6715405515808414 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.036043531623154434 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.4205995432323575 ,l1_regularization_strength=0.24087036090594904 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.30745681292731797 ,beta=0.4503975926859062].
2023-08-03 19:26:52,800 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:26:52,800 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:27:12,337 INFO: Building the Keras optimizer to use for [Base=Adadelta, learning_rate=0.632767127073704, momentum=0.3521974767057935 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.2907925120656214 ,rho=0.9276240925357087 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4365617971306204 ,beta=0.41579647886307936].
2023-08-03 19:27:12,337 INFO: Building Adadelta optimizer to use for [Base=Adadelta, learning_rate=0.632767127073704, momentum=0.3521974767057935 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.2907925120656214 ,rho=0.9276240925357087 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4365617971306204 ,beta=0.41579647886307936].
2023-08-03 19:27:37,099 INFO: Applying selection operators for generation 29.
2023-08-03 19:27:37,099 INFO: Applying genetic operators for generation 29.
2023-08-03 19:27:37,102 INFO: Evaluating fitness for for generation 29.
2023-08-03 19:27:37,102 INFO: Will evaluate fitness for 10 individuals.
2023-08-03 19:27:37,619 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.747992259525063, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.26794411318804345 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.281686901817899 ,initial_accumulator_value=0.4864972219759087 ,l1_regularization_strength=0.6970309920689434 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6407406253503362 ,beta=0.3154390517428888].
2023-08-03 19:27:37,619 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.747992259525063, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.26794411318804345 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.281686901817899 ,initial_accumulator_value=0.4864972219759087 ,l1_regularization_strength=0.6970309920689434 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6407406253503362 ,beta=0.3154390517428888].
2023-08-03 19:27:37,623 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.6229979414969099, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0006793284511413257 ,centered=True ,beta_1=0.760791074303007 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.46089928287991666 ,beta=0.20389467505198877].
2023-08-03 19:27:37,623 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.6229979414969099, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0006793284511413257 ,centered=True ,beta_1=0.760791074303007 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.46089928287991666 ,beta=0.20389467505198877].
2023-08-03 19:27:58,626 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:27:58,626 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:28:02,853 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:28:02,854 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:28:20,296 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:28:20,297 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:28:24,177 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.5707246225369994, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.2925292630421992 ,rho=0.8001976339256608 ,epsilon=0.0008292191489400918 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.5605536644421367 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.31199915560742864 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.04518425629381495 ,beta=0.29123938912535174].
2023-08-03 19:28:24,177 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.5707246225369994, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.2925292630421992 ,rho=0.8001976339256608 ,epsilon=0.0008292191489400918 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.5605536644421367 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.31199915560742864 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.04518425629381495 ,beta=0.29123938912535174].
2023-08-03 19:28:41,236 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.08678101450323028, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.27231645199795174 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.5449426032243474 ,initial_accumulator_value=0.518048433478197 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5761376835944586 ,beta=0.3154390517428888].
2023-08-03 19:28:41,236 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.08678101450323028, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.27231645199795174 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.5449426032243474 ,initial_accumulator_value=0.518048433478197 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5761376835944586 ,beta=0.3154390517428888].
2023-08-03 19:28:50,775 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:28:50,775 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:29:02,187 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:29:02,187 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:29:22,892 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.4180013863576554, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.35499738275921056 ,rho=0.048680108433041935 ,epsilon=0.0006071194819583162 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.4226154533124097 ,learning_rate_power=-0.7043228086320248 ,initial_accumulator_value=0.3847227011925872 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.22300348472951093 ,beta=0.3154390517428888].
2023-08-03 19:29:22,892 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.4180013863576554, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.35499738275921056 ,rho=0.048680108433041935 ,epsilon=0.0006071194819583162 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.4226154533124097 ,learning_rate_power=-0.7043228086320248 ,initial_accumulator_value=0.3847227011925872 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.22300348472951093 ,beta=0.3154390517428888].
2023-08-03 19:29:47,266 INFO: Applying selection operators for generation 30.
2023-08-03 19:29:47,266 INFO: Applying genetic operators for generation 30.
2023-08-03 19:29:47,269 INFO: Evaluating fitness for for generation 30.
2023-08-03 19:29:47,269 INFO: Will evaluate fitness for 14 individuals.
2023-08-03 19:29:47,835 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.2939531279360095, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.0063207025426125895 ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0002542613256365168 ,centered=False ,beta_1=0.5826294447636589 ,beta_2=0.2521311489885508 ,learning_rate_power=-0.8944386784735175 ,initial_accumulator_value=0.41608858033846796 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9908769866433952 ,beta=0.4547346954929634].
2023-08-03 19:29:47,835 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.2939531279360095, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.0063207025426125895 ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0002542613256365168 ,centered=False ,beta_1=0.5826294447636589 ,beta_2=0.2521311489885508 ,learning_rate_power=-0.8944386784735175 ,initial_accumulator_value=0.41608858033846796 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9908769866433952 ,beta=0.4547346954929634].
2023-08-03 19:29:47,889 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.638646415079422, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.4014160913075807 ,rho=0.42990092173831407 ,epsilon=0.0005627877827450785 ,centered=False ,beta_1=0.08220661700580156 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.6133671181643163 ,l1_regularization_strength=0.6337626560607618 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:29:47,890 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.638646415079422, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.4014160913075807 ,rho=0.42990092173831407 ,epsilon=0.0005627877827450785 ,centered=False ,beta_1=0.08220661700580156 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.6133671181643163 ,l1_regularization_strength=0.6337626560607618 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:30:12,206 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:30:12,206 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:30:12,435 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.9475514225149501, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.10062169627717676 ,epsilon=0.0007774091640806355 ,centered=False ,beta_1=0.8968554856772987 ,beta_2=0.025663498822600217 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5952477326738846 ,beta=0.8143430236650993].
2023-08-03 19:30:12,435 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.9475514225149501, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.10062169627717676 ,epsilon=0.0007774091640806355 ,centered=False ,beta_1=0.8968554856772987 ,beta_2=0.025663498822600217 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5952477326738846 ,beta=0.8143430236650993].
2023-08-03 19:30:32,450 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:30:32,450 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:30:32,967 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:30:32,967 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:30:53,919 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.6518069308038303, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.2252714431904228 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.7741341015101303 ,initial_accumulator_value=0.9271900508348205 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5214782074132207 ,beta=0.3154390517428888].
2023-08-03 19:30:53,919 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.6518069308038303, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.2252714431904228 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.7741341015101303 ,initial_accumulator_value=0.9271900508348205 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5214782074132207 ,beta=0.3154390517428888].
2023-08-03 19:30:54,244 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:30:54,245 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:31:14,683 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:31:14,683 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:31:15,112 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:31:15,112 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:31:35,373 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:31:35,373 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:31:35,768 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:31:35,768 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:31:56,977 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:31:56,978 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:32:17,540 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:32:17,540 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:32:38,029 INFO: Applying selection operators for generation 31.
2023-08-03 19:32:38,029 INFO: Applying genetic operators for generation 31.
2023-08-03 19:32:38,033 INFO: Evaluating fitness for for generation 31.
2023-08-03 19:32:38,033 INFO: Will evaluate fitness for 17 individuals.
2023-08-03 19:32:38,541 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:32:38,541 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:32:38,551 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:32:38,551 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:32:59,401 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:32:59,402 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:32:59,454 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:32:59,454 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:33:20,214 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:33:20,214 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:33:21,761 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.11982573641956917, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.790955969633326 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.20647492758276487 ,learning_rate_power=-0.8634787572632148 ,initial_accumulator_value=0.008573728098451583 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6904074730316837 ,beta=0.3154390517428888].
2023-08-03 19:33:21,761 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.11982573641956917, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.790955969633326 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.20647492758276487 ,learning_rate_power=-0.8634787572632148 ,initial_accumulator_value=0.008573728098451583 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6904074730316837 ,beta=0.3154390517428888].
2023-08-03 19:33:41,024 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:33:41,025 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:33:42,868 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.526602935884719, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0027534575386330157 ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.345699726532397 ,l1_regularization_strength=0.6530581135017337 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5182104438434335 ,beta=0.3154390517428888].
2023-08-03 19:33:42,869 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.526602935884719, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.0027534575386330157 ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.345699726532397 ,l1_regularization_strength=0.6530581135017337 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5182104438434335 ,beta=0.3154390517428888].
2023-08-03 19:34:02,367 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:34:02,368 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:34:07,488 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:34:07,489 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:34:23,160 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:34:23,160 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:34:28,738 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.08178040234983852, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.010198202526723654 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.24234642584833932 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.7475157192738062 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7195990637767957 ,beta=0.40536638871428143].
2023-08-03 19:34:28,738 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.08178040234983852, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.010198202526723654 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.24234642584833932 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.7475157192738062 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7195990637767957 ,beta=0.40536638871428143].
2023-08-03 19:34:44,335 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:34:44,336 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:34:50,981 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.3672249433065684, momentum=0.2404077417958146 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.7922468187907331 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.8960135249519232 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.46061403733201645 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.23563047980508722].
2023-08-03 19:34:50,982 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.3672249433065684, momentum=0.2404077417958146 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.7922468187907331 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.8960135249519232 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.46061403733201645 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.23563047980508722].
2023-08-03 19:35:05,390 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:35:05,391 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:35:17,163 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.09367006063371486, momentum=0.72154498732575 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.8818202216422821 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.6716823638455722 ,initial_accumulator_value=0.9526075320021029 ,l1_regularization_strength=0.9869277976765867 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.885514136749765].
2023-08-03 19:35:17,163 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.09367006063371486, momentum=0.72154498732575 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.8818202216422821 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.6716823638455722 ,initial_accumulator_value=0.9526075320021029 ,l1_regularization_strength=0.9869277976765867 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.885514136749765].
2023-08-03 19:35:27,688 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.0729444459850841, momentum=0.45244343850529356 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8637856287188946 ,rho=0.4865673081427724 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.9261724075436847 ,beta_2=0.0587864691769715 ,learning_rate_power=-0.34658618252823403 ,initial_accumulator_value=0.7524880491388458 ,l1_regularization_strength=0.38520324920535987 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.782523966623468 ,beta=0.24243992804675363].
2023-08-03 19:35:27,688 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.0729444459850841, momentum=0.45244343850529356 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.8637856287188946 ,rho=0.4865673081427724 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.9261724075436847 ,beta_2=0.0587864691769715 ,learning_rate_power=-0.34658618252823403 ,initial_accumulator_value=0.7524880491388458 ,l1_regularization_strength=0.38520324920535987 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.782523966623468 ,beta=0.24243992804675363].
2023-08-03 19:36:04,117 INFO: Applying selection operators for generation 32.
2023-08-03 19:36:04,118 INFO: Applying genetic operators for generation 32.
2023-08-03 19:36:04,121 INFO: Evaluating fitness for for generation 32.
2023-08-03 19:36:04,121 INFO: Will evaluate fitness for 12 individuals.
2023-08-03 19:36:04,642 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:36:04,642 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:36:04,645 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.31096558093384297, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0006233963215392142 ,use_ema=True ,ema_momentum=0.42715347960570316 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7085367403388123 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.4056712147220669 ,l1_regularization_strength=0.3361662507508201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.015604997541395327 ,beta=0.3154390517428888].
2023-08-03 19:36:04,645 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.31096558093384297, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0006233963215392142 ,use_ema=True ,ema_momentum=0.42715347960570316 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7085367403388123 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.4056712147220669 ,l1_regularization_strength=0.3361662507508201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.015604997541395327 ,beta=0.3154390517428888].
2023-08-03 19:36:26,789 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:36:26,789 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:36:26,834 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.338767002552149, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.4633158306148326 ,learning_rate_power=-0.3888909897819436 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5421417884994019 ,beta=0.3154390517428888].
2023-08-03 19:36:26,835 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.338767002552149, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.4633158306148326 ,learning_rate_power=-0.3888909897819436 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5421417884994019 ,beta=0.3154390517428888].
2023-08-03 19:36:47,390 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:36:47,390 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:36:48,842 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.72154498732575 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.8818202216422821 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.6716823638455722 ,initial_accumulator_value=0.9526075320021029 ,l1_regularization_strength=0.9869277976765867 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:36:48,842 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.72154498732575 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.8818202216422821 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.6716823638455722 ,initial_accumulator_value=0.9526075320021029 ,l1_regularization_strength=0.9869277976765867 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:37:09,159 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.09367006063371486, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.885514136749765].
2023-08-03 19:37:09,159 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.09367006063371486, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.885514136749765].
2023-08-03 19:37:10,931 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:37:10,931 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:37:29,882 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:37:29,882 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:37:32,646 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.13360046466576403, momentum=0.4584238238220413 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.2853502527944791 ,learning_rate_power=-0.1762375219112463 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.4362424442812868 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.8136633372423328].
2023-08-03 19:37:32,646 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.13360046466576403, momentum=0.4584238238220413 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.2853502527944791 ,learning_rate_power=-0.1762375219112463 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.4362424442812868 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.8136633372423328].
2023-08-03 19:37:51,069 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.16937036502019864, momentum=0.15089375569801533 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.2490116279950112 ,rho=0.48762439141763514 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.5034371042484009 ,learning_rate_power=-0.7125378350131019 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.12218771762561487 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.7936553803339603].
2023-08-03 19:37:51,069 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.16937036502019864, momentum=0.15089375569801533 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.2490116279950112 ,rho=0.48762439141763514 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.5034371042484009 ,learning_rate_power=-0.7125378350131019 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.12218771762561487 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.7936553803339603].
2023-08-03 19:37:54,467 INFO: Building the Keras optimizer to use for [Base=Ftrl, learning_rate=0.846653567587884, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.3831223404773775 ,initial_accumulator_value=0.6717663498813932 ,l1_regularization_strength=0.6256109383128585 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.852882854092062 ,beta=0.015247663764395458].
2023-08-03 19:37:54,467 INFO: Building Ftrl optimizer to use for [Base=Ftrl, learning_rate=0.846653567587884, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.3831223404773775 ,initial_accumulator_value=0.6717663498813932 ,l1_regularization_strength=0.6256109383128585 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.852882854092062 ,beta=0.015247663764395458].
2023-08-03 19:38:21,311 INFO: Applying selection operators for generation 33.
2023-08-03 19:38:21,312 INFO: Applying genetic operators for generation 33.
2023-08-03 19:38:21,316 INFO: Evaluating fitness for for generation 33.
2023-08-03 19:38:21,317 INFO: Will evaluate fitness for 11 individuals.
2023-08-03 19:38:22,875 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:38:22,876 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:38:22,885 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.5218358556304744, momentum=0.8685861420386338 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6370527607540658 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.8987621020374619 ,beta_2=0.5505957402121863 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.31168061683243165 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8093531837905013 ,beta=0.3154390517428888].
2023-08-03 19:38:22,886 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.5218358556304744, momentum=0.8685861420386338 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6370527607540658 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.8987621020374619 ,beta_2=0.5505957402121863 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.31168061683243165 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8093531837905013 ,beta=0.3154390517428888].
2023-08-03 19:38:44,272 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:38:44,272 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:38:44,571 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.7272698749313842 ,nesterov=True ,amsgrad=False ,weight_decay=0.005634848378574174 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0004471097121156106 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.04966304783196518 ,l1_regularization_strength=0.042523556611569036 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.37914506391080294 ,beta=0.3154390517428888].
2023-08-03 19:38:44,571 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.7272698749313842 ,nesterov=True ,amsgrad=False ,weight_decay=0.005634848378574174 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0004471097121156106 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.04966304783196518 ,l1_regularization_strength=0.042523556611569036 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.37914506391080294 ,beta=0.3154390517428888].
2023-08-03 19:39:05,857 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:39:05,858 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:39:10,758 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=0.002476519207715006 ,use_ema=True ,ema_momentum=0.7475992211657387 ,rho=0.8001976339256608 ,epsilon=0.0005966861797639994 ,centered=False ,beta_1=0.6325641828480114 ,beta_2=0.6713146117523268 ,learning_rate_power=-0.3089443477561641 ,initial_accumulator_value=0.054521807678985845 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8730040734798437 ,beta=0.3154390517428888].
2023-08-03 19:39:10,759 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=0.002476519207715006 ,use_ema=True ,ema_momentum=0.7475992211657387 ,rho=0.8001976339256608 ,epsilon=0.0005966861797639994 ,centered=False ,beta_1=0.6325641828480114 ,beta_2=0.6713146117523268 ,learning_rate_power=-0.3089443477561641 ,initial_accumulator_value=0.054521807678985845 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8730040734798437 ,beta=0.3154390517428888].
2023-08-03 19:39:26,747 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.5898449633113867 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.7708482683019233 ,rho=0.21974131094715765 ,epsilon=0.00022723984246323628 ,centered=True ,beta_1=0.9917479226411444 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5063389461534354 ,beta=0.6685844045969987].
2023-08-03 19:39:26,747 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0.5898449633113867 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.7708482683019233 ,rho=0.21974131094715765 ,epsilon=0.00022723984246323628 ,centered=True ,beta_1=0.9917479226411444 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5063389461534354 ,beta=0.6685844045969987].
2023-08-03 19:39:37,428 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:39:37,428 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:39:51,559 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.6584305141281855, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.2427109454366133 ,rho=0.5319470217061874 ,epsilon=0.0006683559847366176 ,centered=False ,beta_1=0.40814575751137594 ,beta_2=0.6724592972031325 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.51897493805826 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.3598216558372812 ,beta=0.0355707098650504].
2023-08-03 19:39:51,560 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.6584305141281855, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.2427109454366133 ,rho=0.5319470217061874 ,epsilon=0.0006683559847366176 ,centered=False ,beta_1=0.40814575751137594 ,beta_2=0.6724592972031325 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.51897493805826 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.3598216558372812 ,beta=0.0355707098650504].
2023-08-03 19:39:58,738 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.3751888416704222 ,rho=0.2634451849564323 ,epsilon=0.0007463981429811796 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.26069762871117763 ,initial_accumulator_value=0.8096547738326692 ,l1_regularization_strength=0.061851592852922854 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.17511127247207492 ,beta=0.3154390517428888].
2023-08-03 19:39:58,738 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.3751888416704222 ,rho=0.2634451849564323 ,epsilon=0.0007463981429811796 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.26069762871117763 ,initial_accumulator_value=0.8096547738326692 ,l1_regularization_strength=0.061851592852922854 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.17511127247207492 ,beta=0.3154390517428888].
2023-08-03 19:40:16,344 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.004571476615300327 ,use_ema=False ,ema_momentum=0.9383067984748613 ,rho=0.8001976339256608 ,epsilon=0.0005302433664730957 ,centered=True ,beta_1=0.6766659977580023 ,beta_2=0.17665487468880692 ,learning_rate_power=-0.9617437612188274 ,initial_accumulator_value=0.18929998672631643 ,l1_regularization_strength=0.23309142455773468 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.2926352032476951 ,beta=0.3154390517428888].
2023-08-03 19:40:16,345 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.004571476615300327 ,use_ema=False ,ema_momentum=0.9383067984748613 ,rho=0.8001976339256608 ,epsilon=0.0005302433664730957 ,centered=True ,beta_1=0.6766659977580023 ,beta_2=0.17665487468880692 ,learning_rate_power=-0.9617437612188274 ,initial_accumulator_value=0.18929998672631643 ,l1_regularization_strength=0.23309142455773468 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.2926352032476951 ,beta=0.3154390517428888].
2023-08-03 19:40:40,514 INFO: Applying selection operators for generation 34.
2023-08-03 19:40:40,514 INFO: Applying genetic operators for generation 34.
2023-08-03 19:40:40,518 INFO: Evaluating fitness for for generation 34.
2023-08-03 19:40:40,518 INFO: Will evaluate fitness for 15 individuals.
2023-08-03 19:40:41,114 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:40:41,114 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:40:41,131 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.6930918761385682 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4208884864184802 ,beta=0.3154390517428888].
2023-08-03 19:40:41,131 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.6930918761385682 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4208884864184802 ,beta=0.3154390517428888].
2023-08-03 19:41:02,062 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:41:02,063 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:41:02,307 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:41:02,307 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:41:22,763 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:41:22,763 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:41:24,542 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.4520348014311292, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.7497963619488048 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.10469124716713774 ,initial_accumulator_value=0.19729001609272556 ,l1_regularization_strength=0.1547059341602024 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9372961374395737 ,beta=0.3154390517428888].
2023-08-03 19:41:24,543 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.4520348014311292, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.7497963619488048 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.10469124716713774 ,initial_accumulator_value=0.19729001609272556 ,l1_regularization_strength=0.1547059341602024 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9372961374395737 ,beta=0.3154390517428888].
2023-08-03 19:41:44,027 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=0.0022290666555380228 ,use_ema=True ,ema_momentum=0.5769852990401191 ,rho=0.8804256811565929 ,epsilon=0.00011146601401847762 ,centered=True ,beta_1=0.0711958542947222 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.8893068297445663 ,l1_regularization_strength=0.7127896265448754 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.26984161906906123 ,beta=0.5542483214621685].
2023-08-03 19:41:44,027 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=0.0022290666555380228 ,use_ema=True ,ema_momentum=0.5769852990401191 ,rho=0.8804256811565929 ,epsilon=0.00011146601401847762 ,centered=True ,beta_1=0.0711958542947222 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.8893068297445663 ,l1_regularization_strength=0.7127896265448754 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.26984161906906123 ,beta=0.5542483214621685].
2023-08-03 19:41:45,868 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:41:45,868 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:42:06,993 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.050862804364365966 ,rho=0.3031778514111988 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.6983971924654685 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.5088214282696237].
2023-08-03 19:42:06,993 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.050862804364365966 ,rho=0.3031778514111988 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.6983971924654685 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.5088214282696237].
2023-08-03 19:42:21,252 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.882680619108186 ,rho=0.8001976339256608 ,epsilon=0.0009216449726904175 ,centered=True ,beta_1=0.16868786882861375 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.13564283848589875 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.4800061731301982 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.593943493033516 ,beta=0.3154390517428888].
2023-08-03 19:42:21,252 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.882680619108186 ,rho=0.8001976339256608 ,epsilon=0.0009216449726904175 ,centered=True ,beta_1=0.16868786882861375 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.13564283848589875 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.4800061731301982 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.593943493033516 ,beta=0.3154390517428888].
2023-08-03 19:42:28,649 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:42:28,649 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:42:47,562 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:42:47,562 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:42:50,125 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:42:50,126 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:43:08,702 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.0729444459850841, momentum=0.23661927874081734 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.9071810086753045 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.6124922487044255 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.7837247427009202 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9075723715242305 ,beta=0.0061973728043113185].
2023-08-03 19:43:08,703 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.0729444459850841, momentum=0.23661927874081734 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.9071810086753045 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.6124922487044255 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.7837247427009202 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9075723715242305 ,beta=0.0061973728043113185].
2023-08-03 19:43:12,464 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:43:12,464 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:43:33,056 INFO: Applying selection operators for generation 35.
2023-08-03 19:43:33,056 INFO: Applying genetic operators for generation 35.
2023-08-03 19:43:33,059 INFO: Evaluating fitness for for generation 35.
2023-08-03 19:43:33,059 INFO: Will evaluate fitness for 15 individuals.
2023-08-03 19:43:33,580 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.0729444459850841, momentum=0.23661927874081734 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.7837247427009202 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9075723715242305 ,beta=0.0061973728043113185].
2023-08-03 19:43:33,580 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.0729444459850841, momentum=0.23661927874081734 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.7837247427009202 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9075723715242305 ,beta=0.0061973728043113185].
2023-08-03 19:43:33,733 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:43:33,733 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:43:53,586 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.9071810086753045 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.6124922487044255 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:43:53,586 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.9071810086753045 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.6124922487044255 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:43:55,353 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:43:55,353 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:44:14,552 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:44:14,552 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:44:17,250 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.8037363061979633, momentum=0.3701147600189424 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.3038350360226827 ,rho=0.8001976339256608 ,epsilon=0.0006398255729073405 ,centered=False ,beta_1=0.061031188296230554 ,beta_2=0.9499360699045695 ,learning_rate_power=-0.5040110643183554 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.0350062044274273 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.055279843207596824].
2023-08-03 19:44:17,250 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.8037363061979633, momentum=0.3701147600189424 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.3038350360226827 ,rho=0.8001976339256608 ,epsilon=0.0006398255729073405 ,centered=False ,beta_1=0.061031188296230554 ,beta_2=0.9499360699045695 ,learning_rate_power=-0.5040110643183554 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.0350062044274273 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.055279843207596824].
2023-08-03 19:44:35,829 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:44:35,829 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:44:44,057 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.005252447271334628 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=6.568219650538958e-06 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.336461536697253 ,initial_accumulator_value=0.4721952843195203 ,l1_regularization_strength=0.5943247720009179 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6201481307222038 ,beta=0.3154390517428888].
2023-08-03 19:44:44,057 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.005252447271334628 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=6.568219650538958e-06 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.336461536697253 ,initial_accumulator_value=0.4721952843195203 ,l1_regularization_strength=0.5943247720009179 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6201481307222038 ,beta=0.3154390517428888].
2023-08-03 19:44:57,485 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:44:57,485 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:45:10,392 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8917380326022736, momentum=0.35165723398622517 ,nesterov=True ,amsgrad=True ,weight_decay=0.006443789682859371 ,use_ema=False ,ema_momentum=0.2019942710645063 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.5243131986938694 ,learning_rate_power=-0.8885007794935115 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.07203118597614211 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:45:10,392 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8917380326022736, momentum=0.35165723398622517 ,nesterov=True ,amsgrad=True ,weight_decay=0.006443789682859371 ,use_ema=False ,ema_momentum=0.2019942710645063 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.5243131986938694 ,learning_rate_power=-0.8885007794935115 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.07203118597614211 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:45:18,780 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:45:18,780 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:45:31,915 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.6124922487044255 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:45:31,915 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.6124922487044255 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:45:39,981 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.0729444459850841, momentum=0.23661927874081734 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.9071810086753045 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.7837247427009202 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9075723715242305 ,beta=0.0061973728043113185].
2023-08-03 19:45:39,981 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.0729444459850841, momentum=0.23661927874081734 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.9071810086753045 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.7837247427009202 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9075723715242305 ,beta=0.0061973728043113185].
2023-08-03 19:45:53,166 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:45:53,167 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:45:59,949 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:45:59,949 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:46:20,122 INFO: Applying selection operators for generation 36.
2023-08-03 19:46:20,122 INFO: Applying genetic operators for generation 36.
2023-08-03 19:46:20,125 INFO: Evaluating fitness for for generation 36.
2023-08-03 19:46:20,125 INFO: Will evaluate fitness for 11 individuals.
2023-08-03 19:46:20,637 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:46:20,637 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:46:20,643 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:46:20,643 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:46:42,400 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:46:42,400 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:46:42,523 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:46:42,524 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:47:03,533 INFO: Building the Keras optimizer to use for [Base=Adam, learning_rate=0.35717537447343717, momentum=0.318185297054866 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9276777807077226 ,rho=0.490552005977991 ,epsilon=0.00037190750306971836 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.5726111665785554 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.06723439403709386 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5113334523050052 ,beta=0.3354284915850524].
2023-08-03 19:47:03,533 INFO: Building Adam optimizer to use for [Base=Adam, learning_rate=0.35717537447343717, momentum=0.318185297054866 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9276777807077226 ,rho=0.490552005977991 ,epsilon=0.00037190750306971836 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.5726111665785554 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.06723439403709386 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5113334523050052 ,beta=0.3354284915850524].
2023-08-03 19:47:03,600 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.2200466742234526, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.4611672675633509 ,rho=0.8405518453437595 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.6737558506956637 ,beta_2=0.08723795044237836 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.662766589679178 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.893277806274772].
2023-08-03 19:47:03,600 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.2200466742234526, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.4611672675633509 ,rho=0.8405518453437595 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.6737558506956637 ,beta_2=0.08723795044237836 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.662766589679178 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.893277806274772].
2023-08-03 19:47:24,357 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.5998950485501178, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0036539017740618565 ,use_ema=True ,ema_momentum=0.6536203152207304 ,rho=0.9110966082738859 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.32464565742461515 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.7977230021161731 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:47:24,358 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.5998950485501178, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.0036539017740618565 ,use_ema=True ,ema_momentum=0.6536203152207304 ,rho=0.9110966082738859 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.32464565742461515 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.7977230021161731 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:47:29,559 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:47:29,560 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:47:45,647 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.0729444459850841, momentum=0.23661927874081734 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9075723715242305 ,beta=0.0061973728043113185].
2023-08-03 19:47:45,648 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.0729444459850841, momentum=0.23661927874081734 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9075723715242305 ,beta=0.0061973728043113185].
2023-08-03 19:47:51,650 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.06227330720294644, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.001865702228450211 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.6180326014613198 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.2378776815167326 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:47:51,650 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.06227330720294644, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=0.001865702228450211 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.6180326014613198 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.2378776815167326 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:48:06,207 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:48:06,208 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:48:26,504 INFO: Applying selection operators for generation 37.
2023-08-03 19:48:26,504 INFO: Applying genetic operators for generation 37.
2023-08-03 19:48:26,507 INFO: Evaluating fitness for for generation 37.
2023-08-03 19:48:26,507 INFO: Will evaluate fitness for 16 individuals.
2023-08-03 19:48:27,024 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.0729444459850841, momentum=0.23661927874081734 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:48:27,024 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.0729444459850841, momentum=0.23661927874081734 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:48:27,029 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:48:27,029 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:48:48,221 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:48:48,221 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:48:50,945 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.7837247427009202 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9075723715242305 ,beta=0.0061973728043113185].
2023-08-03 19:48:50,945 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.7837247427009202 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.9075723715242305 ,beta=0.0061973728043113185].
2023-08-03 19:49:10,268 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:49:10,268 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:49:11,657 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.00027883545032491967 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0003787437921351932 ,centered=False ,beta_1=0.5499459096915241 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.8318125795913204 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.463529616773636].
2023-08-03 19:49:11,657 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.00027883545032491967 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0003787437921351932 ,centered=False ,beta_1=0.5499459096915241 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.8318125795913204 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.463529616773636].
2023-08-03 19:49:32,087 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:49:32,087 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:49:37,602 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:49:37,602 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:49:53,661 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:49:53,661 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:49:58,635 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:49:58,635 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:50:14,669 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:50:14,669 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:50:19,460 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:50:19,460 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:50:35,365 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:50:35,365 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:50:40,229 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.015866816951194518 ,rho=0.8001976339256608 ,epsilon=0.0007388477291970313 ,centered=True ,beta_1=0.6729033454595768 ,beta_2=0.9160367498057492 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.6804344224538084 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.18944341657370123 ,beta=0.3154390517428888].
2023-08-03 19:50:40,229 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.015866816951194518 ,rho=0.8001976339256608 ,epsilon=0.0007388477291970313 ,centered=True ,beta_1=0.6729033454595768 ,beta_2=0.9160367498057492 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.6804344224538084 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.18944341657370123 ,beta=0.3154390517428888].
2023-08-03 19:50:56,658 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:50:56,658 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:51:06,443 INFO: Building the Keras optimizer to use for [Base=Adadelta, learning_rate=0.9016576342731988, momentum=0.8678094383070662 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.8037259712026059 ,rho=0.39600385672411675 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.9876086526325051 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.3989153768117223 ,initial_accumulator_value=0.13018351419634366 ,l1_regularization_strength=0.40813472863656286 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5421623033942132 ,beta=0.3154390517428888].
2023-08-03 19:51:06,443 INFO: Building Adadelta optimizer to use for [Base=Adadelta, learning_rate=0.9016576342731988, momentum=0.8678094383070662 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.8037259712026059 ,rho=0.39600385672411675 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.9876086526325051 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.3989153768117223 ,initial_accumulator_value=0.13018351419634366 ,l1_regularization_strength=0.40813472863656286 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5421623033942132 ,beta=0.3154390517428888].
2023-08-03 19:51:31,015 INFO: Applying selection operators for generation 38.
2023-08-03 19:51:31,015 INFO: Applying genetic operators for generation 38.
2023-08-03 19:51:31,018 INFO: Evaluating fitness for for generation 38.
2023-08-03 19:51:31,018 INFO: Will evaluate fitness for 15 individuals.
2023-08-03 19:51:31,540 INFO: Building the Keras optimizer to use for [Base=RMSprop, learning_rate=0.9260531076018655, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0003913044115202871 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.2912380304504627 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.49388290657522516 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6356127152499771 ,beta=0.3154390517428888].
2023-08-03 19:51:31,540 INFO: Building RMSprop optimizer to use for [Base=RMSprop, learning_rate=0.9260531076018655, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0003913044115202871 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.2912380304504627 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.49388290657522516 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.6356127152499771 ,beta=0.3154390517428888].
2023-08-03 19:51:31,713 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:51:31,713 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:51:53,631 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:51:53,631 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:51:57,387 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:51:57,387 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:52:15,059 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.042299429664539256, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.08969380794924542 ,rho=0.8001976339256608 ,epsilon=0.00027919868928666787 ,centered=False ,beta_1=0.9686622690161687 ,beta_2=0.25860681577558653 ,learning_rate_power=-0.6941809286997161 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7624652274109822 ,beta=0.3154390517428888].
2023-08-03 19:52:15,060 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.042299429664539256, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.08969380794924542 ,rho=0.8001976339256608 ,epsilon=0.00027919868928666787 ,centered=False ,beta_1=0.9686622690161687 ,beta_2=0.25860681577558653 ,learning_rate_power=-0.6941809286997161 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7624652274109822 ,beta=0.3154390517428888].
2023-08-03 19:52:18,419 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.9224579412773222, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.007398647514845938 ,use_ema=False ,ema_momentum=0.345263482601327 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.04842060429144057 ,initial_accumulator_value=0.14691795924998052 ,l1_regularization_strength=0.33866026204227606 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:52:18,420 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.9224579412773222, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.007398647514845938 ,use_ema=False ,ema_momentum=0.345263482601327 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.04842060429144057 ,initial_accumulator_value=0.14691795924998052 ,l1_regularization_strength=0.33866026204227606 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:52:39,181 INFO: Building the Keras optimizer to use for [Base=Adam, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7020532812447133 ,learning_rate_power=-0.8784919271996756 ,initial_accumulator_value=0.4875427944510532 ,l1_regularization_strength=0.1592337596278952 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:52:39,182 INFO: Building Adam optimizer to use for [Base=Adam, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7020532812447133 ,learning_rate_power=-0.8784919271996756 ,initial_accumulator_value=0.4875427944510532 ,l1_regularization_strength=0.1592337596278952 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:52:39,679 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:52:39,679 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:53:01,273 INFO: Building the Keras optimizer to use for [Base=Adam, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.009530626461361335 ,use_ema=True ,ema_momentum=0.17194567183260245 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.5125020468816813 ,beta_2=0.1440778200800601 ,learning_rate_power=-0.6457430653255491 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.05513775763241713 ,beta=0.23883418538238077].
2023-08-03 19:53:01,274 INFO: Building Adam optimizer to use for [Base=Adam, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.009530626461361335 ,use_ema=True ,ema_momentum=0.17194567183260245 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.5125020468816813 ,beta_2=0.1440778200800601 ,learning_rate_power=-0.6457430653255491 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.05513775763241713 ,beta=0.23883418538238077].
2023-08-03 19:53:06,061 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:53:06,062 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:53:26,957 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.00027883545032491967 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:53:26,958 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=0.00027883545032491967 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:53:29,033 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:53:29,034 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:53:48,143 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0003787437921351932 ,centered=False ,beta_1=0.5499459096915241 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.8318125795913204 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.463529616773636].
2023-08-03 19:53:48,143 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0003787437921351932 ,centered=False ,beta_1=0.5499459096915241 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.8318125795913204 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.463529616773636].
2023-08-03 19:53:49,884 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:53:49,884 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:54:09,116 INFO: Building the Keras optimizer to use for [Base=SGD, learning_rate=0.6073585904928976, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9457423803598657 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.7693724247541949 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.6043777623822438 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.14558286339764104 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:54:09,117 INFO: Building SGD optimizer to use for [Base=SGD, learning_rate=0.6073585904928976, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9457423803598657 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.7693724247541949 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.6043777623822438 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.14558286339764104 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:54:28,799 INFO: Applying selection operators for generation 39.
2023-08-03 19:54:28,799 INFO: Applying genetic operators for generation 39.
2023-08-03 19:54:28,802 INFO: Evaluating fitness for for generation 39.
2023-08-03 19:54:28,803 INFO: Will evaluate fitness for 11 individuals.
2023-08-03 19:54:29,316 INFO: Building the Keras optimizer to use for [Base=Ftrl, learning_rate=0.6703039868857844, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.495869013566706 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.9984064467460123 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.23049456483287956 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.07474758927945735 ,beta=0.37661161676825283].
2023-08-03 19:54:29,316 INFO: Building Ftrl optimizer to use for [Base=Ftrl, learning_rate=0.6703039868857844, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.495869013566706 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.9984064467460123 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.23049456483287956 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.07474758927945735 ,beta=0.37661161676825283].
2023-08-03 19:54:29,483 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:54:29,483 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:54:51,377 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:54:51,378 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:54:55,078 INFO: Building the Keras optimizer to use for [Base=Nadam, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9036983987288183 ,rho=0.71499503357248 ,epsilon=0.00015790161103708977 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.35067811843868957 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.19343728864429655 ,beta=0.40895696943006354].
2023-08-03 19:54:55,078 INFO: Building Nadam optimizer to use for [Base=Nadam, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9036983987288183 ,rho=0.71499503357248 ,epsilon=0.00015790161103708977 ,centered=True ,beta_1=0.3059441303683468 ,beta_2=0.35067811843868957 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.19343728864429655 ,beta=0.40895696943006354].
2023-08-03 19:55:12,644 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.001166443405285471 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.025217062748188468 ,learning_rate_power=-0.39560327490367286 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:55:12,644 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.001166443405285471 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.025217062748188468 ,learning_rate_power=-0.39560327490367286 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:55:30,066 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:55:30,066 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:55:39,004 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:55:39,004 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:55:51,771 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.1265100443861562, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0005946698721170041 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.278600297782049 ,l1_regularization_strength=0.26973787725616105 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5895633403731693 ,beta=0.3154390517428888].
2023-08-03 19:55:51,771 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.1265100443861562, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0005946698721170041 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.278600297782049 ,l1_regularization_strength=0.26973787725616105 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.5895633403731693 ,beta=0.3154390517428888].
2023-08-03 19:55:59,973 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.29585239252557083 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.31113523235180074 ,initial_accumulator_value=0.568974341467987 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.3128674157104119 ,beta=0.742053379709495].
2023-08-03 19:55:59,974 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.29585239252557083 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.31113523235180074 ,initial_accumulator_value=0.568974341467987 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.3128674157104119 ,beta=0.742053379709495].
2023-08-03 19:56:16,420 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.5991102023069068, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.5450423456235799 ,rho=0.7907719937617156 ,epsilon=0.0005306327093132606 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.24228434583128988 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38552094051086483 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4185540962611659 ,beta=0.7895340186155785].
2023-08-03 19:56:16,420 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.5991102023069068, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.5450423456235799 ,rho=0.7907719937617156 ,epsilon=0.0005306327093132606 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.24228434583128988 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38552094051086483 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.4185540962611659 ,beta=0.7895340186155785].
2023-08-03 19:56:21,073 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:56:21,073 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:56:42,356 INFO: Applying selection operators for generation 40.
2023-08-03 19:56:42,356 INFO: Applying genetic operators for generation 40.
2023-08-03 19:56:42,359 INFO: Evaluating fitness for for generation 40.
2023-08-03 19:56:42,359 INFO: Will evaluate fitness for 15 individuals.
2023-08-03 19:56:42,990 INFO: Building the Keras optimizer to use for [Base=Adamax, learning_rate=0.7693526311653826, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6862169941559283 ,rho=0.5603147121161867 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8567586398749228 ,beta=0.3154390517428888].
2023-08-03 19:56:42,990 INFO: Building Adamax optimizer to use for [Base=Adamax, learning_rate=0.7693526311653826, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.6862169941559283 ,rho=0.5603147121161867 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.8567586398749228 ,beta=0.3154390517428888].
2023-08-03 19:56:43,064 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:56:43,064 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:57:03,801 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:57:03,801 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:57:04,856 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.811101662683914, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.0041132750723597575 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.2803804046303252 ,epsilon=0.0008989157061379675 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.23005116884377141 ,beta=0.3154390517428888].
2023-08-03 19:57:04,856 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.811101662683914, momentum=0 ,nesterov=False ,amsgrad=False ,weight_decay=0.0041132750723597575 ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.2803804046303252 ,epsilon=0.0008989157061379675 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.23005116884377141 ,beta=0.3154390517428888].
2023-08-03 19:57:24,978 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:57:24,978 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:57:30,581 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:57:30,581 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:57:45,949 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:57:45,950 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:57:51,933 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:57:51,934 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:58:07,057 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:58:07,058 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:58:14,059 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:58:14,059 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:58:28,037 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.8692759059251621, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9178091486163386 ,rho=0.534582850539154 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.9607936088150433 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7380903870480292 ,beta=0.3154390517428888].
2023-08-03 19:58:28,037 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.8692759059251621, momentum=0 ,nesterov=True ,amsgrad=True ,weight_decay=None ,use_ema=True ,ema_momentum=0.9178091486163386 ,rho=0.534582850539154 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.9607936088150433 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7380903870480292 ,beta=0.3154390517428888].
2023-08-03 19:58:35,542 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:58:35,543 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:58:49,165 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:58:49,165 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.9756963034912268 ,rho=0.8001976339256608 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.44644707765656866 ,beta=0.3154390517428888].
2023-08-03 19:58:56,878 INFO: Building the Keras optimizer to use for [Base=Adam, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.5105466245629228 ,rho=0.8001976339256608 ,epsilon=0.0004024314566909313 ,centered=False ,beta_1=0.24392567582475 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.9668573603936413 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7400822311515554 ,beta=0.3154390517428888].
2023-08-03 19:58:56,878 INFO: Building Adam optimizer to use for [Base=Adam, learning_rate=0.0729444459850841, momentum=0 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=True ,ema_momentum=0.5105466245629228 ,rho=0.8001976339256608 ,epsilon=0.0004024314566909313 ,centered=False ,beta_1=0.24392567582475 ,beta_2=0.7772609214979462 ,learning_rate_power=-0.8294380443284037 ,initial_accumulator_value=0.38835748443787177 ,l1_regularization_strength=0.9668573603936413 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.7400822311515554 ,beta=0.3154390517428888].
2023-08-03 19:59:10,607 INFO: Building the Keras optimizer to use for [Base=Adagrad, learning_rate=0.6979741641925846, momentum=0.24703933814577317 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.0013335428577413433 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.21055766124580944 ,learning_rate_power=-0.887922664273766 ,initial_accumulator_value=0.04797041214831643 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.971231527377954 ,beta=0.3154390517428888].
2023-08-03 19:59:10,607 INFO: Building Adagrad optimizer to use for [Base=Adagrad, learning_rate=0.6979741641925846, momentum=0.24703933814577317 ,nesterov=True ,amsgrad=False ,weight_decay=None ,use_ema=False ,ema_momentum=0.9756963034912268 ,rho=0.0013335428577413433 ,epsilon=0.0007712614300004156 ,centered=False ,beta_1=0.3059441303683468 ,beta_2=0.21055766124580944 ,learning_rate_power=-0.887922664273766 ,initial_accumulator_value=0.04797041214831643 ,l1_regularization_strength=0.2752359307601201 ,l2_regularization_strength=0.7165873255754192 ,l2_shrinkage_regularization_strength=0.971231527377954 ,beta=0.3154390517428888].
2023-08-03 19:59:30,562 INFO: Saving the results to the folder specified in the arguments.
2023-08-03 19:59:30,566 INFO: Closing the multiprocessing pool.
2023-08-03 19:59:30,566 INFO: Experiment finished.
